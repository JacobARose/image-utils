<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.big.make_shards API documentation</title>
<meta name="description" content="imutils/big/make_shards.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.big.make_shards</code></h1>
</header>
<section id="section-intro">
<p>imutils/big/make_shards.py</p>
<p>Generate one or more webdataset-compatible tar archive shards from an image classification dataset.</p>
<p>Based on script: <a href="https://github.com/tmbdev-archive/webdataset-examples/blob/7f56e9a8b978254c06aa0a98572a1331968b0eb3/makeshards.py">https://github.com/tmbdev-archive/webdataset-examples/blob/7f56e9a8b978254c06aa0a98572a1331968b0eb3/makeshards.py</a></p>
<p>Added on: Sunday March 6th, 2022</p>
<p>Example usage:</p>
<p>python "/media/data/jacob/GitHub/image-utils/imutils/big/make_shards.py"
&ndash;subsets=train,val,test
&ndash;maxsize='1e9'
&ndash;maxcount=50000
&ndash;shard_dir="/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/webdataset"
&ndash;catalog_dir="/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/catalog"
&ndash;debug</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
imutils/big/make_shards.py

Generate one or more webdataset-compatible tar archive shards from an image classification dataset.

Based on script: https://github.com/tmbdev-archive/webdataset-examples/blob/7f56e9a8b978254c06aa0a98572a1331968b0eb3/makeshards.py

Added on: Sunday March 6th, 2022

Example usage:

python &#34;/media/data/jacob/GitHub/image-utils/imutils/big/make_shards.py&#34; \
        --subsets=train,val,test \
        --maxsize=&#39;1e9&#39; \
        --maxcount=50000 \
        --shard_dir=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/webdataset&#34; \
        --catalog_dir=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/catalog&#34; \
        --debug


&#34;&#34;&#34;


import sys
import os
import os.path
import random
import argparse

from torchvision import datasets

import webdataset as wds


import numpy as np
import os
from typing import Optional, Tuple, Any, Dict
from tqdm import trange, tqdm

import tarfile
tarfile.DEFAULT_FORMAT = tarfile.GNU_FORMAT

import webdataset as wds
# from imutils.big.datamodule import Herbarium2022DataModule, Herbarium2022Dataset
from imutils.ml.data.datamodule import Herbarium2022DataModule, Herbarium2022Dataset

def read_file_binary(fname):
        &#34;Read a binary file from disk.&#34;
        with open(fname, &#34;rb&#34;) as stream:
                return stream.read()

all_keys = set()

def prepare_sample(dataset, index, subset: str=&#34;train&#34;, filekey: bool=False) -&gt; Dict[str, Any]:
        image_binary, label, metadata = dataset[index]

        key = metadata[&#34;catalog_number&#34;]
        assert key not in all_keys
        all_keys.add(key)

        xkey = key if filekey else &#34;%07d&#34; % index
        sample = {&#34;__key__&#34;: xkey, 
                          &#34;image.jpg&#34;: image_binary}
        
        if subset != &#34;test&#34;:
                assert label == dataset.targets[index]
                sample[&#34;label.cls&#34;] = int(label)
        
        return sample

        
def write_dataset(catalog_dir: Optional[str]=None,
                                  shard_dir: Optional[str]=None,
                                  subset=&#34;train&#34;,
                                  maxsize=1e9,
                                  maxcount=100000,
                                  limit_num_samples: Optional[int]=np.inf,
                                  filekey: bool=False,
                                  dataset=None):

        if dataset is None:
                datamodule = Herbarium2022DataModule(catalog_dir=catalog_dir,
                                                                                         num_workers=4,
                                                                                         image_reader=read_file_binary,
                                                                                         remove_transforms=True)
                datamodule.setup()
                dataset = datamodule.get_dataset(subset=subset)
        
        num_samples = len(dataset)
        print(f&#34;With subset={subset}, Total num_samples: {num_samples}&#34;)
        
        if limit_num_samples &lt; num_samples:
                num_samples = limit_num_samples
                print(f&#34;Limiting this run to num_samples: {num_samples}&#34;)
        indices = list(range(num_samples))

        os.makedirs(shard_dir, exist_ok=True)
        pattern = os.path.join(shard_dir, f&#34;herbarium_2022-{subset}-%06d.tar&#34;)

        with wds.ShardWriter(pattern, maxsize=maxsize, maxcount=maxcount) as sink:
                for i in tqdm(indices, desc=f&#34;idx(Total={num_samples})&#34;):
                        
                        sample = prepare_sample(dataset, index=i, subset=subset, filekey=filekey)
                        sink.write(sample)


        return dataset, indices



def parse_args() -&gt; argparse.Namespace:
        parser = argparse.ArgumentParser(&#34;&#34;&#34;Generate sharded dataset from supervised image dataset.&#34;&#34;&#34;)
        parser.add_argument(&#34;--subsets&#34;, default=&#34;train,val,test&#34;, help=&#34;which subsets to write&#34;)
        parser.add_argument(
                &#34;--filekey&#34;, action=&#34;store_true&#34;, help=&#34;use file as key (default: index)&#34;
        )
        parser.add_argument(&#34;--maxsize&#34;, type=float, default=1e9)
        parser.add_argument(&#34;--maxcount&#34;, type=float, default=100000)
        parser.add_argument(
                &#34;--shard_dir&#34;,
                default=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/webdataset&#34;,
                help=&#34;directory where shards are written&#34;
        )
        parser.add_argument(
                &#34;--catalog_dir&#34;,
                default=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/catalog&#34;,
                help=&#34;directory containing csv versions of the original train &amp; test metadata json files from herbarium 2022&#34;,
        )
        parser.add_argument(&#34;--debug&#34;,  action=&#34;store_true&#34;, default=False,
                                           help=&#34;Provide this boolean flag to produce a debugging shard dataset of only a maximum of 200 samples per data subset. [TODO] Switch to temp directories when this flag is passed.&#34;)
        
        args = parser.parse_args()
        return args


def main(args):
        
        # args = parse_args()
        
        assert args.maxsize &gt; 10000000 # Shards must be a minimum of 10+ MB
        assert args.maxcount &lt; 1000000 # Shards must contain a maximum of 1,000,000 samples each

        limit_num_samples = 200 if args.debug else np.inf

#       if not os.path.isdir(os.path.join(args.data, &#34;train&#34;)):
#               print(f&#34;{args.data}: should be directory containing ImageNet&#34;, file=sys.stderr)
#               print(f&#34;suitable as argument for torchvision.datasets.ImageNet(...)&#34;, file=sys.stderr)
#               sys.exit(1)

#       if not os.path.isdir(os.path.join(args.shards, &#34;.&#34;)):
#               print(f&#34;{args.shards}: should be a writable destination directory for shards&#34;, file=sys.stderr)
#               sys.exit(1)

                
        subsets = args.subsets.split(&#34;,&#34;)

        for subset in tqdm(subsets, leave=True, desc=f&#34;Processing {len(subsets)} subsets&#34;):
                # print(&#34;# subset&#34;, subset)
                
                dataset, indices = write_dataset(catalog_dir=args.catalog_dir,
                                                                                 shard_dir=args.shard_dir,
                                                                                 subset=subset,
                                                                                 maxsize=args.maxsize,
                                                                                 maxcount=args.maxcount,
                                                                                 limit_num_samples=limit_num_samples,
                                                                                 filekey=args.filekey)
                
                
CATALOG_DIR = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/catalog&#34;
# SHARD_DIR = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/webdataset&#34;

if __name__ == &#34;__main__&#34;:
        
        args = parse_args()
        main(args)

        written_files = os.listdir(args.shard_dir)
        files_per_subset = {&#34;train&#34;:[],
                                            &#34;val&#34;:[],
                                            &#34;test&#34;:[]}
        for subset,v in files_per_subset.items():
                files_per_subset[subset] = len([f for f in written_files if subset in f])
                
        from rich import print as pp
        print(f&#34;SUCCESS! TARGET SHARD DIR CONTAINS THE FOLLOWING:&#34;)
        pp(files_per_subset)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imutils.big.make_shards.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(args):
        
        # args = parse_args()
        
        assert args.maxsize &gt; 10000000 # Shards must be a minimum of 10+ MB
        assert args.maxcount &lt; 1000000 # Shards must contain a maximum of 1,000,000 samples each

        limit_num_samples = 200 if args.debug else np.inf

#       if not os.path.isdir(os.path.join(args.data, &#34;train&#34;)):
#               print(f&#34;{args.data}: should be directory containing ImageNet&#34;, file=sys.stderr)
#               print(f&#34;suitable as argument for torchvision.datasets.ImageNet(...)&#34;, file=sys.stderr)
#               sys.exit(1)

#       if not os.path.isdir(os.path.join(args.shards, &#34;.&#34;)):
#               print(f&#34;{args.shards}: should be a writable destination directory for shards&#34;, file=sys.stderr)
#               sys.exit(1)

                
        subsets = args.subsets.split(&#34;,&#34;)

        for subset in tqdm(subsets, leave=True, desc=f&#34;Processing {len(subsets)} subsets&#34;):
                # print(&#34;# subset&#34;, subset)
                
                dataset, indices = write_dataset(catalog_dir=args.catalog_dir,
                                                                                 shard_dir=args.shard_dir,
                                                                                 subset=subset,
                                                                                 maxsize=args.maxsize,
                                                                                 maxcount=args.maxcount,
                                                                                 limit_num_samples=limit_num_samples,
                                                                                 filekey=args.filekey)</code></pre>
</details>
</dd>
<dt id="imutils.big.make_shards.parse_args"><code class="name flex">
<span>def <span class="ident">parse_args</span></span>(<span>) â€‘>Â argparse.Namespace</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_args() -&gt; argparse.Namespace:
        parser = argparse.ArgumentParser(&#34;&#34;&#34;Generate sharded dataset from supervised image dataset.&#34;&#34;&#34;)
        parser.add_argument(&#34;--subsets&#34;, default=&#34;train,val,test&#34;, help=&#34;which subsets to write&#34;)
        parser.add_argument(
                &#34;--filekey&#34;, action=&#34;store_true&#34;, help=&#34;use file as key (default: index)&#34;
        )
        parser.add_argument(&#34;--maxsize&#34;, type=float, default=1e9)
        parser.add_argument(&#34;--maxcount&#34;, type=float, default=100000)
        parser.add_argument(
                &#34;--shard_dir&#34;,
                default=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/webdataset&#34;,
                help=&#34;directory where shards are written&#34;
        )
        parser.add_argument(
                &#34;--catalog_dir&#34;,
                default=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/herbarium_2022/catalog&#34;,
                help=&#34;directory containing csv versions of the original train &amp; test metadata json files from herbarium 2022&#34;,
        )
        parser.add_argument(&#34;--debug&#34;,  action=&#34;store_true&#34;, default=False,
                                           help=&#34;Provide this boolean flag to produce a debugging shard dataset of only a maximum of 200 samples per data subset. [TODO] Switch to temp directories when this flag is passed.&#34;)
        
        args = parser.parse_args()
        return args</code></pre>
</details>
</dd>
<dt id="imutils.big.make_shards.prepare_sample"><code class="name flex">
<span>def <span class="ident">prepare_sample</span></span>(<span>dataset, index, subset:Â strÂ =Â 'train', filekey:Â boolÂ =Â False) â€‘>Â Dict[str,Â Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_sample(dataset, index, subset: str=&#34;train&#34;, filekey: bool=False) -&gt; Dict[str, Any]:
        image_binary, label, metadata = dataset[index]

        key = metadata[&#34;catalog_number&#34;]
        assert key not in all_keys
        all_keys.add(key)

        xkey = key if filekey else &#34;%07d&#34; % index
        sample = {&#34;__key__&#34;: xkey, 
                          &#34;image.jpg&#34;: image_binary}
        
        if subset != &#34;test&#34;:
                assert label == dataset.targets[index]
                sample[&#34;label.cls&#34;] = int(label)
        
        return sample</code></pre>
</details>
</dd>
<dt id="imutils.big.make_shards.read_file_binary"><code class="name flex">
<span>def <span class="ident">read_file_binary</span></span>(<span>fname)</span>
</code></dt>
<dd>
<div class="desc"><p>Read a binary file from disk.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_file_binary(fname):
        &#34;Read a binary file from disk.&#34;
        with open(fname, &#34;rb&#34;) as stream:
                return stream.read()</code></pre>
</details>
</dd>
<dt id="imutils.big.make_shards.write_dataset"><code class="name flex">
<span>def <span class="ident">write_dataset</span></span>(<span>catalog_dir:Â Optional[str]Â =Â None, shard_dir:Â Optional[str]Â =Â None, subset='train', maxsize=1000000000.0, maxcount=100000, limit_num_samples:Â Optional[int]Â =Â inf, filekey:Â boolÂ =Â False, dataset=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_dataset(catalog_dir: Optional[str]=None,
                                  shard_dir: Optional[str]=None,
                                  subset=&#34;train&#34;,
                                  maxsize=1e9,
                                  maxcount=100000,
                                  limit_num_samples: Optional[int]=np.inf,
                                  filekey: bool=False,
                                  dataset=None):

        if dataset is None:
                datamodule = Herbarium2022DataModule(catalog_dir=catalog_dir,
                                                                                         num_workers=4,
                                                                                         image_reader=read_file_binary,
                                                                                         remove_transforms=True)
                datamodule.setup()
                dataset = datamodule.get_dataset(subset=subset)
        
        num_samples = len(dataset)
        print(f&#34;With subset={subset}, Total num_samples: {num_samples}&#34;)
        
        if limit_num_samples &lt; num_samples:
                num_samples = limit_num_samples
                print(f&#34;Limiting this run to num_samples: {num_samples}&#34;)
        indices = list(range(num_samples))

        os.makedirs(shard_dir, exist_ok=True)
        pattern = os.path.join(shard_dir, f&#34;herbarium_2022-{subset}-%06d.tar&#34;)

        with wds.ShardWriter(pattern, maxsize=maxsize, maxcount=maxcount) as sink:
                for i in tqdm(indices, desc=f&#34;idx(Total={num_samples})&#34;):
                        
                        sample = prepare_sample(dataset, index=i, subset=subset, filekey=filekey)
                        sink.write(sample)


        return dataset, indices</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils.big" href="index.html">imutils.big</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imutils.big.make_shards.main" href="#imutils.big.make_shards.main">main</a></code></li>
<li><code><a title="imutils.big.make_shards.parse_args" href="#imutils.big.make_shards.parse_args">parse_args</a></code></li>
<li><code><a title="imutils.big.make_shards.prepare_sample" href="#imutils.big.make_shards.prepare_sample">prepare_sample</a></code></li>
<li><code><a title="imutils.big.make_shards.read_file_binary" href="#imutils.big.make_shards.read_file_binary">read_file_binary</a></code></li>
<li><code><a title="imutils.big.make_shards.write_dataset" href="#imutils.big.make_shards.write_dataset">write_dataset</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>