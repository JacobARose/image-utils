<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.generate_multires_images API documentation</title>
<meta name="description" content="The following command: …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.generate_multires_images</code></h1>
</header>
<section id="section-intro">
<p>The following command:</p>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py" &ndash;run-all</p>
<h1 id="run-all-3-datasets-for-all-4-resolutions-in-serial">Run all 3 datasets for all 4 resolutions in serial</h1>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py" &ndash;dataset_name "all" &ndash;run-all</p>
<h1 id="clean-create-all-symlink-dirs-for-all-thresholds-and-all-datasets">Clean, &amp; create, all symlink dirs for all thresholds and all datasets.</h1>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multithresh_symlink_trees.py" &ndash;task "clean+create" -data "all" -a</p>
<h1 id="run-1-dataset-for-all-4-resolutions-in-serial">Run 1 dataset for all 4 resolutions in serial</h1>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py" &ndash;dataset_name Extant_Leaves &ndash;run-all -skip 512</p>
<p>is equivalent to the following 4 cmdline statements (Run in parallel):</p>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py" &ndash;resolution=512 &ndash;dataset_name Extant_Leaves
python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py" &ndash;resolution=1024 &ndash;dataset_name Extant_Leaves
python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py" &ndash;resolution=1536 &ndash;dataset_name Extant_Leaves
python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py" &ndash;resolution=2048 &ndash;dataset_name Extant_Leaves</p>
<h1 id="clean-create-all-symlink-dirs">Clean, &amp; create, all symlink dirs.</h1>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multithresh_symlink_trees.py" &ndash;task "clean+create" -a</p>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multithresh_symlink_trees.py" &ndash;task "clean+create" -data "General_Fossil"</p>
<p>Note: When launching in parallel from the cmdline, be mindful of specifying low enough num_workers.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding: utf-8

&#34;&#34;&#34;

The following command:

python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py&#34; --run-all



# Run all 3 datasets for all 4 resolutions in serial

python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py&#34; --dataset_name &#34;all&#34; --run-all

# Clean, &amp; create, all symlink dirs for all thresholds and all datasets.
python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multithresh_symlink_trees.py&#34; --task &#34;clean+create&#34; -data &#34;all&#34; -a



# Run 1 dataset for all 4 resolutions in serial

python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py&#34; --dataset_name Extant_Leaves --run-all -skip 512

is equivalent to the following 4 cmdline statements (Run in parallel):

python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py&#34; --resolution=512 --dataset_name Extant_Leaves
python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py&#34; --resolution=1024 --dataset_name Extant_Leaves
python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py&#34; --resolution=1536 --dataset_name Extant_Leaves
python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multires_images.py&#34; --resolution=2048 --dataset_name Extant_Leaves

# Clean, &amp; create, all symlink dirs.
python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multithresh_symlink_trees.py&#34; --task &#34;clean+create&#34; -a

python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/generate_multithresh_symlink_trees.py&#34; --task &#34;clean+create&#34; -data &#34;General_Fossil&#34;


Note: When launching in parallel from the cmdline, be mindful of specifying low enough num_workers.
&#34;&#34;&#34;


# # generate_images_multires_leavesdbv1_0-prerelease
#
# Created by: Jacob A Rose
# Created on: Tuesday June 29th, 2021

import argparse
import logging
import os
import random
import time
from pathlib import Path
from typing import Callable, List, Optional, Tuple, Union

import icecream as ic

# import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import PIL
import torch
import torchvision
from PIL import Image, ImageStat
from rich import print as pp
from torch import nn
from torchvision import transforms, utils
from torchvision.datasets import ImageFolder
from tqdm.auto import tqdm, trange


seed = 334455
random.seed(seed)
np.random.seed(seed)
pd.set_option(&#34;display.max_columns&#34;, 500)
pd.set_option(&#34;display.max_colwidth&#34;, 200)

from pandarallel import pandarallel
from imutils.catalog_registry import available_datasets
from imutils.utils.dataset_management_utils import (
    CleverCrop,
    diff_dataset_catalogs,
    parse_df_catalog_from_image_directory,
)

tqdm.pandas()

clever_crop = CleverCrop()


def resize_and_resave_dataset(data, res: int = 512):
    num_samples = len(data)
    target_shape = (3, res, res)
    for sample_idx in trange(num_samples, desc=f&#34;Smart Crop {dataset_name}&#34;):

        row = data.iloc[sample_idx]
        source_path = row.path
        target_path = row.target_path

        resize_and_save_img(img=source_path, target_path=target_path, target_shape=(3, res, res))


##############################


def resize_and_save_img(
    img: Union[str, Path, PIL.Image.Image], target_path: str, target_shape=None
):
    if os.path.isfile(target_path):
        return

    if isinstance(img, (str, Path)):
        img = Image.open(img)
    img = transforms.ToTensor()(img)
    target_img = clever_crop(img, target_shape=target_shape)
    target_img = (target_img * 255.0).to(torch.uint8)
    try:
        torchvision.io.write_jpeg(target_img, target_path, quality=100)
    except Exception as e:
        print(target_path, e)
    assert os.path.isfile(target_path)


def resize_and_resave_dataset_parallel(data, resolution: int = 512, parallel=True):
    target_shape = (3, resolution, resolution)
    if parallel:
        data.parallel_apply(
            lambda x: resize_and_save_img(
                img=x[&#34;path&#34;], target_path=x[&#34;target_path&#34;], target_shape=target_shape
            ),
            axis=1,
        )
    else:
        data.apply(
            lambda x: resize_and_save_img(
                img=x[&#34;path&#34;], target_path=x[&#34;target_path&#34;], target_shape=target_shape
            ),
            axis=1,
        )

    return data


##################
##################


def query_and_preprocess_catalog(config):
    &#34;&#34;&#34;

    Arguments:
        config (NameSpace):
            ::dataset_name
            ::y_col
            ::threshold
            ::resolution
            ::version, default=&#34;latest&#34;

    &#34;&#34;&#34;
    tag = available_datasets.query_tags(
        dataset_name=config.dataset_name,
        y_col=config.y_col,
        threshold=config.threshold,
        resolution=config.resolution,
    )
    if config.get(&#34;version&#34;) in [None, &#34;latest&#34;]:
        root_dir = available_datasets.get_latest(tag)
    else:
        root_dir = available_datasets.get(tag, config.version)
    print(f&#34;Tag: {tag}&#34;)
    print(f&#34;Root Dir: {root_dir}&#34;)
    print(f&#34;Version: {config.get(&#39;version&#39;)}&#34;)
    if not os.path.isdir(root_dir):
        print(
            f&#34;[INFO] There is not yet any directory located at: {root_dir}. Skipping dataset&#34;
            &#34; validation.&#34;
        )
        return pd.DataFrame(), root_dir
    data_df = parse_df_catalog_from_image_directory(
        root_dir=root_dir, dataset_name=config.dataset_name
    )

    return data_df, root_dir


def preprocess_target_catalog(data_df: pd.DataFrame, config, target_dir: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;

    Arguments:
        config (NameSpace):
            ::dataset_name
            ::y_col
            ::threshold
            ::resolution

    &#34;&#34;&#34;
    target_path = None
    if data_df.shape[0] &gt; 0:
        target_path = data_df.apply(
            lambda x: str(Path(target_dir, x.family, Path(x.path).name)), axis=1
        )
    data_df = data_df.assign(target_path=target_path)
    family_dirs = list(set(data_df.target_path.apply(lambda x: str(Path(x).parent))))
    [os.makedirs(subdir, exist_ok=True) for subdir in family_dirs]

    return data_df


def clean_unused_images(data_df: pd.DataFrame, path_col: str = &#34;target_path&#34;):

    print(
        f&#34;Running [CLEAN] process on target dir. Removing {data_df.shape[0]} image files that do&#34;
        &#34; not have a corresponding match in source dir.&#34;
    )

    if data_df.shape[0] == 0:
        print(
            f&#34;[SKIP] Happily skipping clean_unused_images() since there were 0 extraneous images&#34;
            f&#34; detected in target that do not exist in source&#34;
        )
        return

    data_df[path_col].progress_apply(os.unlink)
    remaining_stragglers = data_df[path_col].progress_apply(os.path.isfile).sum()
    assert remaining_stragglers == 0
    print(f&#34;[SUCCESS] Removed {data_df.shape[0]} image files from where they dont belong!&#34;)


def warm_start_catalogs(
    config, target_config, run_clean: bool = False, save_report: bool = False
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Save time by skipping previously processed files and keeping only the yet-to-be processed catalog.

    Query and Preprocess catalogs from source and target configs,
    return only the rows that are unique to the source location,
    then format the remainder in preparation to run the multi-res image generation script (i.e. source_path -&gt; &#34;path&#34;, target_path -&gt; &#34;target_path&#34; columns)

    &#34;&#34;&#34;
    source_catalog, source_dir = query_and_preprocess_catalog(config)
    target_catalog, target_dir = query_and_preprocess_catalog(target_config)

    if target_catalog.shape[0] &gt; 0:
        target_paths = target_catalog.apply(
            lambda x: str(Path(target_dir, x.relative_path)), axis=1
        )
        target_catalog = target_catalog.assign(path=target_paths)

    if target_catalog.shape[0] == 0:
        data_df = source_catalog
    else:
        shared, diff, source_only, target_only = diff_dataset_catalogs(
            source_catalog=source_catalog, target_catalog=target_catalog
        )

        if run_clean:
            clean_unused_images(data_df=target_only, path_col=&#34;path&#34;)
        print(shared.shape, diff.shape, source_only.shape, target_only.shape)
        data_df = source_only

        if save_report is not None:
            report_dir = Path(target_dir).parent / &#34;audit_logs&#34;
            os.makedirs(report_dir, exist_ok=True)
            shared.to_csv(Path(report_dir, &#34;shared_catalog.csv&#34;))
            diff.to_csv(Path(report_dir, &#34;diff_catalog.csv&#34;))
            print(f&#34;Exported catalog validation logs to directory: {report_dir}&#34;)
            print(f&#34;Contents:&#34;)
            pp(os.listdir(report_dir))

    target_dir = source_dir.replace(&#34;original&#34;, f&#34;{target_config.resolution}&#34;)

    data_df = preprocess_target_catalog(
        data_df=data_df, config=target_config, target_dir=target_dir
    )

    return data_df


def setup_configs(args):
    &#34;&#34;&#34;
    args:
        ::dataset_name
        ::resolution

    &#34;&#34;&#34;

    ic(args)
    config = Munch(
        dataset_name=args.dataset_name, y_col=&#34;family&#34;, threshold=0, resolution=&#34;original&#34;
    )

    target_config = Munch(
        dataset_name=args.dataset_name, resolution=args.resolution, y_col=&#34;family&#34;, threshold=0
    )
    config.target_shape = (3, config.resolution, config.resolution)
    target_config.target_shape = (3, target_config.resolution, target_config.resolution)

    return config, target_config


def validate_dataset(args) -&gt; bool:
    &#34;&#34;&#34;
    Compare source &amp; target datasets, optionally save comparison report to csv files, and return True only if they are identical.

    Return:
        (bool)
    &#34;&#34;&#34;

    config, target_config = setup_configs(args)
    data_df = warm_start_catalogs(
        config, target_config, run_clean=args.run_clean, save_report=args.save_report
    )
    return data_df.shape[0] == 0


def process(args):

    config, target_config = setup_configs(args)
    data_df = warm_start_catalogs(
        config, target_config, run_clean=args.run_clean, save_report=args.save_report
    )
    if data_df.shape[0] == 0:
        print(
            &#34;[SKIPPING PROCESS] Warm start skipping previously generated dataset:&#34;
            f&#34; {args.dataset_name} at resolution: {args.resolution} &#34;
        )
        return

    pp(target_config)
    resize_and_resave_dataset_parallel(
        data=data_df, resolution=target_config.resolution, parallel=True
    )


#########################################
#########################################


def main(args):

    dataset_names = args.dataset_name
    resolutions = args.resolution

    print(f&#34;[INITIATING] Multi-resolution dataset creation&#34;)
    print(f&#34;Datasets: {dataset_names}&#34;)
    print(f&#34;Resolutions: {resolutions}&#34;)

    num_trials = len(dataset_names) * len(resolutions)
    i = 0
    for dataset_name in dataset_names:
        for res in resolutions:
            args.dataset_name = dataset_name
            args.resolution = res
            print(f&#34;Trial #{i}/{num_trials-1}&#34;)

            if args.validate_only:
                print(&#34;[RUNNING] Dataset validation only.&#34;)
                validate_dataset(args)  # , save_report=args.save_report)
                i += 1
                continue
            print(
                &#34;[RUNNING DATASET PROCESSING].&#34;, f&#34;: ({time.strftime(&#39;%H:%M%p %Z on %b %d, %Y&#39;)})&#34;
            )
            print(f&#34;[ARGS] dataset_name: {args.dataset_name}, resolution: {args.resolution}&#34;)

            process(args)
            print(f&#34;[FINISHED PROCESSING]&#34;, f&#34;: ({time.strftime(&#39;%H:%M%p %Z on %b %d, %Y&#39;)})&#34;)
            print(&#34;[RUNNING POST-PROCESSING DATA VALIDATION]&#34;)
            validate_dataset(args)  # , save_report=args.save_report)
            i += 1

        print(
            f&#34;[DATASET COMPLETE] - Finished generating dataset={dataset_name} for&#34;
            f&#34; resolutions={resolutions}&#34;
        )

    print(&#34;=&#34; * 20)
    print(
        f&#34;[DATASET(s) COMPLETE] - Finished generating datasets={dataset_names} for&#34;
        f&#34; resolutions={resolutions}&#34;
    )


#########################################
#########################################


def cmdline_args(args=None):
    p = argparse.ArgumentParser(
        description=(
            &#34;Resize datasets to a new resolution, using the clever crop resize function. Requires&#34;
            &#34; source images to exist in {root_dir}/original/jpg and be organized into 1 subdir&#34;
            &#34; per-class.&#34;
        )
    )
    p.add_argument(
        &#34;-r&#34;,
        &#34;--resolution&#34;,
        dest=&#34;resolution&#34;,
        type=int,
        nargs=&#34;*&#34;,
        help=&#34;target resolution, images resized to (3, res, res).&#34;,
    )
    p.add_argument(
        &#34;-n&#34;,
        &#34;--dataset_name&#34;,
        dest=&#34;dataset_name&#34;,
        type=str,
        nargs=&#34;*&#34;,
        default=[&#34;Extant_Leaves&#34;],
        help=&#34;&#34;&#34;Base dataset_name to be used to query the source root_dir.&#34;&#34;&#34;,
    )
    p.add_argument(
        &#34;-a&#34;,
        &#34;--run-all&#34;,
        dest=&#34;run_all&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;Flag for when user would like to run through all default resolution arguments on a&#34;
            &#34; given dataset. Currently includes resolutions = [512, 1024, 1536, 2048].&#34;
        ),
    )
    p.add_argument(
        &#34;-skip&#34;,
        dest=&#34;skip&#34;,
        nargs=&#34;*&#34;,
        type=int,
        default=[],
        help=&#34;Explicitly skip any provided dataset resolutions.&#34;,
    )
    p.add_argument(
        &#34;--validate_only&#34;,
        dest=&#34;validate_only&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=(
            &#34;Flag for when user would like to only validate datasets without launching any of the&#34;
            &#34; multi-res processing stages..&#34;
        ),
    )
    p.add_argument(
        &#34;--save_report&#34;,
        dest=&#34;save_report&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Flag for when user would like to save csv files from the validation report.&#34;,
    )
    p.add_argument(
        &#34;-clean&#34;,
        &#34;--run_clean&#34;,
        dest=&#34;run_clean&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=(
            &#34;Pass this flag to run the target cleaning process prior to generating any new files.&#34;
            &#34; Removes any images from target dirs that are not represented in the source catalog.&#34;
        ),
    )
    p.add_argument(&#34;--num_workers&#34;, dest=&#34;num_workers&#34;, type=int, default=15)
    args = p.parse_args()  # args or sys.argv)

    print(args)
    if args.dataset_name == &#34;all&#34;:
        args.dataset_name = [&#34;Extant_Leaves&#34;, &#34;Florissant_Fossil&#34;, &#34;General_Fossil&#34;]
    if args.run_all:
        args.resolution = [512, 1024, 1536, 2048]
    args.skip = list(set(args.skip).intersection(set(args.resolution)))
    if len(args.skip) &gt; 0:
        print(f&#34;Skipping resolutions {args.skip}&#34;)
        args.resolution = sorted(list(set(args.resolution) - set(args.skip)))

    return args


if __name__ == &#34;__main__&#34;:

    args = cmdline_args()

    num_workers = args.num_workers
    pandarallel.initialize(nb_workers=num_workers, progress_bar=True)

    main(args)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imutils.generate_multires_images.clean_unused_images"><code class="name flex">
<span>def <span class="ident">clean_unused_images</span></span>(<span>data_df: pandas.core.frame.DataFrame, path_col: str = 'target_path')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_unused_images(data_df: pd.DataFrame, path_col: str = &#34;target_path&#34;):

    print(
        f&#34;Running [CLEAN] process on target dir. Removing {data_df.shape[0]} image files that do&#34;
        &#34; not have a corresponding match in source dir.&#34;
    )

    if data_df.shape[0] == 0:
        print(
            f&#34;[SKIP] Happily skipping clean_unused_images() since there were 0 extraneous images&#34;
            f&#34; detected in target that do not exist in source&#34;
        )
        return

    data_df[path_col].progress_apply(os.unlink)
    remaining_stragglers = data_df[path_col].progress_apply(os.path.isfile).sum()
    assert remaining_stragglers == 0
    print(f&#34;[SUCCESS] Removed {data_df.shape[0]} image files from where they dont belong!&#34;)</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.cmdline_args"><code class="name flex">
<span>def <span class="ident">cmdline_args</span></span>(<span>args=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cmdline_args(args=None):
    p = argparse.ArgumentParser(
        description=(
            &#34;Resize datasets to a new resolution, using the clever crop resize function. Requires&#34;
            &#34; source images to exist in {root_dir}/original/jpg and be organized into 1 subdir&#34;
            &#34; per-class.&#34;
        )
    )
    p.add_argument(
        &#34;-r&#34;,
        &#34;--resolution&#34;,
        dest=&#34;resolution&#34;,
        type=int,
        nargs=&#34;*&#34;,
        help=&#34;target resolution, images resized to (3, res, res).&#34;,
    )
    p.add_argument(
        &#34;-n&#34;,
        &#34;--dataset_name&#34;,
        dest=&#34;dataset_name&#34;,
        type=str,
        nargs=&#34;*&#34;,
        default=[&#34;Extant_Leaves&#34;],
        help=&#34;&#34;&#34;Base dataset_name to be used to query the source root_dir.&#34;&#34;&#34;,
    )
    p.add_argument(
        &#34;-a&#34;,
        &#34;--run-all&#34;,
        dest=&#34;run_all&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;Flag for when user would like to run through all default resolution arguments on a&#34;
            &#34; given dataset. Currently includes resolutions = [512, 1024, 1536, 2048].&#34;
        ),
    )
    p.add_argument(
        &#34;-skip&#34;,
        dest=&#34;skip&#34;,
        nargs=&#34;*&#34;,
        type=int,
        default=[],
        help=&#34;Explicitly skip any provided dataset resolutions.&#34;,
    )
    p.add_argument(
        &#34;--validate_only&#34;,
        dest=&#34;validate_only&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=(
            &#34;Flag for when user would like to only validate datasets without launching any of the&#34;
            &#34; multi-res processing stages..&#34;
        ),
    )
    p.add_argument(
        &#34;--save_report&#34;,
        dest=&#34;save_report&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=&#34;Flag for when user would like to save csv files from the validation report.&#34;,
    )
    p.add_argument(
        &#34;-clean&#34;,
        &#34;--run_clean&#34;,
        dest=&#34;run_clean&#34;,
        action=&#34;store_true&#34;,
        default=False,
        help=(
            &#34;Pass this flag to run the target cleaning process prior to generating any new files.&#34;
            &#34; Removes any images from target dirs that are not represented in the source catalog.&#34;
        ),
    )
    p.add_argument(&#34;--num_workers&#34;, dest=&#34;num_workers&#34;, type=int, default=15)
    args = p.parse_args()  # args or sys.argv)

    print(args)
    if args.dataset_name == &#34;all&#34;:
        args.dataset_name = [&#34;Extant_Leaves&#34;, &#34;Florissant_Fossil&#34;, &#34;General_Fossil&#34;]
    if args.run_all:
        args.resolution = [512, 1024, 1536, 2048]
    args.skip = list(set(args.skip).intersection(set(args.resolution)))
    if len(args.skip) &gt; 0:
        print(f&#34;Skipping resolutions {args.skip}&#34;)
        args.resolution = sorted(list(set(args.resolution) - set(args.skip)))

    return args</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(args):

    dataset_names = args.dataset_name
    resolutions = args.resolution

    print(f&#34;[INITIATING] Multi-resolution dataset creation&#34;)
    print(f&#34;Datasets: {dataset_names}&#34;)
    print(f&#34;Resolutions: {resolutions}&#34;)

    num_trials = len(dataset_names) * len(resolutions)
    i = 0
    for dataset_name in dataset_names:
        for res in resolutions:
            args.dataset_name = dataset_name
            args.resolution = res
            print(f&#34;Trial #{i}/{num_trials-1}&#34;)

            if args.validate_only:
                print(&#34;[RUNNING] Dataset validation only.&#34;)
                validate_dataset(args)  # , save_report=args.save_report)
                i += 1
                continue
            print(
                &#34;[RUNNING DATASET PROCESSING].&#34;, f&#34;: ({time.strftime(&#39;%H:%M%p %Z on %b %d, %Y&#39;)})&#34;
            )
            print(f&#34;[ARGS] dataset_name: {args.dataset_name}, resolution: {args.resolution}&#34;)

            process(args)
            print(f&#34;[FINISHED PROCESSING]&#34;, f&#34;: ({time.strftime(&#39;%H:%M%p %Z on %b %d, %Y&#39;)})&#34;)
            print(&#34;[RUNNING POST-PROCESSING DATA VALIDATION]&#34;)
            validate_dataset(args)  # , save_report=args.save_report)
            i += 1

        print(
            f&#34;[DATASET COMPLETE] - Finished generating dataset={dataset_name} for&#34;
            f&#34; resolutions={resolutions}&#34;
        )

    print(&#34;=&#34; * 20)
    print(
        f&#34;[DATASET(s) COMPLETE] - Finished generating datasets={dataset_names} for&#34;
        f&#34; resolutions={resolutions}&#34;
    )</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.preprocess_target_catalog"><code class="name flex">
<span>def <span class="ident">preprocess_target_catalog</span></span>(<span>data_df: pandas.core.frame.DataFrame, config, target_dir: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><h2 id="arguments">Arguments</h2>
<p>config (NameSpace):
::dataset_name
::y_col
::threshold
::resolution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_target_catalog(data_df: pd.DataFrame, config, target_dir: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;

    Arguments:
        config (NameSpace):
            ::dataset_name
            ::y_col
            ::threshold
            ::resolution

    &#34;&#34;&#34;
    target_path = None
    if data_df.shape[0] &gt; 0:
        target_path = data_df.apply(
            lambda x: str(Path(target_dir, x.family, Path(x.path).name)), axis=1
        )
    data_df = data_df.assign(target_path=target_path)
    family_dirs = list(set(data_df.target_path.apply(lambda x: str(Path(x).parent))))
    [os.makedirs(subdir, exist_ok=True) for subdir in family_dirs]

    return data_df</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(args):

    config, target_config = setup_configs(args)
    data_df = warm_start_catalogs(
        config, target_config, run_clean=args.run_clean, save_report=args.save_report
    )
    if data_df.shape[0] == 0:
        print(
            &#34;[SKIPPING PROCESS] Warm start skipping previously generated dataset:&#34;
            f&#34; {args.dataset_name} at resolution: {args.resolution} &#34;
        )
        return

    pp(target_config)
    resize_and_resave_dataset_parallel(
        data=data_df, resolution=target_config.resolution, parallel=True
    )</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.query_and_preprocess_catalog"><code class="name flex">
<span>def <span class="ident">query_and_preprocess_catalog</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="arguments">Arguments</h2>
<p>config (NameSpace):
::dataset_name
::y_col
::threshold
::resolution
::version, default="latest"</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_and_preprocess_catalog(config):
    &#34;&#34;&#34;

    Arguments:
        config (NameSpace):
            ::dataset_name
            ::y_col
            ::threshold
            ::resolution
            ::version, default=&#34;latest&#34;

    &#34;&#34;&#34;
    tag = available_datasets.query_tags(
        dataset_name=config.dataset_name,
        y_col=config.y_col,
        threshold=config.threshold,
        resolution=config.resolution,
    )
    if config.get(&#34;version&#34;) in [None, &#34;latest&#34;]:
        root_dir = available_datasets.get_latest(tag)
    else:
        root_dir = available_datasets.get(tag, config.version)
    print(f&#34;Tag: {tag}&#34;)
    print(f&#34;Root Dir: {root_dir}&#34;)
    print(f&#34;Version: {config.get(&#39;version&#39;)}&#34;)
    if not os.path.isdir(root_dir):
        print(
            f&#34;[INFO] There is not yet any directory located at: {root_dir}. Skipping dataset&#34;
            &#34; validation.&#34;
        )
        return pd.DataFrame(), root_dir
    data_df = parse_df_catalog_from_image_directory(
        root_dir=root_dir, dataset_name=config.dataset_name
    )

    return data_df, root_dir</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.resize_and_resave_dataset"><code class="name flex">
<span>def <span class="ident">resize_and_resave_dataset</span></span>(<span>data, res: int = 512)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resize_and_resave_dataset(data, res: int = 512):
    num_samples = len(data)
    target_shape = (3, res, res)
    for sample_idx in trange(num_samples, desc=f&#34;Smart Crop {dataset_name}&#34;):

        row = data.iloc[sample_idx]
        source_path = row.path
        target_path = row.target_path

        resize_and_save_img(img=source_path, target_path=target_path, target_shape=(3, res, res))</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.resize_and_resave_dataset_parallel"><code class="name flex">
<span>def <span class="ident">resize_and_resave_dataset_parallel</span></span>(<span>data, resolution: int = 512, parallel=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resize_and_resave_dataset_parallel(data, resolution: int = 512, parallel=True):
    target_shape = (3, resolution, resolution)
    if parallel:
        data.parallel_apply(
            lambda x: resize_and_save_img(
                img=x[&#34;path&#34;], target_path=x[&#34;target_path&#34;], target_shape=target_shape
            ),
            axis=1,
        )
    else:
        data.apply(
            lambda x: resize_and_save_img(
                img=x[&#34;path&#34;], target_path=x[&#34;target_path&#34;], target_shape=target_shape
            ),
            axis=1,
        )

    return data</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.resize_and_save_img"><code class="name flex">
<span>def <span class="ident">resize_and_save_img</span></span>(<span>img: Union[str, pathlib.Path, PIL.Image.Image], target_path: str, target_shape=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resize_and_save_img(
    img: Union[str, Path, PIL.Image.Image], target_path: str, target_shape=None
):
    if os.path.isfile(target_path):
        return

    if isinstance(img, (str, Path)):
        img = Image.open(img)
    img = transforms.ToTensor()(img)
    target_img = clever_crop(img, target_shape=target_shape)
    target_img = (target_img * 255.0).to(torch.uint8)
    try:
        torchvision.io.write_jpeg(target_img, target_path, quality=100)
    except Exception as e:
        print(target_path, e)
    assert os.path.isfile(target_path)</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.setup_configs"><code class="name flex">
<span>def <span class="ident">setup_configs</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"><p>args:
::dataset_name
::resolution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_configs(args):
    &#34;&#34;&#34;
    args:
        ::dataset_name
        ::resolution

    &#34;&#34;&#34;

    ic(args)
    config = Munch(
        dataset_name=args.dataset_name, y_col=&#34;family&#34;, threshold=0, resolution=&#34;original&#34;
    )

    target_config = Munch(
        dataset_name=args.dataset_name, resolution=args.resolution, y_col=&#34;family&#34;, threshold=0
    )
    config.target_shape = (3, config.resolution, config.resolution)
    target_config.target_shape = (3, target_config.resolution, target_config.resolution)

    return config, target_config</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.validate_dataset"><code class="name flex">
<span>def <span class="ident">validate_dataset</span></span>(<span>args) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Compare source &amp; target datasets, optionally save comparison report to csv files, and return True only if they are identical.</p>
<h2 id="return">Return</h2>
<p>(bool)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_dataset(args) -&gt; bool:
    &#34;&#34;&#34;
    Compare source &amp; target datasets, optionally save comparison report to csv files, and return True only if they are identical.

    Return:
        (bool)
    &#34;&#34;&#34;

    config, target_config = setup_configs(args)
    data_df = warm_start_catalogs(
        config, target_config, run_clean=args.run_clean, save_report=args.save_report
    )
    return data_df.shape[0] == 0</code></pre>
</details>
</dd>
<dt id="imutils.generate_multires_images.warm_start_catalogs"><code class="name flex">
<span>def <span class="ident">warm_start_catalogs</span></span>(<span>config, target_config, run_clean: bool = False, save_report: bool = False) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Save time by skipping previously processed files and keeping only the yet-to-be processed catalog.</p>
<p>Query and Preprocess catalogs from source and target configs,
return only the rows that are unique to the source location,
then format the remainder in preparation to run the multi-res image generation script (i.e. source_path -&gt; "path", target_path -&gt; "target_path" columns)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def warm_start_catalogs(
    config, target_config, run_clean: bool = False, save_report: bool = False
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Save time by skipping previously processed files and keeping only the yet-to-be processed catalog.

    Query and Preprocess catalogs from source and target configs,
    return only the rows that are unique to the source location,
    then format the remainder in preparation to run the multi-res image generation script (i.e. source_path -&gt; &#34;path&#34;, target_path -&gt; &#34;target_path&#34; columns)

    &#34;&#34;&#34;
    source_catalog, source_dir = query_and_preprocess_catalog(config)
    target_catalog, target_dir = query_and_preprocess_catalog(target_config)

    if target_catalog.shape[0] &gt; 0:
        target_paths = target_catalog.apply(
            lambda x: str(Path(target_dir, x.relative_path)), axis=1
        )
        target_catalog = target_catalog.assign(path=target_paths)

    if target_catalog.shape[0] == 0:
        data_df = source_catalog
    else:
        shared, diff, source_only, target_only = diff_dataset_catalogs(
            source_catalog=source_catalog, target_catalog=target_catalog
        )

        if run_clean:
            clean_unused_images(data_df=target_only, path_col=&#34;path&#34;)
        print(shared.shape, diff.shape, source_only.shape, target_only.shape)
        data_df = source_only

        if save_report is not None:
            report_dir = Path(target_dir).parent / &#34;audit_logs&#34;
            os.makedirs(report_dir, exist_ok=True)
            shared.to_csv(Path(report_dir, &#34;shared_catalog.csv&#34;))
            diff.to_csv(Path(report_dir, &#34;diff_catalog.csv&#34;))
            print(f&#34;Exported catalog validation logs to directory: {report_dir}&#34;)
            print(f&#34;Contents:&#34;)
            pp(os.listdir(report_dir))

    target_dir = source_dir.replace(&#34;original&#34;, f&#34;{target_config.resolution}&#34;)

    data_df = preprocess_target_catalog(
        data_df=data_df, config=target_config, target_dir=target_dir
    )

    return data_df</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#run-all-3-datasets-for-all-4-resolutions-in-serial">Run all 3 datasets for all 4 resolutions in serial</a></li>
<li><a href="#clean-create-all-symlink-dirs-for-all-thresholds-and-all-datasets">Clean, &amp; create, all symlink dirs for all thresholds and all datasets.</a></li>
<li><a href="#run-1-dataset-for-all-4-resolutions-in-serial">Run 1 dataset for all 4 resolutions in serial</a></li>
<li><a href="#clean-create-all-symlink-dirs">Clean, &amp; create, all symlink dirs.</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils" href="index.html">imutils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imutils.generate_multires_images.clean_unused_images" href="#imutils.generate_multires_images.clean_unused_images">clean_unused_images</a></code></li>
<li><code><a title="imutils.generate_multires_images.cmdline_args" href="#imutils.generate_multires_images.cmdline_args">cmdline_args</a></code></li>
<li><code><a title="imutils.generate_multires_images.main" href="#imutils.generate_multires_images.main">main</a></code></li>
<li><code><a title="imutils.generate_multires_images.preprocess_target_catalog" href="#imutils.generate_multires_images.preprocess_target_catalog">preprocess_target_catalog</a></code></li>
<li><code><a title="imutils.generate_multires_images.process" href="#imutils.generate_multires_images.process">process</a></code></li>
<li><code><a title="imutils.generate_multires_images.query_and_preprocess_catalog" href="#imutils.generate_multires_images.query_and_preprocess_catalog">query_and_preprocess_catalog</a></code></li>
<li><code><a title="imutils.generate_multires_images.resize_and_resave_dataset" href="#imutils.generate_multires_images.resize_and_resave_dataset">resize_and_resave_dataset</a></code></li>
<li><code><a title="imutils.generate_multires_images.resize_and_resave_dataset_parallel" href="#imutils.generate_multires_images.resize_and_resave_dataset_parallel">resize_and_resave_dataset_parallel</a></code></li>
<li><code><a title="imutils.generate_multires_images.resize_and_save_img" href="#imutils.generate_multires_images.resize_and_save_img">resize_and_save_img</a></code></li>
<li><code><a title="imutils.generate_multires_images.setup_configs" href="#imutils.generate_multires_images.setup_configs">setup_configs</a></code></li>
<li><code><a title="imutils.generate_multires_images.validate_dataset" href="#imutils.generate_multires_images.validate_dataset">validate_dataset</a></code></li>
<li><code><a title="imutils.generate_multires_images.warm_start_catalogs" href="#imutils.generate_multires_images.warm_start_catalogs">warm_start_catalogs</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>