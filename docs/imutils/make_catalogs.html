<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.make_catalogs API documentation</title>
<meta name="description" content="lightning_hydra_classifiers/data/utils/make_catalogs.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.make_catalogs</code></h1>
</header>
<section id="section-intro">
<p>lightning_hydra_classifiers/data/utils/make_catalogs.py</p>
<p>Author: Jacob A Rose
Created: Wednesday July 28th, 2021</p>
<p>generate experiment directories containing csv datasets and yaml configs</p>
<p>Currently covers:
- leavesdb v0_3
- leavesdb v1_0</p>
<p>Work In Progress:
- leavesdb v1_1</p>
<h2 id="todo">Todo</h2>
<ul>
<li>Add to a make file in base directory</li>
<li>Add more flexible configuration</li>
</ul>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/make_catalogs.py" &ndash;all</p>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/make_catalogs.py" &ndash;make_original</p>
<p>python "/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/make_catalogs.py" &ndash;make_extant</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;

lightning_hydra_classifiers/data/utils/make_catalogs.py

Author: Jacob A Rose
Created: Wednesday July 28th, 2021

generate experiment directories containing csv datasets and yaml configs

Currently covers:
    - leavesdb v0_3
    - leavesdb v1_0

Work In Progress:
    - leavesdb v1_1

TODO:
    - Add to a make file in base directory
    - Add more flexible configuration


python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/make_catalogs.py&#34; --all


python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/make_catalogs.py&#34; --make_original


python &#34;/media/data/jacob/GitHub/lightning-hydra-classifiers/lightning_hydra_classifiers/data/utils/make_catalogs.py&#34; --make_extant


&#34;&#34;&#34;

import argparse
import collections
import os
import shutil
from copy import deepcopy
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import *

import pandas as pd
import torch
# import torchdata
import torchvision

# from lightning_hydra_classifiers.utils.common_utils import LabelEncoder
from IPython.display import display

# from lightning_hydra_classifiers.data.utils import catalog_registry
# from lightning_hydra_classifiers.data.datasets.common import (
#     ETL,
#     CSVDataset,
#     CSVDatasetConfig,
#     ImageFileDataset,
#     ImageFileDatasetConfig,
#     PathSchema,
#     SampleSchema,
# )
# from lightning_hydra_classifiers.utils.common_utils import (
#     DataSplitter,
#     LabelEncoder,
#     trainval_split,
#     trainvaltest_split,
# )
from omegaconf import DictConfig
from PIL import Image

# from lightning_hydra_classifiers.data.common import PathSchema
# from lightning_hydra_classifiers.utils.dataset_management_utils import Extract as ExtractBase
# from lightning_hydra_classifiers.utils.etl_utils import ETL as ETLBase


#######################################################
#######################################################


def create_dataset_A_in_B(dataset_A, dataset_B) -&gt; pd.DataFrame:

    A_w_B = dataset_A.intersection(dataset_B)

    columns = [*[col for col in A_w_B.columns if col.endswith(&#34;_x&#34;)], *[&#34;catalog_number&#34;]]
    A_in_B = A_w_B.reset_index()[columns].sort_values(&#34;catalog_number&#34;)

    print(f&#34;A_in_B.columns: {A_in_B.columns}&#34;)
    A_in_B = A_in_B.rename(columns={col: col.split(&#34;_x&#34;)[0] for col in A_in_B.columns})

    return A_in_B


#######################################################
#######################################################


def export_dataset_catalog_configuration(
    output_dir: str = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v1_0&#34;,
    base_dataset_name=&#34;Extant_Leaves&#34;,
    threshold=100,
    resolution=512,
    version: str = &#34;v1_0&#34;,
    path_schema: str = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;,
):

    image_file_config = ImageFileDatasetConfig(
        base_dataset_name=base_dataset_name,
        class_type=&#34;family&#34;,
        threshold=threshold,
        resolution=resolution,
        version=version,
        path_schema=path_schema,
    )

    out_dir = os.path.join(output_dir, image_file_config.full_name)
    os.makedirs(out_dir, exist_ok=True)

    csv_out_path = os.path.join(out_dir, f&#34;{image_file_config.full_name}-full_dataset.csv&#34;)
    image_file_config_out_path = os.path.join(out_dir, &#34;ImageFileDataset-config.yaml&#34;)
    csv_config_out_path = os.path.join(out_dir, &#34;CSVDataset-config.yaml&#34;)

    dataset = ImageFileDataset.from_config(image_file_config, subset_keys=[&#34;all&#34;])
    ETL.df2csv(dataset.samples_df, path=csv_out_path)
    image_file_config.save(image_file_config_out_path)

    csv_config = CSVDatasetConfig(
        full_name=image_file_config.full_name, data_path=csv_out_path, subset_key=&#34;all&#34;
    )

    csv_config.save(csv_config_out_path)

    print(f&#34;[FINISHED] DATASET FULL NAME: {csv_config.full_name}&#34;)
    print(f&#34;Newly created dataset assets located at:  {out_dir}&#34;)

    return dataset, image_file_config, csv_config


##############################################


def export_composite_dataset_catalog_configuration(
    output_dir: str = &#34;.&#34;,
    csv_cfg_path_A: str = None,
    csv_cfg_path_B: str = None,
    composition: str = &#34;-&#34;,
) -&gt; Tuple[&#34;CSVDataset&#34;, &#34;CSVDatasetConfig&#34;]:

    csv_config_A = CSVDatasetConfig.load(path=csv_cfg_path_A)
    csv_config_B = CSVDatasetConfig.load(path=csv_cfg_path_B)

    dataset_A = CSVDataset.from_config(csv_config_A)
    dataset_B = CSVDataset.from_config(csv_config_B)
    print(f&#34;num_samples A: {len(dataset_A)}&#34;)
    print(f&#34;num_samples B: {len(dataset_B)}&#34;)

    print(f&#34;producing composition: {composition}&#34;)
    if composition == &#34;-&#34;:
        dataset_A_composed_B = dataset_A - dataset_B
        full_name = f&#34;{csv_config_A.full_name}_minus_{csv_config_B.full_name}&#34;
        out_dir = os.path.join(output_dir, full_name)
        os.makedirs(out_dir, exist_ok=True)
        print(f&#34;num_samples A-B: {len(dataset_A_composed_B)}&#34;)

    if composition == &#34;intersection&#34;:
        dataset_A_composed_B = dataset_A.intersection(dataset_B)
        full_name = f&#34;{csv_config_A.full_name}_w_{csv_config_B.full_name}&#34;
        print(f&#34;num_samples A_w_B: {len(dataset_A_composed_B)}&#34;)

        out_dir = os.path.join(output_dir, full_name)
        os.makedirs(out_dir, exist_ok=True)

    ########################################
    A_in_B = create_dataset_A_in_B(dataset_A, dataset_B)
    csv_dataset_pathname = f&#34;{csv_config_A.full_name}_in_{csv_config_B.full_name}&#34;
    csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
    ETL.df2csv(A_in_B, path=csv_dataset_out_path)
    A_in_B_config = CSVDatasetConfig(
        full_name=csv_dataset_pathname, data_path=csv_dataset_out_path, subset_key=&#34;all&#34;
    )
    csv_dataset_config_out_path = os.path.join(out_dir, f&#34;A_in_B-CSVDataset-config.yaml&#34;)
    A_in_B_config.save(csv_dataset_config_out_path)

    #########################################
    B_in_A = create_dataset_A_in_B(dataset_B, dataset_A)

    csv_dataset_pathname = f&#34;{csv_config_B.full_name}_in_{csv_config_A.full_name}&#34;
    csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
    ETL.df2csv(B_in_A, path=csv_dataset_out_path)
    B_in_A_config = CSVDatasetConfig(
        full_name=csv_dataset_pathname, data_path=csv_dataset_out_path, subset_key=&#34;all&#34;
    )

    csv_dataset_config_out_path = os.path.join(out_dir, f&#34;B_in_A-CSVDataset-config.yaml&#34;)
    B_in_A_config.save(csv_dataset_config_out_path)

    #########################################

    inputs_dir = os.path.join(out_dir, &#34;inputs&#34;)
    os.makedirs(os.path.join(inputs_dir, &#34;A&#34;), exist_ok=True)
    os.makedirs(os.path.join(inputs_dir, &#34;B&#34;), exist_ok=True)
    shutil.copyfile(csv_cfg_path_A, os.path.join(inputs_dir, &#34;A&#34;, Path(csv_cfg_path_A).name))
    shutil.copyfile(csv_cfg_path_B, os.path.join(inputs_dir, &#34;B&#34;, Path(csv_cfg_path_B).name))

    csv_dataset_pathname = f&#34;{full_name}-full_dataset&#34;
    csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
    ETL.df2csv(dataset_A_composed_B, path=csv_dataset_out_path)
    ####################
    ####################
    csv_dataset_config_out_path = os.path.join(out_dir, f&#34;CSVDataset-config.yaml&#34;)
    A_composed_B_config = CSVDatasetConfig(
        full_name=full_name, data_path=csv_dataset_out_path, subset_key=&#34;all&#34;
    )
    A_composed_B_config.save(csv_dataset_config_out_path)

    print(f&#34;[FINISHED] DATASET: {full_name}&#34;)
    print(f&#34;Newly created dataset assets located at:  {out_dir}&#34;)

    if composition == &#34;-&#34;:
        dataset_A_composed_B = CSVDataset.from_config(A_composed_B_config)
        return dataset_A_composed_B, A_composed_B_config

    if composition == &#34;intersection&#34;:
        return (A_in_B, B_in_A), A_composed_B_config


############################################
############################################
############################################
############################################


def make_all_original(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;&#34;  # original&#34;
    #                           &#34;Extant_Leaves&#34;,
    #                           &#34;Florissant_Fossil&#34;,
    #                           &#34;General_Fossil&#34;]

    resolution = &#34;original&#34;
    path_schema = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;

    print(f&#34;Beginning make_all_original()&#34;)
    #     for base_dataset_name in base_dataset_names:
    export_dataset_catalog_configuration(
        output_dir=output_dir,
        base_dataset_name=base_dataset_name,
        threshold=0,
        resolution=resolution,
        version=version,
        path_schema=path_schema,
    )

    print(f&#34;FINISHED ALL IN original datasets&#34;)
    print(&#34;==&#34; * 15)


def make_fossil(args):

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;Fossil&#34;
    thresholds = [None, 3]
    resolutions = [512, 1024]
    path_schema = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;

    print(
        f&#34;Beginning make_fossil() for {len(resolutions)}x resolutions and {len(thresholds)}x&#34;
        &#34; thresholds&#34;
    )

    for threshold in thresholds:
        for resolution in resolutions:
            export_dataset_catalog_configuration(
                output_dir=output_dir,
                base_dataset_name=base_dataset_name,
                threshold=threshold,
                resolution=resolution,
                version=version,
                path_schema=path_schema,
            )

    print(f&#34;FINISHED ALL IN Fossil&#34;)
    print(&#34;==&#34; * 15)


def make_extant(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;Extant_Leaves&#34;
    thresholds = [None, 3, 10, 100]
    resolutions = [512, 1024]
    path_schema = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;

    print(
        f&#34;Beginning make_extant() for {len(resolutions)}x resolutions and {len(thresholds)}x&#34;
        &#34; thresholds&#34;
    )
    for threshold in thresholds:
        for resolution in resolutions:
            export_dataset_catalog_configuration(
                output_dir=output_dir,
                base_dataset_name=base_dataset_name,
                threshold=threshold,
                resolution=resolution,
                version=version,
                path_schema=path_schema,
            )

    print(f&#34;FINISHED ALL IN Extant_Leaves&#34;)
    print(&#34;==&#34; * 15)


def make_pnas(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;PNAS&#34;
    thresholds = [100]
    resolutions = [512, 1024]
    path_schema = &#34;{family}_{genus}_{species}_{catalog_number}&#34;

    print(
        f&#34;Beginning make_pnas() for {len(resolutions)}x resolutions and {len(thresholds)}x&#34;
        &#34; thresholds&#34;
    )
    for threshold in thresholds:
        for resolution in resolutions:
            export_dataset_catalog_configuration(
                output_dir=output_dir,
                base_dataset_name=base_dataset_name,
                threshold=threshold,
                resolution=resolution,
                version=version,
                path_schema=path_schema,
            )

    print(f&#34;FINISHED ALL IN PNAS&#34;)
    print(&#34;==&#34; * 15)


def make_extant_minus_pnas(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_names = {&#34;A&#34;: &#34;Extant_Leaves&#34;, &#34;B&#34;: &#34;PNAS&#34;}
    thresholds = [{&#34;A&#34;: 100, &#34;B&#34;: 100}, {&#34;A&#34;: 10, &#34;B&#34;: 100}]
    resolutions = [512, 1024]
    class_type = &#34;family&#34;

    for threshold in thresholds:
        for resolution in resolutions:
            dataset_full_names = {
                &#34;A&#34;: &#34;_&#34;.join([base_names[&#34;A&#34;], class_type, str(threshold[&#34;A&#34;]), str(resolution)]),
                &#34;B&#34;: &#34;_&#34;.join([base_names[&#34;B&#34;], class_type, str(threshold[&#34;B&#34;]), str(resolution)]),
            }

            csv_cfg_path_A = os.path.join(
                output_dir, dataset_full_names[&#34;A&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            csv_cfg_path_B = os.path.join(
                output_dir, dataset_full_names[&#34;B&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            dataset, cfg = export_composite_dataset_catalog_configuration(
                output_dir=output_dir,
                csv_cfg_path_A=csv_cfg_path_A,
                csv_cfg_path_B=csv_cfg_path_B,
                composition=&#34;-&#34;,
            )

    print(f&#34;FINISHED ALL IN Extant-PNAS&#34;)
    print(&#34;==&#34; * 15)


def make_pnas_minus_extant(args):

    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;
    output_dir = args.output_dir

    base_names = {&#34;A&#34;: &#34;PNAS&#34;, &#34;B&#34;: &#34;Extant_Leaves&#34;}
    thresholds = [{&#34;A&#34;: 100, &#34;B&#34;: 100}, {&#34;A&#34;: 100, &#34;B&#34;: 10}]
    resolutions = [512, 1024]
    class_type = &#34;family&#34;

    for threshold in thresholds:
        for resolution in resolutions:
            dataset_full_names = {
                &#34;A&#34;: &#34;_&#34;.join([base_names[&#34;A&#34;], class_type, str(threshold[&#34;A&#34;]), str(resolution)]),
                &#34;B&#34;: &#34;_&#34;.join([base_names[&#34;B&#34;], class_type, str(threshold[&#34;B&#34;]), str(resolution)]),
            }

            csv_cfg_path_A = os.path.join(
                output_dir, dataset_full_names[&#34;A&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            csv_cfg_path_B = os.path.join(
                output_dir, dataset_full_names[&#34;B&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            dataset, cfg = export_composite_dataset_catalog_configuration(
                output_dir=output_dir,
                csv_cfg_path_A=csv_cfg_path_A,
                csv_cfg_path_B=csv_cfg_path_B,
                composition=&#34;-&#34;,
            )

    print(f&#34;FINISHED ALL IN Extant-PNAS&#34;)
    print(&#34;==&#34; * 15)


def make_extant_w_pnas(args):

    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;
    output_dir = args.output_dir

    base_names = {&#34;A&#34;: &#34;Extant_Leaves&#34;, &#34;B&#34;: &#34;PNAS&#34;}
    thresholds = [{&#34;A&#34;: 100, &#34;B&#34;: 100}, {&#34;A&#34;: 10, &#34;B&#34;: 100}]
    resolutions = [512, 1024]
    class_type = &#34;family&#34;

    for threshold in thresholds:
        for resolution in resolutions:
            dataset_full_names = {
                &#34;A&#34;: &#34;_&#34;.join([base_names[&#34;A&#34;], class_type, str(threshold[&#34;A&#34;]), str(resolution)]),
                &#34;B&#34;: &#34;_&#34;.join([base_names[&#34;B&#34;], class_type, str(threshold[&#34;B&#34;]), str(resolution)]),
            }

            csv_cfg_path_A = os.path.join(
                output_dir, dataset_full_names[&#34;A&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            csv_cfg_path_B = os.path.join(
                output_dir, dataset_full_names[&#34;B&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            dataset, cfg = export_composite_dataset_catalog_configuration(
                output_dir=output_dir,
                csv_cfg_path_A=csv_cfg_path_A,
                csv_cfg_path_B=csv_cfg_path_B,
                composition=&#34;intersection&#34;,
            )

    print(f&#34;FINISHED ALL IN Extant_w_PNAS&#34;)
    print(&#34;==&#34; * 15)


CSV_CATALOG_DIR_V0_3 = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;
CSV_CATALOG_DIR_V1_0 = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v1_0&#34;
EXPERIMENTAL_DATASETS_DIR = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/experimental_datasets&#34;


def cmdline_args():
    p = argparse.ArgumentParser(
        description=(
            &#34;Export a series of dataset artifacts (containing csv catalog, yml config, json labels)&#34;
            &#34; for each dataset, provided that the corresponding images are pointed to by one of the&#34;
            &#34; file paths hard-coded in catalog_registry.py.&#34;
        )
    )
    p.add_argument(
        &#34;-o&#34;,
        &#34;--output_dir&#34;,
        dest=&#34;output_dir&#34;,
        type=str,
        default=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v1_0&#34;,
        help=(
            &#34;Output root directory. Each unique dataset will be allotted its own subdirectory&#34;
            &#34; within this root dir.&#34;
        ),
    )
    p.add_argument(
        &#34;-a&#34;,
        &#34;--all&#34;,
        dest=&#34;make_all&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production datasets in the most&#34;
            &#34; recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    p.add_argument(
        &#34;-v&#34;,
        &#34;--version&#34;,
        dest=&#34;version&#34;,
        type=str,
        default=&#34;v1_0&#34;,
        help=&#34;Available dataset versions: [v0_3, v1_0].&#34;,
    )
    p.add_argument(
        &#34;--fossil&#34;,
        dest=&#34;make_fossil&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all configurations of the combined Florissant +&#34;
            &#34; General Fossil collections.&#34;
        ),
    )
    p.add_argument(
        &#34;--extant&#34;,
        dest=&#34;make_extant&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: Extant_Leaves&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    p.add_argument(
        &#34;--pnas&#34;,
        dest=&#34;make_pnas&#34;,
        action=&#34;store_true&#34;,
        help=(
            f&#34;If user provides this flag, produce all currently in-production&#34;
            f&#34; resolutions+thresholds in the dataset: PNAS&#39;s most recent version (currently ==&#34;
            f&#34; &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--extant-pnas&#34;,
        dest=&#34;make_extant_minus_pnas&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: Extant-PNAS&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--pnas-extant&#34;,
        dest=&#34;make_pnas_minus_extant&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: PNAS-Extant&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--extant-w-pnas&#34;,
        dest=&#34;make_extant_w_pnas&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: Extant_w_PNAS&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--original&#34;,
        dest=&#34;make_all_original&#34;,
        action=&#34;store_true&#34;,
        help=(
            f&#34;If user provides this flag, produce all currently in-production&#34;
            f&#34; resolutions+thresholds in the original datasets&#39; () most recent version (currently&#34;
            f&#34; == &#39;v1_0&#39;). Note that this excludes PNAS.&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    return p.parse_args()


# make_all_original

if __name__ == &#34;__main__&#34;:
    #     import sys
    #     args = sys.argv

    args = cmdline_args()

    os.makedirs(args.output_dir, exist_ok=True)

    if args.make_fossil or args.make_all:
        make_fossil(args)
    if args.make_extant or args.make_all:
        make_extant(args)
    if args.make_pnas or args.make_all:
        make_pnas(args)
    if args.make_extant_minus_pnas or args.make_all:
        make_extant_minus_pnas(args)
    if args.make_pnas_minus_extant or args.make_all:
        make_pnas_minus_extant(args)
    if args.make_extant_w_pnas or args.make_all:
        make_extant_w_pnas(args)
    if args.make_all_original or args.make_all:
        make_all_original(args)


#######################################################
#######################################################
#######################################################
#######################################################


#######################################################
#######################################################
#######################################################
#######################################################


# totensor: Callable = torchvision.transforms.ToTensor()
# def toPIL(img: torch.Tensor, mode=&#34;RGB&#34;) -&gt; Callable:
#     return torchvision.transforms.ToPILImage(mode)(img)

# class Extract(ExtractBase):


#     @classmethod
#     def export_dataset_state(cls,
#                              output_dir: Union[str, Path],
#                              df: pd.DataFrame=None,
#                              config: DictConfig=None,
#                              encoder: LabelEncoder=None,
#                              dataset_name: Optional[str]=&#34;dataset&#34;
#                              ) -&gt; None:

#         paths = {ftype: str(output_dir / str(dataset_name + ext)) for ftype, ext in cls.data_file_ext_maps.items()}

#         output_dir = Path(output_dir)
#         if isinstance(df, pd.DataFrame):
#             cls.df2csv(df = df,
#                        path = paths[&#34;df&#34;])
#             if config:
#                 config.data_path = paths[&#34;df&#34;]
#         if isinstance(encoder, LabelEncoder):
#             cls.labels2json(encoder=encoder,
#                             path = paths[&#34;encoder&#34;])
#             if config:
#                 config.label_encoder_path = paths[&#34;encoder&#34;]
#         if isinstance(config, CSVDatasetConfig):
#             config.save(path = paths[&#34;config&#34;])
# #             cls.config2yaml(config=config,
# #                             path = paths[&#34;config&#34;])


#     @classmethod
#     def import_dataset_state(cls,
#                              data_dir: Optional[Union[str, Path]]=None,
#                              config_path: Optional[Union[Path, str]]=None,
#                             ) -&gt; Tuple[&#34;CSVDataset&#34;, &#34;CSVDatasetConfig&#34;]:
#         if (not os.path.exists(str(data_dir))) and (not os.path.exists(config_path)):
#             raise ValueError(&#34;Either data_dir or config_path must be existing paths&#34;)

#         if os.path.isdir(str(data_dir)):
#             data_dir = Path(data_dir)
#         paths = {}

#         # import config yaml file
#         if os.path.isfile(str(config_path)):
#             paths[&#39;config&#39;] = config_path
# #             config = cls.config_from_yaml(path = paths[&#34;config&#34;])
#             config = CSVDatasetConfig.load(path = paths[&#34;config&#34;])
#             if hasattr(config, &#34;data_path&#34;):
#                 paths[&#34;df&#34;] = str(config.data_path)
#             if hasattr(config, &#34;label_encoder_path&#34;):
#                 paths[&#34;encoder&#34;] = str(config.label_encoder_path)
#             data_dir = Path(os.path.dirname(config_path))

#         for ftype, ext in cls.data_file_ext_maps.items():
#             if ftype not in paths:
#                 paths[ftype] = str(list(data_dir.glob(&#34;*&#34; + ext))[0])


#         config.data_path = str(paths[&#34;df&#34;])
#         config.label_encoder_path = str(paths[&#34;encoder&#34;])
#         if os.path.isfile(paths[&#34;encoder&#34;]):
#             # import label encodings json file if it exists
#             label_encoder = cls.labels_from_json(path = paths[&#34;encoder&#34;])

#         # import dataset samples from a csv file as a CustomDataset/CSVDataset object
#         dataset = CSVDataset.from_config(config,
#                                          eager_encode_targets=True)
#         dataset.setup(samples_df=dataset.samples_df,
#                       label_encoder=label_encoder,
#                       fit_targets=True)

#         return dataset, config


# @dataclass
# class BaseDatasetConfig:

#     def save(self,
#              path: Union[str, Path]) -&gt; None:

#         cfg = asdict(self)
# #         cfg = DictConfig({k: getattr(self,k) for k in self.keys()})
#         Extract.config2yaml(cfg, path)

#     @classmethod
#     def load(cls,
#              path: Union[str, Path]) -&gt; &#34;DatasetConfig&#34;:

#         cfg = Extract.config_from_yaml(path)

# #         keys = cls.__dataclass_fields__.keys()
#         cfg = cls(**{k: cfg[k] for k in cls.keys()})
#         return cfg

#     @classmethod
#     def keys(cls):
#         return cls.__dataclass_fields__.keys()

#     def __repr__(self):
#         out = f&#34;{type(self)}&#34; + &#34;\n&#34;
#         out += &#34;\n&#34;.join([f&#34;{k}: {getattr(self, k)}&#34; for k in self.keys()])
# #         out += f&#34;\nroot_dir: {self.root_dir}&#34;
# #         out += &#34;\nsubset_dirs: \n\t&#34; + &#39;\n\t&#39;.join(self.subset_dirs)
#         return out


# @dataclass
# class DatasetConfig(BaseDatasetConfig):
#     base_dataset_name: str = &#34;Extant_Leaves&#34;
#     class_type: str = &#34;family&#34;
#     threshold: Optional[int] = 10
#     resolution: int = 512
#     version: str = &#34;v1_0&#34;
#     path_schema: str = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;

#     @property
#     def available_versions(self) -&gt; List[str]:
#         return list(catalog_registry.available_datasets().versions.keys())

#     @property
#     def full_name(self) -&gt; str:
#         name  = self.base_dataset_name
#         if self.threshold:
#             name += f&#34;_{self.class_type}_{self.threshold}&#34;
#         name += f&#34;_{self.resolution}&#34;
#         return name


# class ImageFileDatasetConfig(DatasetConfig):

#     @property
#     def root_dir(self):
#         return catalog_registry.available_datasets.get(self.full_name, version=self.version)

#     def is_valid_subset(self, subset: str):
#         for s in (&#34;train&#34;, &#34;val&#34;, &#34;test&#34;):
#             if s in subset:
#                 return True
#         return False

#     @property
#     def subsets(self):
#         if isinstance(self.root_dir, list):
#             return []
#         return [s for s in os.listdir(self.root_dir) if self.is_valid_subset(s)]

#     @property
#     def subset_dirs(self):
#         return [os.path.join(self.root_dir, subset) for subset in self.subsets]
# #         subsets = self.subsets
# #         return [os.path.join(self.root_dir, subset) for subset in subsets if self.is_valid_subset(subset)]

#     def locate_files(self) -&gt; Dict[str, List[Path]]:
#         return Extract.locate_files(self.root_dir)


#     @cached_property
#     def num_samples(self):
# #         subset_dirs = {Path(subset_dir).stem: Path(subset_dir) for subset_dir in self.subset_dirs}
#         files = {subset: f for subset, f in self.locate_files().items() if self.is_valid_subset(subset)}
#         return {subset: len(list(f)) for subset, f in files.items()}
# #         return {subset: len(list(subset_dirs[subset].rglob(&#34;*/*.jpg&#34;))) for subset in subset_dirs.keys()}


#     def __repr__(self):
#         out = super().__repr__()
#         out += f&#34;\nroot_dir: {self.root_dir}&#34;
#         out += &#34;\nsubsets: &#34;
#         for i, subset in enumerate(self.subsets):
#             out += &#39;\n\t&#39; + f&#34;{subset}:&#34;
#             out += &#39;\n\t\t&#39; + f&#34;subdir: {self.subset_dirs[i]}&#34;
#             out += &#39;\n\t\t&#39; + f&#34;subset_num_samples: {self.num_samples[subset]}&#34;
#         return out


# @dataclass
# class CSVDatasetConfig(BaseDatasetConfig):

#     full_name: str = None
#     data_path: str = None
#     label_encoder_path: Optional[str] = None
#     subset_key: str = &#34;all&#34;

#     def update(self, **kwargs):
#         if &#34;subset_key&#34; in kwargs:
#             self.subset_key = kwargs[&#34;subset_key&#34;]
#         if &#34;num_samples&#34; in kwargs:
#             self.num_samples = {self.subset_key: kwargs[&#34;num_samples&#34;]}

#     @cached_property
#     def num_samples(self):
#         return {self.subset_key: len(self.locate_files())}

#     def __repr__(self):
#         out = super().__repr__()
#         out += &#39;\n&#39; + f&#34;num_samples: {self.num_samples[self.subset_key]}&#34;
#         return out

#     def locate_files(self) -&gt; pd.DataFrame: #Dict[str, List[Path]]:
#         return Extract.df_from_csv(self.data_path)

#     def load_label_encoder(self) -&gt; Union[None, LabelEncoder]:
#         if os.path.exists(str(self.label_encoder_path)):
#             return Extract.labels_from_json(str(self.label_encoder_path))
#         return

#     @classmethod
#     def export_dataset_state(cls,
#                              output_dir: Union[str, Path],
#                              df: pd.DataFrame=None,
#                              config: DictConfig=None,
#                              encoder: LabelEncoder=None,
#                              dataset_name: Optional[str]=&#34;dataset&#34;
#                              ) -&gt; None:
#         Extract.export_dataset_state(output_dir=output_dir,
#                                      df=df,
#                                      config=config,
#                                      encoder=encoder,
#                                      dataset_name=dataset_name)


#     @classmethod
#     def import_dataset_state(cls,
#                              data_dir: Optional[Union[str, Path]]=None,
#                              config_path: Optional[Union[Path, str]]=None,
#                             ) -&gt; Tuple[&#34;CSVDataset&#34;, &#34;CSVDatasetConfig&#34;]:

#         return Extract.import_dataset_state(data_dir=data_dir,
#                                             config_path=config_path)


# ####################################
# ####################################


# @dataclass
# class SampleSchema:
#     path : Union[str, Path] = None
#     family : str = None
#     genus : str = None
#     species : str = None
#     collection : str = None
#     catalog_number : str = None

#     @classmethod
#     def keys(cls):
#         return list(cls.__dataclass_fields__.keys())

#     def __getitem__(self, index: int):
#         return getattr(self, self.keys()[index])


# class CustomDataset(torchdata.datasets.Files): # (CommonDataset):

#     def __init__(self,
#                  files: List[Path]=None,
#                  samples_df: pd.DataFrame=None,
#                  path_schema: Path = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;,
#                  return_signature: List[str] = [&#34;image&#34;,&#34;target&#34;], #,&#34;path&#34;],
#                  eager_encode_targets: bool = False,
#                  config: Optional[BaseDatasetConfig]=None,
#                  transform=None):
#         files = files or []
#         super().__init__(files=files)
# #         self.samples_df = samples_df
#         self.path_schema = PathSchema(path_schema)
#         self._return_signature = collections.namedtuple(&#34;return_signature&#34;, return_signature)

#         self.x_col = &#34;path&#34;
#         self.y_col = &#34;family&#34;
#         self.id_col = &#34;catalog_number&#34;
#         self.config = config or {}
#         self.transform = transform
#         self.eager_encode_targets = eager_encode_targets
#         self.setup(samples_df=samples_df)


#     def fetch_item(self, index: int) -&gt; Tuple[str]:
#         sample = self.parse_sample(index)
#         image = Image.open(sample.path)
#         return self.return_signature(image=image,
#                                      target=getattr(sample, self.y_col),
#                                      path=getattr(sample, self.x_col))

#     def return_signature(self, **kwargs):
#         return self._return_signature(**{key: kwargs[key] for key in self._return_signature._fields if key in kwargs})

#     def __getitem__(self, index: int):

#         item = self.fetch_item(index)
#         image, target, path = item.image, item.target, str(getattr(item, &#34;path&#34;, None))

#         target = self.label_encoder.class2idx[target]

#         if self.transform is not None:
# #             image = totensor(image)
#             image = self.transform(image)
# #         if self.target_transform is not None:
# #             target = self.target_transform(target)

#         if path:
#             return tuple(self.return_signature(image=image, target=target, path=path))
#         return tuple(self.return_signature(image=image, target=target))


#     def setup(self,
#               samples_df: pd.DataFrame=None,
#               label_encoder: LabelEncoder=None,
#               fit_targets: bool=True):
#         &#34;&#34;&#34;
#         Running setup() should result in the Dataset having assigned values for:
#             self.samples
#             self.targets
#             self.samples_df
#             self.label_encoder

#         &#34;&#34;&#34;
#         if samples_df is not None:
#             self.samples_df = samples_df.convert_dtypes()
#         self.samples = [self.parse_sample(idx) for idx in range((len(self)))]
#         self.targets = [sample[1] for sample in self.samples]
#         self.samples_df = pd.DataFrame(self.samples).convert_dtypes()

#         self.label_encoder = label_encoder or LabelEncoder()
#         if fit_targets:
#             self.label_encoder.fit(self.targets)

#         if self.eager_encode_targets:
#             self.targets = self.label_encoder.encode(self.targets).tolist()

#     @classmethod
#     def from_config(cls, config: DatasetConfig, subset_keys: List[str]=None) -&gt; &#34;CustomDataset&#34;:
#         pass

#     def parse_sample(self, index: int):
#         pass

#     @property
#     def classes(self):
#         return self.label_encoder.classes

#     def __repr__(self):
#         disp = f&#34;&#34;&#34;&lt;{str(type(self)).strip(&#34;&#39;&gt;&#34;).split(&#39;.&#39;)[1]}&gt;:&#34;&#34;&#34;
#         disp += &#39;\n\t&#39; + self.config.__repr__().replace(&#39;\n&#39;,&#39;\n\t&#39;)
# #         if len(self):
# #             disp += &#34;\n\t&#34; + f&#34;num_samples: {len(self)}&#34;
#         return disp


#     @classmethod
#     def get_files_from_samples(cls,
#                                samples: Union[pd.DataFrame, List],
#                                x_col: Optional[str]=&#34;path&#34;):
#         if isinstance(samples, pd.DataFrame):
#             if x_col in samples.columns:
#                 files = list(samples[x_col].values)
#             else:
#                 files = list(samples.iloc[:,0].values)
#         elif isinstance(samples, list):
#             files = [s[0] for s in self.samples]

#         return files

#     def intersection(self, other, suffixes=(&#34;_x&#34;,&#34;_y&#34;)):
#         samples_df = self.samples_df
#         other_df = other.samples_df

#         intersection = samples_df.merge(other_df, how=&#39;inner&#39;, on=self.id_col, suffixes=suffixes)
#         return intersection

#     def __add__(self, other):

#         intersection = self.intersection(other)[self.id_col].tolist()
#         samples_df = self.samples_df

#         left_union = samples_df[samples_df[self.id_col].apply(lambda x: x in intersection)]

#         return left_union

#     def __sub__(self, other):

#         intersection = self.intersection(other)[self.id_col].tolist()
#         samples_df = self.samples_df

#         remainder = samples_df[samples_df[self.id_col].apply(lambda x: x not in intersection)]

#         return remainder

#     def filter(self, indices, subset_key: Optional[str]=&#34;all&#34;):
#         out = type(self)(samples_df = self.samples_df.iloc[indices,:],
#                          config = deepcopy(self.config))
#         out.config.update(subset_key=subset_key,
#                           num_samples=len(out))
#         return out

#     def get_unsupervised(self):
#         return UnsupervisedDatasetWrapper(self)


# class UnsupervisedDatasetWrapper(torchdata.datasets.Files):#torchvision.datasets.ImageFolder):

#     def __init__(self, dataset):
#         super().__init__(files=dataset.files)
#         self.dataset = dataset
# #         super().__init__(samples_df=dataset.samples_df,
# #                            path_schema=dataset.path_schema)


#     def __getitem__(self, index):
#         return self.dataset[index][0]

#     def __len__(self):
#         return len(self.dataset)

#     def __repr__(self):
#         out = &#34;&lt;UnsupervisedDatasetWrapper&gt;\n&#34;
#         out += self.dataset.__repr__()
#         return out

# #     def parse_sample(self, index: int):
# #         path = self.files[index]


# class ImageFileDataset(CustomDataset):

#     @classmethod
#     def from_config(cls, config: DatasetConfig, subset_keys: List[str]=None) -&gt; &#34;CustomDataset&#34;:
#         files = config.locate_files()
#         if isinstance(subset_keys, list):
#             files = {k: files[k] for k in subset_keys}
#         if len(files.keys())==1:
#             files = files[subset_keys[0]]
#         new = cls(files=files,
#                   path_schema=config.path_schema)
#         new.config = config
#         return new

#     def parse_sample(self, index: int):
#         path = self.files[index]
#         family, genus, species, collection, catalog_number = self.path_schema.parse(path)

#         return SampleSchema(path=path,
#                              family=family,
#                              genus=genus,
#                              species=species,
#                              collection=collection,
#                              catalog_number=catalog_number)


# class CSVDataset(CustomDataset):

#     @classmethod
#     def from_config(cls,
#                     config: DatasetConfig,
#                     subset_keys: List[str]=None,
#                     eager_encode_targets: bool=False) -&gt; Union[Dict[str, &#34;CSVDataset&#34;], &#34;CSVDataset&#34;]:

#         files_df = config.locate_files()
#         if subset_keys is None:
#             subset_keys = [&#39;all&#39;]
#         if isinstance(subset_keys, list) and isinstance(files_df, dict):
#             files_df = {k: files_df[k] for k in subset_keys}
#             new = {k: cls(samples_df=files_df[k],  \
#                           eager_encode_targets=eager_encode_targets) for k in subset_keys}
#             for k in subset_keys:
#                 new[k].config = deepcopy(config)
#                 new[k].config.subset_key = k

#         if len(subset_keys)==1:
#             if isinstance(files_df, dict):
#                 files_df = files_df[subset_keys[0]]
#             new = cls(samples_df=files_df,
#                       eager_encode_targets=eager_encode_targets)
#             new.config = config
#             new.config.subset_key = subset_keys[0]
#         return new

#     def setup(self,
#               samples_df: pd.DataFrame=None,
#               label_encoder: LabelEncoder=None,
#               fit_targets: bool=True):

#         if samples_df is not None:
#             self.samples_df = samples_df.convert_dtypes()
#         self.files = self.samples_df[self.x_col].apply(lambda x: Path(x)).tolist()
#         super().setup(samples_df=self.samples_df,
#                       label_encoder=label_encoder,
#                       fit_targets=fit_targets)
# #         self.samples = [self.parse_sample(idx) for idx in range((len(self)))]
# #         self.targets = [sample[1] for sample in self.samples]
# #         self.samples_df = pd.DataFrame(self.samples).convert_dtypes()

# #         self.label_encoder = label_encoder or LabelEncoder()
# #         if fit_targets:
# #             self.label_encoder.fit(self.targets)

# #         if self.eager_encode_targets:
# #             self.targets = self.label_encoder.encode(self.targets).tolist()


#     def parse_sample(self, index: int):

#         row = self.samples_df.iloc[index,:].tolist()
#         path, family, genus, species, collection, catalog_number = row
#         return SampleSchema(path=path,
#                              family=family,
#                              genus=genus,
#                              species=species,
#                              collection=collection,
#                              catalog_number=catalog_number)


#######################################################
#######################################################
#######################################################
#######################################################


#         #########################################

#         A_in_B = create_dataset_A_in_B(dataset_A,
#                                        dataset_B)

#         csv_dataset_pathname = f&#34;{csv_config_A.full_name}_in_{csv_config_B.full_name}&#34;
#         csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
#         csv_dataset_config_out_path = os.path.join(out_dir,f&#34;A_in_B-CSVDataset-config.yaml&#34;)
# #         print(f&#34;A_in_B.columns: {A_in_B.columns}&#34;)
#         Extract.df2csv(A_in_B,
#                        path = csv_dataset_out_path)
#         A_in_B_config = CSVDatasetConfig(full_name = csv_dataset_pathname,
#                                          data_path = csv_dataset_out_path,
#                                          subset_key = &#34;all&#34;)
#         A_in_B_config.save(csv_dataset_config_out_path)

#         #########################################
#         B_in_A = create_dataset_A_in_B(dataset_B,
#                                        dataset_A)

#         csv_dataset_pathname = f&#34;{csv_config_B.full_name}_in_{csv_config_A.full_name}&#34;
#         csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
#         Extract.df2csv(B_in_A,
#                        path = csv_dataset_out_path)
#         B_in_A_config = CSVDatasetConfig(full_name = csv_dataset_pathname,
#                                          data_path = csv_dataset_out_path,
#                                          subset_key = &#34;all&#34;)

#         csv_dataset_config_out_path = os.path.join(out_dir,f&#34;B_in_A-CSVDataset-config.yaml&#34;)
#         B_in_A_config.save(csv_dataset_config_out_path)


##############################################

# def export_composite_dataset_catalog_configuration(output_dir: str = &#34;.&#34;,
#                                                    csv_cfg_path_A: str=None,
#                                                    csv_cfg_path_B: str=None,
#                                                    composition: str=&#34;-&#34;) -&gt; Tuple[CSVDataset, CSVDatasetConfig]:


#     csv_config_A = CSVDatasetConfig.load(path = csv_cfg_path_A)
#     csv_config_B = CSVDatasetConfig.load(path = csv_cfg_path_B)

#     dataset_A = CSVDataset.from_config(csv_config_A)
#     dataset_B = CSVDataset.from_config(csv_config_B)
#     print(f&#34;num_samples A: {len(dataset_A)}&#34;)
#     print(f&#34;num_samples B: {len(dataset_B)}&#34;)

#     print(f&#34;producing composition: {composition}&#34;)
#     if composition == &#39;-&#39;:
#         dataset_A_composed_B = dataset_A - dataset_B
#         full_name = f&#34;{csv_config_A.full_name}_minus_{csv_config_B.full_name}&#34;
#         print(f&#34;num_samples A-B: {len(dataset_A_composed_B)}&#34;)

#     if composition == &#39;intersection&#39;:
#         dataset_A_composed_B = dataset_A.intersection(dataset_B)
#         full_name = f&#34;{csv_config_A.full_name}_w_{csv_config_B.full_name}&#34;
#         print(f&#34;num_samples A_w_B: {len(dataset_A_composed_B)}&#34;)

#         out_dir = os.path.join(output_dir, full_name)
#         os.makedirs(out_dir, exist_ok=True)

#         csv_dataset_pathname = f&#34;{csv_config_A.full_name}_in_{csv_config_B.full_name}&#34;
#         csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
#         csv_dataset_config_out_path = os.path.join(out_dir,f&#34;A_in_B-CSVDataset-config.yaml&#34;)
#         columns = [*[col for col in dataset_A_composed_B.columns if col.endswith(&#34;_x&#34;)], *[&#34;catalog_number&#34;]]
#         extant_in_pnas = dataset_A_composed_B.reset_index()[columns].sort_values(&#34;catalog_number&#34;)
# #         extant_in_pnas = extant_in_pnas.loc[:,[*list(SampleSchema.keys())]]

#         print(f&#34;extant_in_pnas.columns: {extant_in_pnas.columns}&#34;)

#         extant_in_pnas = extant_in_pnas.rename(columns = {col: col.split(&#34;_x&#34;)[0] for col in extant_in_pnas.columns})
#         Extract.df2csv(extant_in_pnas,
#                        path = csv_dataset_out_path)

#         A_in_B_config = CSVDatasetConfig(full_name = csv_dataset_pathname,
#                                          data_path = csv_dataset_out_path,
#                                          subset_key = &#34;all&#34;)
#         A_in_B_config.save(csv_dataset_config_out_path)

#         csv_dataset_pathname = f&#34;{csv_config_B.full_name}_in_{csv_config_A.full_name}&#34;
#         csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
#         csv_dataset_config_out_path = os.path.join(out_dir,f&#34;B_in_A-CSVDataset-config.yaml&#34;)
#         columns = [*[col for col in dataset_A_composed_B.columns if col.endswith(&#34;_y&#34;)], *[&#34;catalog_number&#34;]]
#         pnas_in_extant = dataset_A_composed_B.reset_index()[columns].sort_values(&#34;catalog_number&#34;)
# #         pnas_in_extant = pnas_in_extant.loc[:,[*list(SampleSchema.keys())]]
#         pnas_in_extant = pnas_in_extant.rename(columns = {col: col.split(&#34;_y&#34;)[0] for col in pnas_in_extant.columns})
#         Extract.df2csv(pnas_in_extant,
#                        path = csv_dataset_out_path)
#         B_in_A_config = CSVDatasetConfig(full_name = csv_dataset_pathname,
#                                          data_path = csv_dataset_out_path,
#                                          subset_key = &#34;all&#34;)
#         B_in_A_config.save(csv_dataset_config_out_path)


#     out_dir = os.path.join(output_dir, full_name)
#     os.makedirs(out_dir, exist_ok=True)

#     inputs_dir = os.path.join(out_dir, &#34;inputs&#34;)
# #     os.makedirs(inputs_dir, exist_ok=True)
#     os.makedirs(os.path.join(inputs_dir, &#34;A&#34;), exist_ok=True)
#     os.makedirs(os.path.join(inputs_dir, &#34;B&#34;), exist_ok=True)
#     shutil.copyfile(csv_cfg_path_A, os.path.join(inputs_dir, &#34;A&#34;, Path(csv_cfg_path_A).name))
#     shutil.copyfile(csv_cfg_path_B, os.path.join(inputs_dir, &#34;B&#34;, Path(csv_cfg_path_B).name))

#     csv_dataset_pathname = f&#34;{full_name}-full_dataset&#34;
#     csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
#     csv_dataset_config_out_path = os.path.join(out_dir,f&#34;CSVDataset-config.yaml&#34;)

#     Extract.df2csv(dataset_A_composed_B,
#                    path = csv_dataset_out_path)

#     A_composed_B_config = CSVDatasetConfig(full_name = full_name,
#                                         data_path = csv_dataset_out_path,
#                                         subset_key = &#34;all&#34;)
#     A_composed_B_config.save(csv_dataset_config_out_path)

#     print(f&#34;[FINISHED] DATASET: {full_name}&#34;)
#     print(f&#34;Newly created dataset assets located at:  {out_dir}&#34;)

#     if composition == &#39;-&#39;:
#         dataset_A_composed_B = CSVDataset.from_config(A_composed_B_config)
#         return dataset_A_composed_B, A_composed_B_config

#     if composition == &#39;intersection&#39;:
#         return (extant_in_pnas, pnas_in_extant), A_composed_B_config</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imutils.make_catalogs.cmdline_args"><code class="name flex">
<span>def <span class="ident">cmdline_args</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cmdline_args():
    p = argparse.ArgumentParser(
        description=(
            &#34;Export a series of dataset artifacts (containing csv catalog, yml config, json labels)&#34;
            &#34; for each dataset, provided that the corresponding images are pointed to by one of the&#34;
            &#34; file paths hard-coded in catalog_registry.py.&#34;
        )
    )
    p.add_argument(
        &#34;-o&#34;,
        &#34;--output_dir&#34;,
        dest=&#34;output_dir&#34;,
        type=str,
        default=&#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v1_0&#34;,
        help=(
            &#34;Output root directory. Each unique dataset will be allotted its own subdirectory&#34;
            &#34; within this root dir.&#34;
        ),
    )
    p.add_argument(
        &#34;-a&#34;,
        &#34;--all&#34;,
        dest=&#34;make_all&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production datasets in the most&#34;
            &#34; recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    p.add_argument(
        &#34;-v&#34;,
        &#34;--version&#34;,
        dest=&#34;version&#34;,
        type=str,
        default=&#34;v1_0&#34;,
        help=&#34;Available dataset versions: [v0_3, v1_0].&#34;,
    )
    p.add_argument(
        &#34;--fossil&#34;,
        dest=&#34;make_fossil&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all configurations of the combined Florissant +&#34;
            &#34; General Fossil collections.&#34;
        ),
    )
    p.add_argument(
        &#34;--extant&#34;,
        dest=&#34;make_extant&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: Extant_Leaves&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    p.add_argument(
        &#34;--pnas&#34;,
        dest=&#34;make_pnas&#34;,
        action=&#34;store_true&#34;,
        help=(
            f&#34;If user provides this flag, produce all currently in-production&#34;
            f&#34; resolutions+thresholds in the dataset: PNAS&#39;s most recent version (currently ==&#34;
            f&#34; &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--extant-pnas&#34;,
        dest=&#34;make_extant_minus_pnas&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: Extant-PNAS&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--pnas-extant&#34;,
        dest=&#34;make_pnas_minus_extant&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: PNAS-Extant&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--extant-w-pnas&#34;,
        dest=&#34;make_extant_w_pnas&#34;,
        action=&#34;store_true&#34;,
        help=(
            &#34;If user provides this flag, produce all currently in-production resolutions+thresholds&#34;
            &#34; in the dataset: Extant_w_PNAS&#39;s most recent version (currently == &#39;v1_0&#39;).&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    p.add_argument(
        &#34;--original&#34;,
        dest=&#34;make_all_original&#34;,
        action=&#34;store_true&#34;,
        help=(
            f&#34;If user provides this flag, produce all currently in-production&#34;
            f&#34; resolutions+thresholds in the original datasets&#39; () most recent version (currently&#34;
            f&#34; == &#39;v1_0&#39;). Note that this excludes PNAS.&#34;
        ),
    )
    #                    help=&#34;If user provides this flag, produce all currently in-production datasets in the most recent version (currently == &#39;v1_0&#39;).&#34;)
    return p.parse_args()</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.create_dataset_A_in_B"><code class="name flex">
<span>def <span class="ident">create_dataset_A_in_B</span></span>(<span>dataset_A, dataset_B) â€‘>Â pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_dataset_A_in_B(dataset_A, dataset_B) -&gt; pd.DataFrame:

    A_w_B = dataset_A.intersection(dataset_B)

    columns = [*[col for col in A_w_B.columns if col.endswith(&#34;_x&#34;)], *[&#34;catalog_number&#34;]]
    A_in_B = A_w_B.reset_index()[columns].sort_values(&#34;catalog_number&#34;)

    print(f&#34;A_in_B.columns: {A_in_B.columns}&#34;)
    A_in_B = A_in_B.rename(columns={col: col.split(&#34;_x&#34;)[0] for col in A_in_B.columns})

    return A_in_B</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.export_composite_dataset_catalog_configuration"><code class="name flex">
<span>def <span class="ident">export_composite_dataset_catalog_configuration</span></span>(<span>output_dir:Â strÂ =Â '.', csv_cfg_path_A:Â strÂ =Â None, csv_cfg_path_B:Â strÂ =Â None, composition:Â strÂ =Â '-') â€‘>Â Tuple[CSVDataset,Â CSVDatasetConfig]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_composite_dataset_catalog_configuration(
    output_dir: str = &#34;.&#34;,
    csv_cfg_path_A: str = None,
    csv_cfg_path_B: str = None,
    composition: str = &#34;-&#34;,
) -&gt; Tuple[&#34;CSVDataset&#34;, &#34;CSVDatasetConfig&#34;]:

    csv_config_A = CSVDatasetConfig.load(path=csv_cfg_path_A)
    csv_config_B = CSVDatasetConfig.load(path=csv_cfg_path_B)

    dataset_A = CSVDataset.from_config(csv_config_A)
    dataset_B = CSVDataset.from_config(csv_config_B)
    print(f&#34;num_samples A: {len(dataset_A)}&#34;)
    print(f&#34;num_samples B: {len(dataset_B)}&#34;)

    print(f&#34;producing composition: {composition}&#34;)
    if composition == &#34;-&#34;:
        dataset_A_composed_B = dataset_A - dataset_B
        full_name = f&#34;{csv_config_A.full_name}_minus_{csv_config_B.full_name}&#34;
        out_dir = os.path.join(output_dir, full_name)
        os.makedirs(out_dir, exist_ok=True)
        print(f&#34;num_samples A-B: {len(dataset_A_composed_B)}&#34;)

    if composition == &#34;intersection&#34;:
        dataset_A_composed_B = dataset_A.intersection(dataset_B)
        full_name = f&#34;{csv_config_A.full_name}_w_{csv_config_B.full_name}&#34;
        print(f&#34;num_samples A_w_B: {len(dataset_A_composed_B)}&#34;)

        out_dir = os.path.join(output_dir, full_name)
        os.makedirs(out_dir, exist_ok=True)

    ########################################
    A_in_B = create_dataset_A_in_B(dataset_A, dataset_B)
    csv_dataset_pathname = f&#34;{csv_config_A.full_name}_in_{csv_config_B.full_name}&#34;
    csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
    ETL.df2csv(A_in_B, path=csv_dataset_out_path)
    A_in_B_config = CSVDatasetConfig(
        full_name=csv_dataset_pathname, data_path=csv_dataset_out_path, subset_key=&#34;all&#34;
    )
    csv_dataset_config_out_path = os.path.join(out_dir, f&#34;A_in_B-CSVDataset-config.yaml&#34;)
    A_in_B_config.save(csv_dataset_config_out_path)

    #########################################
    B_in_A = create_dataset_A_in_B(dataset_B, dataset_A)

    csv_dataset_pathname = f&#34;{csv_config_B.full_name}_in_{csv_config_A.full_name}&#34;
    csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
    ETL.df2csv(B_in_A, path=csv_dataset_out_path)
    B_in_A_config = CSVDatasetConfig(
        full_name=csv_dataset_pathname, data_path=csv_dataset_out_path, subset_key=&#34;all&#34;
    )

    csv_dataset_config_out_path = os.path.join(out_dir, f&#34;B_in_A-CSVDataset-config.yaml&#34;)
    B_in_A_config.save(csv_dataset_config_out_path)

    #########################################

    inputs_dir = os.path.join(out_dir, &#34;inputs&#34;)
    os.makedirs(os.path.join(inputs_dir, &#34;A&#34;), exist_ok=True)
    os.makedirs(os.path.join(inputs_dir, &#34;B&#34;), exist_ok=True)
    shutil.copyfile(csv_cfg_path_A, os.path.join(inputs_dir, &#34;A&#34;, Path(csv_cfg_path_A).name))
    shutil.copyfile(csv_cfg_path_B, os.path.join(inputs_dir, &#34;B&#34;, Path(csv_cfg_path_B).name))

    csv_dataset_pathname = f&#34;{full_name}-full_dataset&#34;
    csv_dataset_out_path = os.path.join(out_dir, csv_dataset_pathname + &#34;.csv&#34;)
    ETL.df2csv(dataset_A_composed_B, path=csv_dataset_out_path)
    ####################
    ####################
    csv_dataset_config_out_path = os.path.join(out_dir, f&#34;CSVDataset-config.yaml&#34;)
    A_composed_B_config = CSVDatasetConfig(
        full_name=full_name, data_path=csv_dataset_out_path, subset_key=&#34;all&#34;
    )
    A_composed_B_config.save(csv_dataset_config_out_path)

    print(f&#34;[FINISHED] DATASET: {full_name}&#34;)
    print(f&#34;Newly created dataset assets located at:  {out_dir}&#34;)

    if composition == &#34;-&#34;:
        dataset_A_composed_B = CSVDataset.from_config(A_composed_B_config)
        return dataset_A_composed_B, A_composed_B_config

    if composition == &#34;intersection&#34;:
        return (A_in_B, B_in_A), A_composed_B_config</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.export_dataset_catalog_configuration"><code class="name flex">
<span>def <span class="ident">export_dataset_catalog_configuration</span></span>(<span>output_dir:Â strÂ =Â '/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v1_0', base_dataset_name='Extant_Leaves', threshold=100, resolution=512, version:Â strÂ =Â 'v1_0', path_schema:Â strÂ =Â '{family}_{genus}_{species}_{collection}_{catalog_number}')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_dataset_catalog_configuration(
    output_dir: str = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v1_0&#34;,
    base_dataset_name=&#34;Extant_Leaves&#34;,
    threshold=100,
    resolution=512,
    version: str = &#34;v1_0&#34;,
    path_schema: str = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;,
):

    image_file_config = ImageFileDatasetConfig(
        base_dataset_name=base_dataset_name,
        class_type=&#34;family&#34;,
        threshold=threshold,
        resolution=resolution,
        version=version,
        path_schema=path_schema,
    )

    out_dir = os.path.join(output_dir, image_file_config.full_name)
    os.makedirs(out_dir, exist_ok=True)

    csv_out_path = os.path.join(out_dir, f&#34;{image_file_config.full_name}-full_dataset.csv&#34;)
    image_file_config_out_path = os.path.join(out_dir, &#34;ImageFileDataset-config.yaml&#34;)
    csv_config_out_path = os.path.join(out_dir, &#34;CSVDataset-config.yaml&#34;)

    dataset = ImageFileDataset.from_config(image_file_config, subset_keys=[&#34;all&#34;])
    ETL.df2csv(dataset.samples_df, path=csv_out_path)
    image_file_config.save(image_file_config_out_path)

    csv_config = CSVDatasetConfig(
        full_name=image_file_config.full_name, data_path=csv_out_path, subset_key=&#34;all&#34;
    )

    csv_config.save(csv_config_out_path)

    print(f&#34;[FINISHED] DATASET FULL NAME: {csv_config.full_name}&#34;)
    print(f&#34;Newly created dataset assets located at:  {out_dir}&#34;)

    return dataset, image_file_config, csv_config</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.make_all_original"><code class="name flex">
<span>def <span class="ident">make_all_original</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_all_original(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;&#34;  # original&#34;
    #                           &#34;Extant_Leaves&#34;,
    #                           &#34;Florissant_Fossil&#34;,
    #                           &#34;General_Fossil&#34;]

    resolution = &#34;original&#34;
    path_schema = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;

    print(f&#34;Beginning make_all_original()&#34;)
    #     for base_dataset_name in base_dataset_names:
    export_dataset_catalog_configuration(
        output_dir=output_dir,
        base_dataset_name=base_dataset_name,
        threshold=0,
        resolution=resolution,
        version=version,
        path_schema=path_schema,
    )

    print(f&#34;FINISHED ALL IN original datasets&#34;)
    print(&#34;==&#34; * 15)</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.make_extant"><code class="name flex">
<span>def <span class="ident">make_extant</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_extant(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;Extant_Leaves&#34;
    thresholds = [None, 3, 10, 100]
    resolutions = [512, 1024]
    path_schema = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;

    print(
        f&#34;Beginning make_extant() for {len(resolutions)}x resolutions and {len(thresholds)}x&#34;
        &#34; thresholds&#34;
    )
    for threshold in thresholds:
        for resolution in resolutions:
            export_dataset_catalog_configuration(
                output_dir=output_dir,
                base_dataset_name=base_dataset_name,
                threshold=threshold,
                resolution=resolution,
                version=version,
                path_schema=path_schema,
            )

    print(f&#34;FINISHED ALL IN Extant_Leaves&#34;)
    print(&#34;==&#34; * 15)</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.make_extant_minus_pnas"><code class="name flex">
<span>def <span class="ident">make_extant_minus_pnas</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_extant_minus_pnas(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_names = {&#34;A&#34;: &#34;Extant_Leaves&#34;, &#34;B&#34;: &#34;PNAS&#34;}
    thresholds = [{&#34;A&#34;: 100, &#34;B&#34;: 100}, {&#34;A&#34;: 10, &#34;B&#34;: 100}]
    resolutions = [512, 1024]
    class_type = &#34;family&#34;

    for threshold in thresholds:
        for resolution in resolutions:
            dataset_full_names = {
                &#34;A&#34;: &#34;_&#34;.join([base_names[&#34;A&#34;], class_type, str(threshold[&#34;A&#34;]), str(resolution)]),
                &#34;B&#34;: &#34;_&#34;.join([base_names[&#34;B&#34;], class_type, str(threshold[&#34;B&#34;]), str(resolution)]),
            }

            csv_cfg_path_A = os.path.join(
                output_dir, dataset_full_names[&#34;A&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            csv_cfg_path_B = os.path.join(
                output_dir, dataset_full_names[&#34;B&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            dataset, cfg = export_composite_dataset_catalog_configuration(
                output_dir=output_dir,
                csv_cfg_path_A=csv_cfg_path_A,
                csv_cfg_path_B=csv_cfg_path_B,
                composition=&#34;-&#34;,
            )

    print(f&#34;FINISHED ALL IN Extant-PNAS&#34;)
    print(&#34;==&#34; * 15)</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.make_extant_w_pnas"><code class="name flex">
<span>def <span class="ident">make_extant_w_pnas</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_extant_w_pnas(args):

    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;
    output_dir = args.output_dir

    base_names = {&#34;A&#34;: &#34;Extant_Leaves&#34;, &#34;B&#34;: &#34;PNAS&#34;}
    thresholds = [{&#34;A&#34;: 100, &#34;B&#34;: 100}, {&#34;A&#34;: 10, &#34;B&#34;: 100}]
    resolutions = [512, 1024]
    class_type = &#34;family&#34;

    for threshold in thresholds:
        for resolution in resolutions:
            dataset_full_names = {
                &#34;A&#34;: &#34;_&#34;.join([base_names[&#34;A&#34;], class_type, str(threshold[&#34;A&#34;]), str(resolution)]),
                &#34;B&#34;: &#34;_&#34;.join([base_names[&#34;B&#34;], class_type, str(threshold[&#34;B&#34;]), str(resolution)]),
            }

            csv_cfg_path_A = os.path.join(
                output_dir, dataset_full_names[&#34;A&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            csv_cfg_path_B = os.path.join(
                output_dir, dataset_full_names[&#34;B&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            dataset, cfg = export_composite_dataset_catalog_configuration(
                output_dir=output_dir,
                csv_cfg_path_A=csv_cfg_path_A,
                csv_cfg_path_B=csv_cfg_path_B,
                composition=&#34;intersection&#34;,
            )

    print(f&#34;FINISHED ALL IN Extant_w_PNAS&#34;)
    print(&#34;==&#34; * 15)</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.make_fossil"><code class="name flex">
<span>def <span class="ident">make_fossil</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_fossil(args):

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;Fossil&#34;
    thresholds = [None, 3]
    resolutions = [512, 1024]
    path_schema = &#34;{family}_{genus}_{species}_{collection}_{catalog_number}&#34;

    print(
        f&#34;Beginning make_fossil() for {len(resolutions)}x resolutions and {len(thresholds)}x&#34;
        &#34; thresholds&#34;
    )

    for threshold in thresholds:
        for resolution in resolutions:
            export_dataset_catalog_configuration(
                output_dir=output_dir,
                base_dataset_name=base_dataset_name,
                threshold=threshold,
                resolution=resolution,
                version=version,
                path_schema=path_schema,
            )

    print(f&#34;FINISHED ALL IN Fossil&#34;)
    print(&#34;==&#34; * 15)</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.make_pnas"><code class="name flex">
<span>def <span class="ident">make_pnas</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_pnas(args):
    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;

    output_dir = args.output_dir
    version = args.version

    base_dataset_name = &#34;PNAS&#34;
    thresholds = [100]
    resolutions = [512, 1024]
    path_schema = &#34;{family}_{genus}_{species}_{catalog_number}&#34;

    print(
        f&#34;Beginning make_pnas() for {len(resolutions)}x resolutions and {len(thresholds)}x&#34;
        &#34; thresholds&#34;
    )
    for threshold in thresholds:
        for resolution in resolutions:
            export_dataset_catalog_configuration(
                output_dir=output_dir,
                base_dataset_name=base_dataset_name,
                threshold=threshold,
                resolution=resolution,
                version=version,
                path_schema=path_schema,
            )

    print(f&#34;FINISHED ALL IN PNAS&#34;)
    print(&#34;==&#34; * 15)</code></pre>
</details>
</dd>
<dt id="imutils.make_catalogs.make_pnas_minus_extant"><code class="name flex">
<span>def <span class="ident">make_pnas_minus_extant</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_pnas_minus_extant(args):

    #     if &#34;output_dir&#34; not in args:
    #         args.output_dir = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/experiments/July2021-Nov2021/csv_datasets/leavesdb-v0_3&#34;
    output_dir = args.output_dir

    base_names = {&#34;A&#34;: &#34;PNAS&#34;, &#34;B&#34;: &#34;Extant_Leaves&#34;}
    thresholds = [{&#34;A&#34;: 100, &#34;B&#34;: 100}, {&#34;A&#34;: 100, &#34;B&#34;: 10}]
    resolutions = [512, 1024]
    class_type = &#34;family&#34;

    for threshold in thresholds:
        for resolution in resolutions:
            dataset_full_names = {
                &#34;A&#34;: &#34;_&#34;.join([base_names[&#34;A&#34;], class_type, str(threshold[&#34;A&#34;]), str(resolution)]),
                &#34;B&#34;: &#34;_&#34;.join([base_names[&#34;B&#34;], class_type, str(threshold[&#34;B&#34;]), str(resolution)]),
            }

            csv_cfg_path_A = os.path.join(
                output_dir, dataset_full_names[&#34;A&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            csv_cfg_path_B = os.path.join(
                output_dir, dataset_full_names[&#34;B&#34;], &#34;CSVDataset-config.yaml&#34;
            )
            dataset, cfg = export_composite_dataset_catalog_configuration(
                output_dir=output_dir,
                csv_cfg_path_A=csv_cfg_path_A,
                csv_cfg_path_B=csv_cfg_path_B,
                composition=&#34;-&#34;,
            )

    print(f&#34;FINISHED ALL IN Extant-PNAS&#34;)
    print(&#34;==&#34; * 15)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils" href="index.html">imutils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imutils.make_catalogs.cmdline_args" href="#imutils.make_catalogs.cmdline_args">cmdline_args</a></code></li>
<li><code><a title="imutils.make_catalogs.create_dataset_A_in_B" href="#imutils.make_catalogs.create_dataset_A_in_B">create_dataset_A_in_B</a></code></li>
<li><code><a title="imutils.make_catalogs.export_composite_dataset_catalog_configuration" href="#imutils.make_catalogs.export_composite_dataset_catalog_configuration">export_composite_dataset_catalog_configuration</a></code></li>
<li><code><a title="imutils.make_catalogs.export_dataset_catalog_configuration" href="#imutils.make_catalogs.export_dataset_catalog_configuration">export_dataset_catalog_configuration</a></code></li>
<li><code><a title="imutils.make_catalogs.make_all_original" href="#imutils.make_catalogs.make_all_original">make_all_original</a></code></li>
<li><code><a title="imutils.make_catalogs.make_extant" href="#imutils.make_catalogs.make_extant">make_extant</a></code></li>
<li><code><a title="imutils.make_catalogs.make_extant_minus_pnas" href="#imutils.make_catalogs.make_extant_minus_pnas">make_extant_minus_pnas</a></code></li>
<li><code><a title="imutils.make_catalogs.make_extant_w_pnas" href="#imutils.make_catalogs.make_extant_w_pnas">make_extant_w_pnas</a></code></li>
<li><code><a title="imutils.make_catalogs.make_fossil" href="#imutils.make_catalogs.make_fossil">make_fossil</a></code></li>
<li><code><a title="imutils.make_catalogs.make_pnas" href="#imutils.make_catalogs.make_pnas">make_pnas</a></code></li>
<li><code><a title="imutils.make_catalogs.make_pnas_minus_extant" href="#imutils.make_catalogs.make_pnas_minus_extant">make_pnas_minus_extant</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>