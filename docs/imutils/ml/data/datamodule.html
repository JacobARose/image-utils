<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.ml.data.datamodule API documentation</title>
<meta name="description" content="imutils/ml/data/datamodule.py …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.ml.data.datamodule</code></h1>
</header>
<section id="section-intro">
<p>imutils/ml/data/datamodule.py</p>
<p>Created on: Wednesday March 16th, 2022<br>
Created by: Jacob Alexander Rose
</p>
<ul>
<li>Update (Wednesday March 24th, 2022)
- Refactored to make more modular BaseDataset and BaseDataModule, which Herbarium2022Dataset and Herbarium2022DataModule inherit, respectively.</li>
<li>Update (Wednesday April 6th, 2022)
- Wonky refactor to add ExtantLeavesDataset and ExtantLeavesDataModule to definitions. Involed some blurring of abstractions &ndash; will need to refactor the base class AbstractCatalogDataset.</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
imutils/ml/data/datamodule.py

Created on: Wednesday March 16th, 2022  
Created by: Jacob Alexander Rose  


- Update (Wednesday March 24th, 2022)
        - Refactored to make more modular BaseDataset and BaseDataModule, which Herbarium2022Dataset and Herbarium2022DataModule inherit, respectively.
- Update (Wednesday April 6th, 2022)
        - Wonky refactor to add ExtantLeavesDataset and ExtantLeavesDataModule to definitions. Involed some blurring of abstractions -- will need to refactor the base class AbstractCatalogDataset.
&#34;&#34;&#34;


import matplotlib.pyplot as plt
from icecream import ic
import jpeg4py as jpeg
import numpy as np
from omegaconf import DictConfig, OmegaConf
import os
import pandas as pd
from pathlib import Path
from PIL import Image
from rich import print as pp
import multiprocessing as mproc
import pytorch_lightning as pl
from sklearn import preprocessing
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms as T
import torchvision
from typing import *


# from imutils.big.make_train_val_splits import main as make_train_val_splits
from imutils.big.make_train_val_splits import (check_already_built,
                                                                                           read_encoded_splits,
                                                                                           find_data_splits_dir,
                                                                                           main as make_train_val_splits)

# from imutils.big.transforms.image import (Preprocess,
from imutils.ml.aug.image.images import (Preprocess,
                                                                                 BatchTransform,
                                                                                 get_default_transforms,
                                                                                 DEFAULT_CFG as DEFAULT_TRANSFORM_CFG)

from imutils.ml.utils import label_utils

__all__ = [&#34;Herbarium2022DataModule&#34;, 
                   &#34;Herbarium2022Dataset&#34;,
                   &#34;ExtantLeavesDataset&#34;, 
                   &#34;ExtantLeavesDataModule&#34;,
                   &#34;get_default_transforms&#34;]

import torch

def tensor_to_image(x: torch.Tensor) -&gt; np.ndarray:
        if x.ndim==3:
                return x.permute(1,2,0)
        return x.permute(0, 2, 3, 1)


def read_jpeg(path):
        return jpeg.JPEG(path).decode()

def read_pil(path):
        return Image.open(path)

def read_torchvision_img(path):
        img=torchvision.io.read_image(path)
        return img


default_reader = read_jpeg


IMAGE_READERS = {
        &#34;jpeg.JPEG&#34;:read_jpeg,
        &#34;PIL&#34;:read_pil,
        &#34;torchvision&#34;:read_torchvision_img,
        &#34;default&#34;:default_reader
}



class AbstractCatalogDataset(Dataset):

        def __init__(self):
                super().__init__()
                &#34;&#34;&#34;
                TBD: document these function kwargs
                
                &#34;&#34;&#34;
                self.x_col = &#34;path&#34;
                self.y_col = &#34;y&#34;
                self.id_col = &#34;image_id&#34;
        
        @property
        def splits_dir(self) -&gt; Path:
                return find_data_splits_dir(source_dir=self.catalog_dir,
                                                                        train_size=self.train_size)
        
        @property
        def split_file_path(self) -&gt; Path:
                &#34;&#34;&#34;
                Should return the path of this subset&#39;s on-disk csv catalog file. I emphasize should.
                &#34;&#34;&#34;
                return self.splits_dir / f&#34;{self.subset}_metadata.csv&#34;
        
        
        @property
        def already_built(self) -&gt; bool:
                return check_already_built(self.splits_dir)

        
        def prepare_metadata(self):

                if not self.already_built:              
                        data = make_train_val_splits(source_dir=self.catalog_dir,
                                                                                 splits_dir=self.splits_dir,
                                                                                 label_col=self.label_col,
                                                                                 train_size=self.train_size,
                                                                                 seed=self.seed)

                        return data


        def get_data_subset(self,
                                                subset: str=&#34;train&#34;) -&gt; Tuple[&#34;LabelEncoder&#34;, pd.DataFrame]:
                &#34;&#34;&#34;
                Read the selected data subset into a pd.DataFrame
                
                Returns a Tuple containing an sklearn LabelEncoder and a pd.DataFrame of the subset&#39;s data catalog
                &#34;&#34;&#34;
                data = read_encoded_splits(source_dir=self.splits_dir,
                                                                                        include=[subset])
                encoder = data[&#34;label_encoder&#34;]
                data = data[&#34;subsets&#34;][subset]
                
                return encoder, data

        
        def setup(self):
                &#34;&#34;&#34;
                Assigns the following instance attributes:
                
                        ::self.label_encoder
                        ::self.df
                        ::self.paths
                        ::self.targets
                        ::self.num_classes
                &#34;&#34;&#34;
                data = self.prepare_metadata()
                if data is None:
                        encoder, data = self.get_data_subset(subset=self.subset)
                else:
                        encoder = data[&#34;label_encoder&#34;]
                        data = data[&#34;subsets&#34;][self.subset]

                if isinstance(encoder, preprocessing.LabelEncoder):
                        &#34;&#34;&#34;
                        Auto wraps any sklearn LabelEncoder in our custom class.
                        (Added 2022-03-25 - untested)
                        &#34;&#34;&#34;
                        encoder = label_utils.LabelEncoder.from_sklearn(encoder)

                setattr(data, &#34;label_encoder&#34;, encoder)
                self.label_encoder = encoder
                self.df = data
                
                if self.shuffle:
                        self.df = self.df.sample(frac=1, random_state=self.seed).reset_index(drop=False)

                if self.id_col in self.df.columns:
                        self.image_ids = self.df[self.id_col]
                self.paths = self.df[self.x_col]

                if self.is_supervised:
                        # self.imgs = 
                        self.targets = self.df[self.y_col]
                        self.num_classes = len(set(self.df[self.y_col]))
                else:
                        self.targets = None
                        self.num_classes = -1 # Or 0?
        

        def get_decoded_targets(self, with_image_ids: bool=True) -&gt; Tuple[str, np.ndarray]:
                assert self.is_supervised
                return self.label_encoder.inv_transform(self.targets)
        

class BaseDataset(AbstractCatalogDataset):
        catalog_dir: str = os.path.abspath(&#34;./data&#34;)
        
        def __init__(self,
                                 catalog_dir: Optional[str]=None,
                                 subset: str=&#34;train&#34;,
                                 label_col: str=&#34;scientificName&#34;,
                                 train_size: float=0.7,
                                 shuffle: bool=True,
                                 seed: int=14,
                                 image_reader: Union[Callable,str]=&#34;default&#34;, #Image.open,
                                 preprocess: Callable=None,
                                 transform: Callable=None):
                &#34;&#34;&#34;
                
                Arguments:
                
                        catalog_dir: Optional[str]=None,
                                
                        subset: str=&#34;train&#34;,
                        
                        label_col: str=&#34;scientificName&#34;,
                                Column containing the fully decoded str labels
                        train_size: float=0.7,
                        shuffle: bool=True,
                        seed: int=14,
                        image_reader: Union[Callable,str]=&#34;default&#34;, #Image.open,
                        preprocess: Callable=None,
                        transform: Callable=None
                
                &#34;&#34;&#34;
                super().__init__()
                
                self.catalog_dir = catalog_dir or self.catalog_dir

                self.label_col = label_col
                self.train_size = train_size
                self.shuffle = shuffle
                self.seed = seed
                self.subset = subset
                self.is_supervised = bool(subset != &#34;test&#34;)
                self.set_image_reader(image_reader)
                self.preprocess = preprocess
                self.transform = transform
                self.setup()
                                
        @classmethod
        def from_cfg(cls,
                                 cfg: DictConfig,
                                 **kwargs):
                cfg = OmegaConf.merge(cfg, kwargs)
                return cls(**cfg)


        def get_cfg(self,
                                 cfg: DictConfig=None,
                                 **kwargs):
                cfg=cfg or {}
                default_cfg = DictConfig(dict(                  
                        catalog_dir=self.catalog_dir or None,
                        subset=self.subset or &#34;train&#34;,
                        label_col=self.label_col or &#34;scientificName&#34;,
                        train_size=self.train_size or 0.7,
                        shuffle=self.shuffle,
                        seed=self.seed or 14))
                
                cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
                return cfg

        def __len__(self):
                return len(self.df)
        
        def set_image_reader(self,
                                                 reader: Union[Callable,str]) -&gt; None:
                if isinstance(reader, str):
                        if reader not in IMAGE_READERS:
                                print(f&#34;specified image_reader is invalid, using default jpeg.JPEG&#34;)
                        reader = IMAGE_READERS.get(reader, IMAGE_READERS[&#34;default&#34;])
                elif not isinstance(reader, Callable):
                        raise InvalidArgument
                
                self.reader = reader
        
        def parse_sample(self, index: int):
                return self.df.iloc[index, :]
                
        def fetch_item(self, index: int) -&gt; Tuple[str]:
                &#34;&#34;&#34;
                Returns identically-structured namedtuple as __getitem__, with the following differences:
                        - PIL Image (or raw bytes) as returned by self.reader function w/o any transforms
                                vs.
                          torch.Tensor after all transformssx
                        - target text label vs, target int label
                        - image path
                        - image catalog_number
                
                &#34;&#34;&#34;
                sample = self.parse_sample(index)
                path = getattr(sample, self.x_col)
                image_id = getattr(sample, self.id_col)
                
                image = self.reader(path)
                
                metadata={
                        &#34;path&#34;:path,
                        &#34;image_id&#34;:image_id
                                  # &#34;catalog_number&#34;:catalog_number
                                 }
                label = -1
                if self.is_supervised:
                        label = getattr(sample, self.y_col, -1)
                return image, label, metadata
                
        def __getitem__(self, index: int):
                
                image, label, metadata = self.fetch_item(index)
                
                if self.preprocess is not None:
                        image = self.preprocess(image)
                
                if self.transform is not None:
                        image = self.transform(image)
                
                return image, label, metadata



class BaseDataModule(pl.LightningDataModule):
        dataset_cls = None #Herbarium2022Dataset
        train_dataset = None
        val_dataset = None
        test_dataset = None
        
        train_transform = None
        val_transform = None
        test_transform = None
        
        transform_cfg = DEFAULT_TRANSFORM_CFG

#       def __init__(self,
#                                catalog_dir: Optional[str]=None,
#                                label_col=&#34;scientificName&#34;,
#                                train_size=0.7,
#                                shuffle: bool=True,
#                                seed=14,
#                                batch_size: int = 128,
#                                num_workers: int = None,
#                                pin_memory: bool=True,
#                                persistent_workers: Optional[bool]=False,
#                                train_transform=None,
#                                val_transform=None,
#                                test_transform=None,
#                                transform_cfg=None,
#                                remove_transforms: bool=False,
#                                image_reader: Callable=&#34;default&#34;, #Image.open,
#                                **kwargs
#       ):
#               super().__init__()
                
#               self.catalog_dir = catalog_dir
                
#               self.label_col = label_col
#               self.train_size = train_size
#               self.shuffle = shuffle
#               self.seed = seed
#               self.batch_size = batch_size
#               self.num_workers = num_workers if num_workers is not None else mproc.cpu_count()
#               self.pin_memory = pin_memory
#               self.persistent_workers = persistent_workers
#               self.image_reader = image_reader

#               self.setup_transforms(transform_cfg=transform_cfg,
#                                                         train_transform=train_transform,
#                                                         val_transform=val_transform,
#                                                         test_transform=test_transform,
#                                                         remove_transforms=remove_transforms)
#               self.cfg = self.get_cfg()
#               self.kwargs = kwargs
                

        @classmethod
        def from_cfg(cls,
                                 cfg: DictConfig,
                                 **kwargs):
                cfg = OmegaConf.merge(cfg, kwargs)
                return cls(**cfg)

        def get_cfg(self,
                                 cfg: DictConfig=None,
                                 **kwargs):
                cfg=cfg or {}
                default_cfg = DictConfig(dict(
                        catalog_dir=self.catalog_dir or None,
                        label_col=self.label_col or &#34;scientificName&#34;,
                        train_size=self.train_size or 0.7,
                        shuffle=self.shuffle,
                        seed=self.seed or 14,
                        batch_size = self.batch_size or 128,
                        num_workers = self.num_workers or None,
                        pin_memory=self.pin_memory,
                        transform_cfg=self.transform_cfg,
                        remove_transforms=self.remove_transforms,
                ))
                
                cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
                return cfg


        def prepare_data(self):
                pass

        def setup(self, stage=None):
                raise NotImplementedError


        def setup_transforms(self,
                                                 transform_cfg: dict=None,
                                                 train_transform=None,
                                                 val_transform=None,
                                                 test_transform=None,
                                                 remove_transforms: bool=False):
                transform_cfg = transform_cfg or {}
                self.transform_cfg = OmegaConf.merge(self.transform_cfg, transform_cfg)
                self.remove_transforms = remove_transforms
                if self.remove_transforms:
                        self.train_transform = None
                        self.val_transform = None
                        self.test_transform = None
                else:
                        print(&#34;self.transform_cfg:&#34;); pp(self.transform_cfg)
                        self.train_transform = (
                                get_default_transforms(mode=&#34;train&#34;, config=self.transform_cfg)
                                if train_transform is None else train_transform
                        )
                        self.val_transform = (
                                get_default_transforms(mode=&#34;val&#34;, config=self.transform_cfg)
                                if val_transform is None else val_transform
                        )
                        self.test_transform = (
                                get_default_transforms(mode=&#34;test&#34;, config=self.transform_cfg)
                                if test_transform is None else test_transform
                        )


        def set_image_reader(self,
                                                 reader: Callable) -&gt; None:
                &#34;&#34;&#34;
                Pass in a callable that reads image data from disk,
                which is assigned to each of this datamodule&#39;s datasets, respectively.
                &#34;&#34;&#34;
                for data in [self.train_dataset, self.val_dataset, self.test_dataset]:
                        if data is None:
                                continue
                        data.set_image_reader(reader)


        def train_dataloader(self):
                return DataLoader(
                        self.train_dataset,
                        batch_size=self.batch_size,
                        num_workers=self.num_workers,
                        shuffle=True,
                        pin_memory=self.pin_memory,
                        persistent_workers=self.persistent_workers
                )

        def val_dataloader(self):
                return DataLoader(
                        self.val_dataset,
                        batch_size=self.batch_size,#*2,
                        num_workers=self.num_workers,
                        shuffle=False,
                        pin_memory=self.pin_memory,
                        persistent_workers=self.persistent_workers
                )

        def test_dataloader(self):
                return DataLoader(
                        self.test_dataset,
                        batch_size=self.batch_size,#*2,
                        num_workers=self.num_workers,
                        shuffle=False,
                        pin_memory=self.pin_memory
                )

        def get_dataloader(self,
                                           subset:str=&#34;train&#34;):
                if subset == &#34;train&#34;:
                        return self.train_dataloader()
                elif subset == &#34;val&#34;:
                        return self.val_dataloader()
                elif subset == &#34;test&#34;:
                        return self.test_dataloader()
                else:
                        return None

        def get_dataset(self,
                                           subset:str=&#34;train&#34;):
                if subset == &#34;train&#34;:
                        return self.train_dataset
                elif subset == &#34;val&#34;:
                        return self.val_dataset
                elif subset == &#34;test&#34;:
                        return self.test_dataset
                else:
                        return None

        @property
        def num_classes(self) -&gt; int:
                assert self.train_dataset and self.val_dataset
                return max(self.train_dataset.num_classes, self.val_dataset.num_classes)

        def num_samples(self, 
                                        subset: str=&#34;train&#34;):
                return len(self.get_dataset(subset=subset))
        
        def num_batches(self, 
                                        subset: str=&#34;train&#34;):
                return len(self.get_dataloader(subset=subset))
        
        def get_dataset_size(self, 
                                                subset: str=&#34;train&#34;,
                                                verbose: bool=False):
                num_samples = self.num_samples(subset) # len(datamodule.get_dataset(subset=subset))
                num_batches = self.num_batches(subset) # len(datamodule.get_dataloader(subset=subset))
                if verbose:
                        # print(f&#34;{subset} --&gt; (num_samples: {num_samples:,}), (num_batches: {num_batches:,})&#34;)
                        ic(subset, num_samples, num_batches, self.num_classes, self.batch_size)
                return num_samples, num_batches



        def show_batch(self, win_size=(10, 10)):
                def _to_vis(data):
                        return tensor_to_image(torchvision.utils.make_grid(data, nrow=8, normalize=True))
                transform_cfg = self.transform_cfg

                # get a batch from the training set: try with `val_datlaoader` :)
                # train_transform = self.train_transform
                bsz = self.batch_size

                self.setup_transforms(remove_transforms=True)
                imgs = [self.train_dataset[i][0] for i in range(bsz)]
                
                self.setup_transforms(transform_cfg=self.transform_cfg)
                batch = next(iter(self.train_dataloader()))
                imgs_aug, labels = batch[:2]
                # imgs_aug = train_transform(imgs.numpy())  # apply transforms
                # use matplotlib to visualize
                plt.figure(figsize=win_size)
                plt.imshow(_to_vis(imgs))
                plt.figure(figsize=win_size)
                plt.imshow(_to_vis(imgs_aug))
                


##############################
##############################


class Herbarium2022Dataset(BaseDataset):
        catalog_dir: str = os.path.abspath(&#34;./data&#34;)



class Herbarium2022DataModule(BaseDataModule):
        dataset_cls = Herbarium2022Dataset
        transform_cfg = DEFAULT_TRANSFORM_CFG


        def __init__(self,
                                 catalog_dir: Optional[str]=None,
                                 label_col=&#34;scientificName&#34;,
                                 train_size=0.7,
                                 shuffle: bool=True,
                                 seed=14,
                                 batch_size: int = 128,
                                 num_workers: int = None,
                                 pin_memory: bool=True,
                                 persistent_workers: Optional[bool]=False,
                                 train_transform=None,
                                 val_transform=None,
                                 test_transform=None,
                                 transform_cfg=None,
                                 remove_transforms: bool=False,
                                 image_reader: Callable=&#34;default&#34;, #Image.open,
                                 **kwargs
        ):
                super().__init__()
                
                self.catalog_dir = catalog_dir
                
                self.label_col = label_col
                self.train_size = train_size
                self.shuffle = shuffle
                self.seed = seed
                self.batch_size = batch_size
                self.num_workers = num_workers if num_workers is not None else mproc.cpu_count()
                self.pin_memory = pin_memory
                self.persistent_workers = persistent_workers
                self.image_reader = image_reader

                self.setup_transforms(transform_cfg=transform_cfg,
                                                          train_transform=train_transform,
                                                          val_transform=val_transform,
                                                          test_transform=test_transform,
                                                          remove_transforms=remove_transforms)
                self.cfg = self.get_cfg()
                self.kwargs = kwargs



        def setup(self, stage=None):
                subsets=[]
                if stage in [&#34;train&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.train_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                          subset=&#34;train&#34;,
                                                                                                          label_col=self.label_col,
                                                                                                          train_size=self.train_size,
                                                                                                          shuffle=self.shuffle,
                                                                                                          seed=self.seed,
                                                                                                          transform=self.train_transform)
                        subsets.append(&#34;train&#34;)
                if stage in [&#34;val&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.val_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                        subset=&#34;val&#34;,
                                                                                                        label_col=self.label_col,
                                                                                                        train_size=self.train_size,
                                                                                                        shuffle=self.shuffle,
                                                                                                        seed=self.seed,
                                                                                                        transform=self.val_transform)
                        subsets.append(&#34;val&#34;)
                if stage in [&#34;test&#34;, &#34;all&#34;, None]:
                        self.test_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                         subset=&#34;test&#34;,
                                                                                                         label_col=self.label_col,
                                                                                                         train_size=self.train_size,
                                                                                                         shuffle=self.shuffle,
                                                                                                         seed=self.seed,
                                                                                                         transform=self.test_transform)
                        subsets.append(&#34;test&#34;)
                        
                for s in subsets:
                        self.get_dataset_size(subset=s,
                                                                  verbose=True)
                        
                self.set_image_reader(self.image_reader)












#################
##################













from imutils.big import make_train_val_test_splits as leavesdb_utils
from imutils.big.make_train_val_test_splits import main as make_train_val_test_splits

                
class ExtantLeavesDataset(BaseDataset):
        catalog_dir: str = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/leavesdb-v1_1/Extant_Leaves_family_10_512/splits/splits=(0.5,0.2,0.3)&#34;

        def __init__(self,
                                 catalog_dir: Optional[str]=None,
                                 subset: str=&#34;train&#34;,
                                 label_col: str=&#34;family&#34;,
                                 splits: float=(0.5,0.2,0.3),
                                 shuffle: bool=True,
                                 seed: int=14,
                                 image_reader: Union[Callable,str]=&#34;default&#34;, #Image.open,
                                 preprocess: Callable=None,
                                 transform: Callable=None):
                &#34;&#34;&#34;
                
                Arguments:
                
                        catalog_dir: Optional[str]=None,
                                
                        subset: str=&#34;train&#34;,
                        
                        label_col: str=&#34;family&#34;,
                                Column containing the fully decoded str labels
                        splits: float=(0.5,0.2,0.3),
                        shuffle: bool=True,
                        seed: int=14,
                        image_reader: Union[Callable,str]=&#34;default&#34;, #Image.open,
                        preprocess: Callable=None,
                        transform: Callable=None
                
                &#34;&#34;&#34;
                # super().__init__()
                self.x_col = &#34;path&#34;
                self.y_col = &#34;y&#34;
                self.id_col = &#34;catalog_number&#34;

                self.catalog_dir = catalog_dir or self.catalog_dir

                self.label_col = label_col
                self.splits = splits
                self.shuffle = shuffle
                self.seed = seed
                self.subset = subset
                self.is_supervised = bool(subset != &#34;test&#34;)
                self.set_image_reader(image_reader)
                self.preprocess = preprocess
                self.transform = transform
                self.setup()
                                
        @classmethod
        def from_cfg(cls,
                                 cfg: DictConfig,
                                 **kwargs):
                cfg = OmegaConf.merge(cfg, kwargs)
                return cls(**cfg)


        def get_cfg(self,
                                 cfg: DictConfig=None,
                                 **kwargs):
                cfg=cfg or {}
                default_cfg = DictConfig(dict(                  
                        catalog_dir=self.catalog_dir or None,
                        subset=self.subset or &#34;train&#34;,
                        label_col=self.label_col or &#34;family&#34;,
                        splits=self.splits or (0.5,0.2,0.3),
                        shuffle=self.shuffle,
                        seed=self.seed or 14))
                
                cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
                return cfg


        @property
        def splits_dir(self) -&gt; Path:
                return leavesdb_utils.find_data_splits_dir(source_dir=self.catalog_dir,
                                                                                                   splits=self.splits)
        
        @property
        def split_file_path(self) -&gt; Path:
                &#34;&#34;&#34;
                Should return the path of this subset&#39;s on-disk csv catalog file. I emphasize should.
                &#34;&#34;&#34;
                return self.splits_dir / f&#34;{self.subset}_metadata.csv&#34;
        
        
        @property
        def already_built(self) -&gt; bool:
                return leavesdb_utils.check_already_built(self.splits_dir)

        
        def prepare_metadata(self):

                if not self.already_built:
                        
                        data = make_train_val_test_splits(source_dir=self.catalog_dir,
                                                                                 splits_dir=self.splits_dir,
                                                                                 label_col=self.label_col,
                                                                                 splits=args.splits,
                                                                                 seed=self.seed)
                        return data


        def get_data_subset(self,
                                                subset: str=&#34;train&#34;) -&gt; Tuple[&#34;LabelEncoder&#34;, pd.DataFrame]:
                &#34;&#34;&#34;
                Read the selected data subset into a pd.DataFrame
                
                Returns a Tuple containing an sklearn LabelEncoder and a pd.DataFrame of the subset&#39;s data catalog
                &#34;&#34;&#34;
                data = leavesdb_utils.read_encoded_splits(source_dir=self.splits_dir,
                                                                                                  include=[subset])
                encoder = data[&#34;label_encoder&#34;]
                data = data[&#34;subsets&#34;][subset]
                
                return encoder, data

        
        def setup(self):
                &#34;&#34;&#34;
                Assigns the following instance attributes:
                
                        ::self.label_encoder
                        ::self.df
                        ::self.paths
                        ::self.targets
                        ::self.num_classes
                &#34;&#34;&#34;
                data = self.prepare_metadata()
                if data is None:
                        encoder, data = self.get_data_subset(subset=self.subset)
                else:
                        encoder = data[&#34;label_encoder&#34;]
                        data = data[&#34;subsets&#34;][self.subset]

                if isinstance(encoder, preprocessing.LabelEncoder):
                        &#34;&#34;&#34;
                        Auto wraps any sklearn LabelEncoder in our custom class.
                        (Added 2022-03-25 - untested)
                        &#34;&#34;&#34;
                        encoder = label_utils.LabelEncoder.from_sklearn(encoder)

                setattr(data, &#34;label_encoder&#34;, encoder)
                self.label_encoder = encoder
                self.df = data
                
                if self.shuffle:
                        self.df = self.df.sample(frac=1, random_state=self.seed).reset_index(drop=False)

                if self.id_col in self.df.columns:
                        self.image_ids = self.df[self.id_col]
                self.paths = self.df[self.x_col]

                if self.is_supervised:
                        # self.imgs = 
                        self.targets = self.df[self.y_col]
                        self.num_classes = len(set(self.df[self.y_col]))
                else:
                        self.targets = None
                        self.num_classes = -1 # Or 0?


class ExtantLeavesDataModule(BaseDataModule):
        dataset_cls = ExtantLeavesDataset       
        transform_cfg = DEFAULT_TRANSFORM_CFG


        def __init__(self,
                                 catalog_dir: Optional[str]=None,
                                 label_col=&#34;family&#34;,
                                 splits: Tuple[float]=(0.5,0.2,0.3),
                                 shuffle: bool=True,
                                 seed=14,
                                 batch_size: int = 128,
                                 num_workers: int = None,
                                 pin_memory: bool=True,
                                 persistent_workers: Optional[bool]=False,
                                 train_transform=None,
                                 val_transform=None,
                                 test_transform=None,
                                 transform_cfg=None,
                                 remove_transforms: bool=False,
                                 image_reader: Callable=&#34;default&#34;, #Image.open,
                                 **kwargs
        ):
                super().__init__()
                
                self.catalog_dir = catalog_dir
                
                self.label_col = label_col
                self.splits = splits
                self.shuffle = shuffle
                self.seed = seed
                self.batch_size = batch_size
                self.num_workers = num_workers if num_workers is not None else mproc.cpu_count()
                self.pin_memory = pin_memory
                self.persistent_workers = persistent_workers
                self.image_reader = image_reader

                self.setup_transforms(transform_cfg=transform_cfg,
                                                          train_transform=train_transform,
                                                          val_transform=val_transform,
                                                          test_transform=test_transform,
                                                          remove_transforms=remove_transforms)
                self.cfg = self.get_cfg()
                self.kwargs = kwargs
                

        @classmethod
        def from_cfg(cls,
                                 cfg: DictConfig,
                                 **kwargs):
                cfg = OmegaConf.merge(cfg, kwargs)
                return cls(**cfg)

        def get_cfg(self,
                                 cfg: DictConfig=None,
                                 **kwargs):
                cfg=cfg or {}
                default_cfg = DictConfig(dict(
                        catalog_dir=self.catalog_dir or None,
                        label_col=self.label_col or &#34;family&#34;,
                        splits=self.splits or (0.5,0.2,0.3),
                        shuffle=self.shuffle,
                        seed=self.seed or 14,
                        batch_size = self.batch_size or 128,
                        num_workers = self.num_workers or None,
                        pin_memory=self.pin_memory,
                        transform_cfg=self.transform_cfg,
                        remove_transforms=self.remove_transforms,
                ))
                
                cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
                return cfg


        def setup(self, stage=None):
                subsets=[]
                if stage in [&#34;train&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.train_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                          subset=&#34;train&#34;,
                                                                                                          label_col=self.label_col,
                                                                                                          splits=self.splits,
                                                                                                          shuffle=self.shuffle,
                                                                                                          seed=self.seed,
                                                                                                          transform=self.train_transform)
                        subsets.append(&#34;train&#34;)
                if stage in [&#34;val&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.val_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                        subset=&#34;val&#34;,
                                                                                                        label_col=self.label_col,
                                                                                                        splits=self.splits,
                                                                                                        shuffle=self.shuffle,
                                                                                                        seed=self.seed,
                                                                                                        transform=self.val_transform)
                        subsets.append(&#34;val&#34;)
                if stage in [&#34;test&#34;, &#34;all&#34;, None]:
                        self.test_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                         subset=&#34;test&#34;,
                                                                                                         label_col=self.label_col,
                                                                                                         splits=self.splits,
                                                                                                         shuffle=self.shuffle,
                                                                                                         seed=self.seed,
                                                                                                         transform=self.test_transform)
                        subsets.append(&#34;test&#34;)
                        
                for s in subsets:
                        self.get_dataset_size(subset=s,
                                                                  verbose=True)
                        
                self.set_image_reader(self.image_reader)






# class AutoDataModule:
#       &#34;&#34;&#34;
        
        
#       &#34;&#34;&#34;
        
#       def __init__(self)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imutils.ml.data.datamodule.get_default_transforms"><code class="name flex">
<span>def <span class="ident">get_default_transforms</span></span>(<span>mode: str = 'train', compose: bool = True, config={'preprocess': {'train': {'resize': 512}, 'val': {'resize': 256}, 'test': {'resize': 256}}, 'batch_transform': {'train': {'random_resize_crop': 224}, 'val': {'center_crop': 224}, 'test': {'center_crop': 224}}, 'normalize': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), 'apply_color_transform': False, 'random_flips': True, 'skip_augmentations': False}) ‑> Tuple[Callable]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_default_transforms(
                mode: str=&#34;train&#34;,
                compose: bool=True,
                config = dict(
                        preprocess={
                                &#39;train&#39;: {&#39;resize&#39;: 512},
                                &#39;val&#39;: {&#39;resize&#39;: 256},
                                &#39;test&#39;: {&#39;resize&#39;: 256}},

                        batch_transform={
                                &#39;train&#39;: {&#39;random_resize_crop&#39;: 224}, 
                                &#39;val&#39;: {&#39;center_crop&#39;: 224},
                                &#39;test&#39;: {&#39;center_crop&#39;: 224}},
                        normalize=(
                                [0.485, 0.456, 0.406],
                                [0.229, 0.224, 0.225]
                        ),
                        apply_color_transform=False,
                        random_flips=True,
                        skip_augmentations=False
                )
        ) -&gt; Tuple[Callable]:
        
        config = OmegaConf.merge(DEFAULT_CFG, config)
        
        if config[&#34;preprocess&#34;][mode].get(&#34;resize&#34;, None):
                preprocess_transforms = Preprocess(mode=mode, 
                                                                                   resize=config[&#34;preprocess&#34;][mode][&#34;resize&#34;])
        else:
                preprocess_transforms = T.ToTensor()
        
        if mode == &#34;train&#34;:
                random_resize_crop = config[&#34;batch_transform&#34;][&#34;train&#34;][&#34;random_resize_crop&#34;]
                center_crop = None
        else:
                random_resize_crop = None
                center_crop = config[&#34;batch_transform&#34;][mode][&#34;center_crop&#34;]
        apply_color_jitter = config.get(&#34;apply_color_transform&#34;, False)
        random_flips = config.get(&#34;random_flips&#34;, True)
        skip_augmentations = config.get(&#34;skip_augmentations&#34;, False)
        normalize = config.get(&#34;normalize&#34;, 
                                                   (
                [0,0,0],
                [1,1,1]
        )
                                                  )
        
        batch_transforms = BatchTransform(mode=mode,
                                                                           random_resize_crop=random_resize_crop,
                                                                           center_crop=center_crop,
                                                                           apply_color_jitter = apply_color_jitter,
                                                                           random_flips = random_flips,
                                                                           normalize = normalize,
                                                                           skip_augmentations=skip_augmentations)
        
        transforms = (preprocess_transforms, 
                                  batch_transforms)
        if compose:
                transforms = T.Compose(transforms)
        return transforms</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataModule"><code class="flex name class">
<span>class <span class="ident">ExtantLeavesDataModule</span></span>
<span>(</span><span>catalog_dir: Optional[str] = None, label_col='family', splits: Tuple[float] = (0.5, 0.2, 0.3), shuffle: bool = True, seed=14, batch_size: int = 128, num_workers: int = None, pin_memory: bool = True, persistent_workers: Optional[bool] = False, train_transform=None, val_transform=None, test_transform=None, transform_cfg=None, remove_transforms: bool = False, image_reader: Callable = 'default', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A DataModule standardizes the training, val, test splits, data preparation and transforms. The main
advantage is consistent data splits, data preparation and transforms across models.</p>
<p>Example::</p>
<pre><code>class MyDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
    def prepare_data(self):
        # download, split, etc...
        # only called on 1 GPU/TPU in distributed
    def setup(self, stage):
        # make assignments here (val/train/test split)
        # called on every process in DDP
    def train_dataloader(self):
        train_split = Dataset(...)
        return DataLoader(train_split)
    def val_dataloader(self):
        val_split = Dataset(...)
        return DataLoader(val_split)
    def test_dataloader(self):
        test_split = Dataset(...)
        return DataLoader(test_split)
    def teardown(self):
        # clean up after fit or test
        # called on every process in DDP
</code></pre>
<p>A DataModule implements 6 key methods:</p>
<ul>
<li><strong>prepare_data</strong> (things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode).</li>
<li><strong>setup</strong>
(things to do on every accelerator in distributed mode).</li>
<li><strong>train_dataloader</strong> the training dataloader.</li>
<li><strong>val_dataloader</strong> the val dataloader(s).</li>
<li><strong>test_dataloader</strong> the test dataloader(s).</li>
<li><strong>teardown</strong> (things to do on every accelerator in distributed mode when finished)</li>
</ul>
<p>This allows you to share a full dataset without explaining how to download, split, transform, and process the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExtantLeavesDataModule(BaseDataModule):
        dataset_cls = ExtantLeavesDataset       
        transform_cfg = DEFAULT_TRANSFORM_CFG


        def __init__(self,
                                 catalog_dir: Optional[str]=None,
                                 label_col=&#34;family&#34;,
                                 splits: Tuple[float]=(0.5,0.2,0.3),
                                 shuffle: bool=True,
                                 seed=14,
                                 batch_size: int = 128,
                                 num_workers: int = None,
                                 pin_memory: bool=True,
                                 persistent_workers: Optional[bool]=False,
                                 train_transform=None,
                                 val_transform=None,
                                 test_transform=None,
                                 transform_cfg=None,
                                 remove_transforms: bool=False,
                                 image_reader: Callable=&#34;default&#34;, #Image.open,
                                 **kwargs
        ):
                super().__init__()
                
                self.catalog_dir = catalog_dir
                
                self.label_col = label_col
                self.splits = splits
                self.shuffle = shuffle
                self.seed = seed
                self.batch_size = batch_size
                self.num_workers = num_workers if num_workers is not None else mproc.cpu_count()
                self.pin_memory = pin_memory
                self.persistent_workers = persistent_workers
                self.image_reader = image_reader

                self.setup_transforms(transform_cfg=transform_cfg,
                                                          train_transform=train_transform,
                                                          val_transform=val_transform,
                                                          test_transform=test_transform,
                                                          remove_transforms=remove_transforms)
                self.cfg = self.get_cfg()
                self.kwargs = kwargs
                

        @classmethod
        def from_cfg(cls,
                                 cfg: DictConfig,
                                 **kwargs):
                cfg = OmegaConf.merge(cfg, kwargs)
                return cls(**cfg)

        def get_cfg(self,
                                 cfg: DictConfig=None,
                                 **kwargs):
                cfg=cfg or {}
                default_cfg = DictConfig(dict(
                        catalog_dir=self.catalog_dir or None,
                        label_col=self.label_col or &#34;family&#34;,
                        splits=self.splits or (0.5,0.2,0.3),
                        shuffle=self.shuffle,
                        seed=self.seed or 14,
                        batch_size = self.batch_size or 128,
                        num_workers = self.num_workers or None,
                        pin_memory=self.pin_memory,
                        transform_cfg=self.transform_cfg,
                        remove_transforms=self.remove_transforms,
                ))
                
                cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
                return cfg


        def setup(self, stage=None):
                subsets=[]
                if stage in [&#34;train&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.train_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                          subset=&#34;train&#34;,
                                                                                                          label_col=self.label_col,
                                                                                                          splits=self.splits,
                                                                                                          shuffle=self.shuffle,
                                                                                                          seed=self.seed,
                                                                                                          transform=self.train_transform)
                        subsets.append(&#34;train&#34;)
                if stage in [&#34;val&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.val_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                        subset=&#34;val&#34;,
                                                                                                        label_col=self.label_col,
                                                                                                        splits=self.splits,
                                                                                                        shuffle=self.shuffle,
                                                                                                        seed=self.seed,
                                                                                                        transform=self.val_transform)
                        subsets.append(&#34;val&#34;)
                if stage in [&#34;test&#34;, &#34;all&#34;, None]:
                        self.test_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                         subset=&#34;test&#34;,
                                                                                                         label_col=self.label_col,
                                                                                                         splits=self.splits,
                                                                                                         shuffle=self.shuffle,
                                                                                                         seed=self.seed,
                                                                                                         transform=self.test_transform)
                        subsets.append(&#34;test&#34;)
                        
                for s in subsets:
                        self.get_dataset_size(subset=s,
                                                                  verbose=True)
                        
                self.set_image_reader(self.image_reader)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>imutils.ml.data.datamodule.BaseDataModule</li>
<li>pytorch_lightning.core.datamodule.LightningDataModule</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataModule.dataset_cls"><code class="name">var <span class="ident">dataset_cls</span></code></dt>
<dd>
<div class="desc"><p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
sampler that yields integral indices.
To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.</p>
</div></div>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataModule.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataModule.transform_cfg"><code class="name">var <span class="ident">transform_cfg</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataModule.from_cfg"><code class="name flex">
<span>def <span class="ident">from_cfg</span></span>(<span>cfg: omegaconf.dictconfig.DictConfig, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_cfg(cls,
                         cfg: DictConfig,
                         **kwargs):
        cfg = OmegaConf.merge(cfg, kwargs)
        return cls(**cfg)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataModule.get_cfg"><code class="name flex">
<span>def <span class="ident">get_cfg</span></span>(<span>self, cfg: omegaconf.dictconfig.DictConfig = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cfg(self,
                         cfg: DictConfig=None,
                         **kwargs):
        cfg=cfg or {}
        default_cfg = DictConfig(dict(
                catalog_dir=self.catalog_dir or None,
                label_col=self.label_col or &#34;family&#34;,
                splits=self.splits or (0.5,0.2,0.3),
                shuffle=self.shuffle,
                seed=self.seed or 14,
                batch_size = self.batch_size or 128,
                num_workers = self.num_workers or None,
                pin_memory=self.pin_memory,
                transform_cfg=self.transform_cfg,
                remove_transforms=self.remove_transforms,
        ))
        
        cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
        return cfg</code></pre>
</details>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataModule.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, stage=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stage</code></strong></dt>
<dd>either <code>'fit'</code>, <code>'validate'</code>, <code>'test'</code>, or <code>'predict'</code></dd>
</dl>
<p>Example::</p>
<pre><code>class LitModel(...):
    def __init__(self):
        self.l1 = None

    def prepare_data(self):
        download_data()
        tokenize()

        # don't do this
        self.something = else

    def setup(stage):
        data = Load_data(...)
        self.l1 = nn.Linear(28, data.num_classes)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, stage=None):
        subsets=[]
        if stage in [&#34;train&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                self.train_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                  subset=&#34;train&#34;,
                                                                                                  label_col=self.label_col,
                                                                                                  splits=self.splits,
                                                                                                  shuffle=self.shuffle,
                                                                                                  seed=self.seed,
                                                                                                  transform=self.train_transform)
                subsets.append(&#34;train&#34;)
        if stage in [&#34;val&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                self.val_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                subset=&#34;val&#34;,
                                                                                                label_col=self.label_col,
                                                                                                splits=self.splits,
                                                                                                shuffle=self.shuffle,
                                                                                                seed=self.seed,
                                                                                                transform=self.val_transform)
                subsets.append(&#34;val&#34;)
        if stage in [&#34;test&#34;, &#34;all&#34;, None]:
                self.test_dataset = self.dataset_cls(catalog_dir=self.catalog_dir,
                                                                                                 subset=&#34;test&#34;,
                                                                                                 label_col=self.label_col,
                                                                                                 splits=self.splits,
                                                                                                 shuffle=self.shuffle,
                                                                                                 seed=self.seed,
                                                                                                 transform=self.test_transform)
                subsets.append(&#34;test&#34;)
                
        for s in subsets:
                self.get_dataset_size(subset=s,
                                                          verbose=True)
                
        self.set_image_reader(self.image_reader)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset"><code class="flex name class">
<span>class <span class="ident">ExtantLeavesDataset</span></span>
<span>(</span><span>catalog_dir: Optional[str] = None, subset: str = 'train', label_col: str = 'family', splits: float = (0.5, 0.2, 0.3), shuffle: bool = True, seed: int = 14, image_reader: Union[Callable, str] = 'default', preprocess: Callable = None, transform: Callable = None)</span>
</code></dt>
<dd>
<div class="desc"><p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
sampler that yields integral indices.
To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.</p>
</div>
<h2 id="arguments">Arguments</h2>
<p>catalog_dir: Optional[str]=None,</p>
<p>subset: str="train",</p>
<p>label_col: str="family",
Column containing the fully decoded str labels
splits: float=(0.5,0.2,0.3),
shuffle: bool=True,
seed: int=14,
image_reader: Union[Callable,str]="default", #Image.open,
preprocess: Callable=None,
transform: Callable=None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExtantLeavesDataset(BaseDataset):
        catalog_dir: str = &#34;/media/data_cifs/projects/prj_fossils/users/jacob/data/leavesdb-v1_1/Extant_Leaves_family_10_512/splits/splits=(0.5,0.2,0.3)&#34;

        def __init__(self,
                                 catalog_dir: Optional[str]=None,
                                 subset: str=&#34;train&#34;,
                                 label_col: str=&#34;family&#34;,
                                 splits: float=(0.5,0.2,0.3),
                                 shuffle: bool=True,
                                 seed: int=14,
                                 image_reader: Union[Callable,str]=&#34;default&#34;, #Image.open,
                                 preprocess: Callable=None,
                                 transform: Callable=None):
                &#34;&#34;&#34;
                
                Arguments:
                
                        catalog_dir: Optional[str]=None,
                                
                        subset: str=&#34;train&#34;,
                        
                        label_col: str=&#34;family&#34;,
                                Column containing the fully decoded str labels
                        splits: float=(0.5,0.2,0.3),
                        shuffle: bool=True,
                        seed: int=14,
                        image_reader: Union[Callable,str]=&#34;default&#34;, #Image.open,
                        preprocess: Callable=None,
                        transform: Callable=None
                
                &#34;&#34;&#34;
                # super().__init__()
                self.x_col = &#34;path&#34;
                self.y_col = &#34;y&#34;
                self.id_col = &#34;catalog_number&#34;

                self.catalog_dir = catalog_dir or self.catalog_dir

                self.label_col = label_col
                self.splits = splits
                self.shuffle = shuffle
                self.seed = seed
                self.subset = subset
                self.is_supervised = bool(subset != &#34;test&#34;)
                self.set_image_reader(image_reader)
                self.preprocess = preprocess
                self.transform = transform
                self.setup()
                                
        @classmethod
        def from_cfg(cls,
                                 cfg: DictConfig,
                                 **kwargs):
                cfg = OmegaConf.merge(cfg, kwargs)
                return cls(**cfg)


        def get_cfg(self,
                                 cfg: DictConfig=None,
                                 **kwargs):
                cfg=cfg or {}
                default_cfg = DictConfig(dict(                  
                        catalog_dir=self.catalog_dir or None,
                        subset=self.subset or &#34;train&#34;,
                        label_col=self.label_col or &#34;family&#34;,
                        splits=self.splits or (0.5,0.2,0.3),
                        shuffle=self.shuffle,
                        seed=self.seed or 14))
                
                cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
                return cfg


        @property
        def splits_dir(self) -&gt; Path:
                return leavesdb_utils.find_data_splits_dir(source_dir=self.catalog_dir,
                                                                                                   splits=self.splits)
        
        @property
        def split_file_path(self) -&gt; Path:
                &#34;&#34;&#34;
                Should return the path of this subset&#39;s on-disk csv catalog file. I emphasize should.
                &#34;&#34;&#34;
                return self.splits_dir / f&#34;{self.subset}_metadata.csv&#34;
        
        
        @property
        def already_built(self) -&gt; bool:
                return leavesdb_utils.check_already_built(self.splits_dir)

        
        def prepare_metadata(self):

                if not self.already_built:
                        
                        data = make_train_val_test_splits(source_dir=self.catalog_dir,
                                                                                 splits_dir=self.splits_dir,
                                                                                 label_col=self.label_col,
                                                                                 splits=args.splits,
                                                                                 seed=self.seed)
                        return data


        def get_data_subset(self,
                                                subset: str=&#34;train&#34;) -&gt; Tuple[&#34;LabelEncoder&#34;, pd.DataFrame]:
                &#34;&#34;&#34;
                Read the selected data subset into a pd.DataFrame
                
                Returns a Tuple containing an sklearn LabelEncoder and a pd.DataFrame of the subset&#39;s data catalog
                &#34;&#34;&#34;
                data = leavesdb_utils.read_encoded_splits(source_dir=self.splits_dir,
                                                                                                  include=[subset])
                encoder = data[&#34;label_encoder&#34;]
                data = data[&#34;subsets&#34;][subset]
                
                return encoder, data

        
        def setup(self):
                &#34;&#34;&#34;
                Assigns the following instance attributes:
                
                        ::self.label_encoder
                        ::self.df
                        ::self.paths
                        ::self.targets
                        ::self.num_classes
                &#34;&#34;&#34;
                data = self.prepare_metadata()
                if data is None:
                        encoder, data = self.get_data_subset(subset=self.subset)
                else:
                        encoder = data[&#34;label_encoder&#34;]
                        data = data[&#34;subsets&#34;][self.subset]

                if isinstance(encoder, preprocessing.LabelEncoder):
                        &#34;&#34;&#34;
                        Auto wraps any sklearn LabelEncoder in our custom class.
                        (Added 2022-03-25 - untested)
                        &#34;&#34;&#34;
                        encoder = label_utils.LabelEncoder.from_sklearn(encoder)

                setattr(data, &#34;label_encoder&#34;, encoder)
                self.label_encoder = encoder
                self.df = data
                
                if self.shuffle:
                        self.df = self.df.sample(frac=1, random_state=self.seed).reset_index(drop=False)

                if self.id_col in self.df.columns:
                        self.image_ids = self.df[self.id_col]
                self.paths = self.df[self.x_col]

                if self.is_supervised:
                        # self.imgs = 
                        self.targets = self.df[self.y_col]
                        self.num_classes = len(set(self.df[self.y_col]))
                else:
                        self.targets = None
                        self.num_classes = -1 # Or 0?</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>imutils.ml.data.datamodule.BaseDataset</li>
<li>imutils.ml.data.datamodule.AbstractCatalogDataset</li>
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.catalog_dir"><code class="name">var <span class="ident">catalog_dir</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.from_cfg"><code class="name flex">
<span>def <span class="ident">from_cfg</span></span>(<span>cfg: omegaconf.dictconfig.DictConfig, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_cfg(cls,
                         cfg: DictConfig,
                         **kwargs):
        cfg = OmegaConf.merge(cfg, kwargs)
        return cls(**cfg)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.already_built"><code class="name">var <span class="ident">already_built</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def already_built(self) -&gt; bool:
        return leavesdb_utils.check_already_built(self.splits_dir)</code></pre>
</details>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.split_file_path"><code class="name">var <span class="ident">split_file_path</span> : pathlib.Path</code></dt>
<dd>
<div class="desc"><p>Should return the path of this subset's on-disk csv catalog file. I emphasize should.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def split_file_path(self) -&gt; Path:
        &#34;&#34;&#34;
        Should return the path of this subset&#39;s on-disk csv catalog file. I emphasize should.
        &#34;&#34;&#34;
        return self.splits_dir / f&#34;{self.subset}_metadata.csv&#34;</code></pre>
</details>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.splits_dir"><code class="name">var <span class="ident">splits_dir</span> : pathlib.Path</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def splits_dir(self) -&gt; Path:
        return leavesdb_utils.find_data_splits_dir(source_dir=self.catalog_dir,
                                                                                           splits=self.splits)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.get_cfg"><code class="name flex">
<span>def <span class="ident">get_cfg</span></span>(<span>self, cfg: omegaconf.dictconfig.DictConfig = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cfg(self,
                         cfg: DictConfig=None,
                         **kwargs):
        cfg=cfg or {}
        default_cfg = DictConfig(dict(                  
                catalog_dir=self.catalog_dir or None,
                subset=self.subset or &#34;train&#34;,
                label_col=self.label_col or &#34;family&#34;,
                splits=self.splits or (0.5,0.2,0.3),
                shuffle=self.shuffle,
                seed=self.seed or 14))
        
        cfg = OmegaConf.merge(default_cfg, cfg, kwargs)
        return cfg</code></pre>
</details>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.get_data_subset"><code class="name flex">
<span>def <span class="ident">get_data_subset</span></span>(<span>self, subset: str = 'train') ‑> Tuple[LabelEncoder, pandas.core.frame.DataFrame]</span>
</code></dt>
<dd>
<div class="desc"><p>Read the selected data subset into a pd.DataFrame</p>
<p>Returns a Tuple containing an sklearn LabelEncoder and a pd.DataFrame of the subset's data catalog</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_subset(self,
                                        subset: str=&#34;train&#34;) -&gt; Tuple[&#34;LabelEncoder&#34;, pd.DataFrame]:
        &#34;&#34;&#34;
        Read the selected data subset into a pd.DataFrame
        
        Returns a Tuple containing an sklearn LabelEncoder and a pd.DataFrame of the subset&#39;s data catalog
        &#34;&#34;&#34;
        data = leavesdb_utils.read_encoded_splits(source_dir=self.splits_dir,
                                                                                          include=[subset])
        encoder = data[&#34;label_encoder&#34;]
        data = data[&#34;subsets&#34;][subset]
        
        return encoder, data</code></pre>
</details>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.prepare_metadata"><code class="name flex">
<span>def <span class="ident">prepare_metadata</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_metadata(self):

        if not self.already_built:
                
                data = make_train_val_test_splits(source_dir=self.catalog_dir,
                                                                         splits_dir=self.splits_dir,
                                                                         label_col=self.label_col,
                                                                         splits=args.splits,
                                                                         seed=self.seed)
                return data</code></pre>
</details>
</dd>
<dt id="imutils.ml.data.datamodule.ExtantLeavesDataset.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Assigns the following instance attributes:</p>
<pre><code>    ::self.label_encoder
    ::self.df
    ::self.paths
    ::self.targets
    ::self.num_classes
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self):
        &#34;&#34;&#34;
        Assigns the following instance attributes:
        
                ::self.label_encoder
                ::self.df
                ::self.paths
                ::self.targets
                ::self.num_classes
        &#34;&#34;&#34;
        data = self.prepare_metadata()
        if data is None:
                encoder, data = self.get_data_subset(subset=self.subset)
        else:
                encoder = data[&#34;label_encoder&#34;]
                data = data[&#34;subsets&#34;][self.subset]

        if isinstance(encoder, preprocessing.LabelEncoder):
                &#34;&#34;&#34;
                Auto wraps any sklearn LabelEncoder in our custom class.
                (Added 2022-03-25 - untested)
                &#34;&#34;&#34;
                encoder = label_utils.LabelEncoder.from_sklearn(encoder)

        setattr(data, &#34;label_encoder&#34;, encoder)
        self.label_encoder = encoder
        self.df = data
        
        if self.shuffle:
                self.df = self.df.sample(frac=1, random_state=self.seed).reset_index(drop=False)

        if self.id_col in self.df.columns:
                self.image_ids = self.df[self.id_col]
        self.paths = self.df[self.x_col]

        if self.is_supervised:
                # self.imgs = 
                self.targets = self.df[self.y_col]
                self.num_classes = len(set(self.df[self.y_col]))
        else:
                self.targets = None
                self.num_classes = -1 # Or 0?</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imutils.ml.data.datamodule.Herbarium2022DataModule"><code class="flex name class">
<span>class <span class="ident">Herbarium2022DataModule</span></span>
<span>(</span><span>catalog_dir: Optional[str] = None, label_col='scientificName', train_size=0.7, shuffle: bool = True, seed=14, batch_size: int = 128, num_workers: int = None, pin_memory: bool = True, persistent_workers: Optional[bool] = False, train_transform=None, val_transform=None, test_transform=None, transform_cfg=None, remove_transforms: bool = False, image_reader: Callable = 'default', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A DataModule standardizes the training, val, test splits, data preparation and transforms. The main
advantage is consistent data splits, data preparation and transforms across models.</p>
<p>Example::</p>
<pre><code>class MyDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
    def prepare_data(self):
        # download, split, etc...
        # only called on 1 GPU/TPU in distributed
    def setup(self, stage):
        # make assignments here (val/train/test split)
        # called on every process in DDP
    def train_dataloader(self):
        train_split = Dataset(...)
        return DataLoader(train_split)
    def val_dataloader(self):
        val_split = Dataset(...)
        return DataLoader(val_split)
    def test_dataloader(self):
        test_split = Dataset(...)
        return DataLoader(test_split)
    def teardown(self):
        # clean up after fit or test
        # called on every process in DDP
</code></pre>
<p>A DataModule implements 6 key methods:</p>
<ul>
<li><strong>prepare_data</strong> (things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode).</li>
<li><strong>setup</strong>
(things to do on every accelerator in distributed mode).</li>
<li><strong>train_dataloader</strong> the training dataloader.</li>
<li><strong>val_dataloader</strong> the val dataloader(s).</li>
<li><strong>test_dataloader</strong> the test dataloader(s).</li>
<li><strong>teardown</strong> (things to do on every accelerator in distributed mode when finished)</li>
</ul>
<p>This allows you to share a full dataset without explaining how to download, split, transform, and process the data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Herbarium2022DataModule(BaseDataModule):
        dataset_cls = Herbarium2022Dataset
        transform_cfg = DEFAULT_TRANSFORM_CFG


        def __init__(self,
                                 catalog_dir: Optional[str]=None,
                                 label_col=&#34;scientificName&#34;,
                                 train_size=0.7,
                                 shuffle: bool=True,
                                 seed=14,
                                 batch_size: int = 128,
                                 num_workers: int = None,
                                 pin_memory: bool=True,
                                 persistent_workers: Optional[bool]=False,
                                 train_transform=None,
                                 val_transform=None,
                                 test_transform=None,
                                 transform_cfg=None,
                                 remove_transforms: bool=False,
                                 image_reader: Callable=&#34;default&#34;, #Image.open,
                                 **kwargs
        ):
                super().__init__()
                
                self.catalog_dir = catalog_dir
                
                self.label_col = label_col
                self.train_size = train_size
                self.shuffle = shuffle
                self.seed = seed
                self.batch_size = batch_size
                self.num_workers = num_workers if num_workers is not None else mproc.cpu_count()
                self.pin_memory = pin_memory
                self.persistent_workers = persistent_workers
                self.image_reader = image_reader

                self.setup_transforms(transform_cfg=transform_cfg,
                                                          train_transform=train_transform,
                                                          val_transform=val_transform,
                                                          test_transform=test_transform,
                                                          remove_transforms=remove_transforms)
                self.cfg = self.get_cfg()
                self.kwargs = kwargs



        def setup(self, stage=None):
                subsets=[]
                if stage in [&#34;train&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.train_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                          subset=&#34;train&#34;,
                                                                                                          label_col=self.label_col,
                                                                                                          train_size=self.train_size,
                                                                                                          shuffle=self.shuffle,
                                                                                                          seed=self.seed,
                                                                                                          transform=self.train_transform)
                        subsets.append(&#34;train&#34;)
                if stage in [&#34;val&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                        self.val_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                        subset=&#34;val&#34;,
                                                                                                        label_col=self.label_col,
                                                                                                        train_size=self.train_size,
                                                                                                        shuffle=self.shuffle,
                                                                                                        seed=self.seed,
                                                                                                        transform=self.val_transform)
                        subsets.append(&#34;val&#34;)
                if stage in [&#34;test&#34;, &#34;all&#34;, None]:
                        self.test_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                         subset=&#34;test&#34;,
                                                                                                         label_col=self.label_col,
                                                                                                         train_size=self.train_size,
                                                                                                         shuffle=self.shuffle,
                                                                                                         seed=self.seed,
                                                                                                         transform=self.test_transform)
                        subsets.append(&#34;test&#34;)
                        
                for s in subsets:
                        self.get_dataset_size(subset=s,
                                                                  verbose=True)
                        
                self.set_image_reader(self.image_reader)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>imutils.ml.data.datamodule.BaseDataModule</li>
<li>pytorch_lightning.core.datamodule.LightningDataModule</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imutils.ml.data.datamodule.Herbarium2022DataModule.dataset_cls"><code class="name">var <span class="ident">dataset_cls</span></code></dt>
<dd>
<div class="desc"><p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
sampler that yields integral indices.
To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.</p>
</div></div>
</dd>
<dt id="imutils.ml.data.datamodule.Herbarium2022DataModule.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imutils.ml.data.datamodule.Herbarium2022DataModule.transform_cfg"><code class="name">var <span class="ident">transform_cfg</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imutils.ml.data.datamodule.Herbarium2022DataModule.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, stage=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stage</code></strong></dt>
<dd>either <code>'fit'</code>, <code>'validate'</code>, <code>'test'</code>, or <code>'predict'</code></dd>
</dl>
<p>Example::</p>
<pre><code>class LitModel(...):
    def __init__(self):
        self.l1 = None

    def prepare_data(self):
        download_data()
        tokenize()

        # don't do this
        self.something = else

    def setup(stage):
        data = Load_data(...)
        self.l1 = nn.Linear(28, data.num_classes)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, stage=None):
        subsets=[]
        if stage in [&#34;train&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                self.train_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                  subset=&#34;train&#34;,
                                                                                                  label_col=self.label_col,
                                                                                                  train_size=self.train_size,
                                                                                                  shuffle=self.shuffle,
                                                                                                  seed=self.seed,
                                                                                                  transform=self.train_transform)
                subsets.append(&#34;train&#34;)
        if stage in [&#34;val&#34;, &#34;fit&#34;, &#34;all&#34;, None]:
                self.val_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                subset=&#34;val&#34;,
                                                                                                label_col=self.label_col,
                                                                                                train_size=self.train_size,
                                                                                                shuffle=self.shuffle,
                                                                                                seed=self.seed,
                                                                                                transform=self.val_transform)
                subsets.append(&#34;val&#34;)
        if stage in [&#34;test&#34;, &#34;all&#34;, None]:
                self.test_dataset = Herbarium2022Dataset(catalog_dir=self.catalog_dir,
                                                                                                 subset=&#34;test&#34;,
                                                                                                 label_col=self.label_col,
                                                                                                 train_size=self.train_size,
                                                                                                 shuffle=self.shuffle,
                                                                                                 seed=self.seed,
                                                                                                 transform=self.test_transform)
                subsets.append(&#34;test&#34;)
                
        for s in subsets:
                self.get_dataset_size(subset=s,
                                                          verbose=True)
                
        self.set_image_reader(self.image_reader)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imutils.ml.data.datamodule.Herbarium2022Dataset"><code class="flex name class">
<span>class <span class="ident">Herbarium2022Dataset</span></span>
<span>(</span><span>catalog_dir: Optional[str] = None, subset: str = 'train', label_col: str = 'scientificName', train_size: float = 0.7, shuffle: bool = True, seed: int = 14, image_reader: Union[Callable, str] = 'default', preprocess: Callable = None, transform: Callable = None)</span>
</code></dt>
<dd>
<div class="desc"><p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
sampler that yields integral indices.
To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.</p>
</div>
<h2 id="arguments">Arguments</h2>
<p>catalog_dir: Optional[str]=None,</p>
<p>subset: str="train",</p>
<p>label_col: str="scientificName",
Column containing the fully decoded str labels
train_size: float=0.7,
shuffle: bool=True,
seed: int=14,
image_reader: Union[Callable,str]="default", #Image.open,
preprocess: Callable=None,
transform: Callable=None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Herbarium2022Dataset(BaseDataset):
        catalog_dir: str = os.path.abspath(&#34;./data&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>imutils.ml.data.datamodule.BaseDataset</li>
<li>imutils.ml.data.datamodule.AbstractCatalogDataset</li>
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imutils.ml.data.datamodule.Herbarium2022Dataset.catalog_dir"><code class="name">var <span class="ident">catalog_dir</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils.ml.data" href="index.html">imutils.ml.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imutils.ml.data.datamodule.get_default_transforms" href="#imutils.ml.data.datamodule.get_default_transforms">get_default_transforms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataModule" href="#imutils.ml.data.datamodule.ExtantLeavesDataModule">ExtantLeavesDataModule</a></code></h4>
<ul class="two-column">
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataModule.dataset_cls" href="#imutils.ml.data.datamodule.ExtantLeavesDataModule.dataset_cls">dataset_cls</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataModule.from_cfg" href="#imutils.ml.data.datamodule.ExtantLeavesDataModule.from_cfg">from_cfg</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataModule.get_cfg" href="#imutils.ml.data.datamodule.ExtantLeavesDataModule.get_cfg">get_cfg</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataModule.name" href="#imutils.ml.data.datamodule.ExtantLeavesDataModule.name">name</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataModule.setup" href="#imutils.ml.data.datamodule.ExtantLeavesDataModule.setup">setup</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataModule.transform_cfg" href="#imutils.ml.data.datamodule.ExtantLeavesDataModule.transform_cfg">transform_cfg</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset" href="#imutils.ml.data.datamodule.ExtantLeavesDataset">ExtantLeavesDataset</a></code></h4>
<ul class="two-column">
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.already_built" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.already_built">already_built</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.catalog_dir" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.catalog_dir">catalog_dir</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.from_cfg" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.from_cfg">from_cfg</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.get_cfg" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.get_cfg">get_cfg</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.get_data_subset" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.get_data_subset">get_data_subset</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.prepare_metadata" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.prepare_metadata">prepare_metadata</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.setup" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.setup">setup</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.split_file_path" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.split_file_path">split_file_path</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.ExtantLeavesDataset.splits_dir" href="#imutils.ml.data.datamodule.ExtantLeavesDataset.splits_dir">splits_dir</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imutils.ml.data.datamodule.Herbarium2022DataModule" href="#imutils.ml.data.datamodule.Herbarium2022DataModule">Herbarium2022DataModule</a></code></h4>
<ul class="">
<li><code><a title="imutils.ml.data.datamodule.Herbarium2022DataModule.dataset_cls" href="#imutils.ml.data.datamodule.Herbarium2022DataModule.dataset_cls">dataset_cls</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.Herbarium2022DataModule.name" href="#imutils.ml.data.datamodule.Herbarium2022DataModule.name">name</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.Herbarium2022DataModule.setup" href="#imutils.ml.data.datamodule.Herbarium2022DataModule.setup">setup</a></code></li>
<li><code><a title="imutils.ml.data.datamodule.Herbarium2022DataModule.transform_cfg" href="#imutils.ml.data.datamodule.Herbarium2022DataModule.transform_cfg">transform_cfg</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imutils.ml.data.datamodule.Herbarium2022Dataset" href="#imutils.ml.data.datamodule.Herbarium2022Dataset">Herbarium2022Dataset</a></code></h4>
<ul class="">
<li><code><a title="imutils.ml.data.datamodule.Herbarium2022Dataset.catalog_dir" href="#imutils.ml.data.datamodule.Herbarium2022Dataset.catalog_dir">catalog_dir</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>