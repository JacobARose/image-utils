<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.ml.models.backbones.backbone API documentation</title>
<meta name="description" content="image-utils/imutils/ml/models/backbone.py …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.ml.models.backbones.backbone</code></h1>
</header>
<section id="section-intro">
<p>image-utils/imutils/ml/models/backbone.py</p>
<p>Author: Jacob A Rose
Created: Saturday May 29th, 2021</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
image-utils/imutils/ml/models/backbone.py

Author: Jacob A Rose
Created: Saturday May 29th, 2021

&#34;&#34;&#34;

from typing import Any, List, Optional, Dict, Tuple, Union
from omegaconf import DictConfig
import torch
from torch import nn, functional as F
import torchvision
from pytorch_lightning import LightningModule

import torchmetrics as metrics
import timm
from torchsummary import summary
import pandas as pd
from pathlib import Path
from stuf import stuf
import wandb
import numpy as np
from rich import print as pp
# from . import resnet, senet, efficientnet

from imutils.ml.models.layers.pool_layers import build_global_pool, Flatten

__all__ = [&#34;build_model&#34;, &#34;build_model_head&#34;, &#34;build_model_backbone&#34;, &#34;load_model_checkpoint&#34;]

# AVAILABLE_MODELS = {&#34;resnet&#34;:resnet.AVAILABLE_MODELS,
#                     &#34;senet&#34;:senet.AVAILABLE_MODELS,
#                     &#34;efficientnet&#34;:efficientnet.AVAILABLE_MODELS}


def load_model_checkpoint(model, ckpt_path: str):
    ckpt_path = glob.glob(hydra.utils.to_absolute_path(ckpt_path))
    model = model.load_state_dict(torch.load(ckpt_path[0]))
    return model

from collections import OrderedDict
from imutils.ml.models.base import BaseModule


def build_timm_backbone(backbone_name=&#39;gluon_seresnext50_32x4d&#39;,
                         pretrained: Union[bool, str]=True,
                         num_classes: int=1000,
                         freeze_backbone: bool=False,
                         feature_layer: int=-2):
    if pretrained == &#34;imagenet&#34;:
        num_classes = 1000

    model = timm.create_model(model_name=backbone_name, num_classes=num_classes, pretrained=pretrained)
    if isinstance(pretrained, str) and pretrained != &#34;imagenet&#34;:
        model = load_model_checkpoint(model, ckpt_path=pretrained)
        
#     body = nn.Sequential(*list(model.children())[:feature_layer])

    body = nn.Sequential(OrderedDict(
        list(
            model.named_children()
        )[:feature_layer]
    ))
    
    if freeze_backbone:
        print(f&#34;Freezing all layers in backbone of model.&#34;)
        BaseModule.freeze(body)
    
    return body



# def build_generic_backbone(backbone_name=&#39;gluon_seresnext50_32x4d&#39;,
#                             pretrained: Union[bool, str]=True,
#                             num_classes: int=1000,
#                             feature_layer: int=-2,
#                             model_repo: str= &#34;timm&#34;,
#                             _target_=None):
#     cfg = DictConf(dict(
#         backbone_name=backbone_name,
#         pretrained=pretrained,
#         num_classes=1000,
#         feature_layer=feature_layer,
#         model_repo=model_repo
#     ))
#     hydra.utils.log.info(f&#34;Instantiating &lt;{cfg.model._target_}&gt;&#34;)
#     model: pl.LightningModule = hydra.utils.instantiate(cfg, _recursive_=False)
#     return model                    




def build_model_backbone(backbone_name=&#39;gluon_seresnext50_32x4d&#39;,
                         pretrained: Union[bool, str]=True,
                         num_classes: int=1000,
                         feature_layer: int=-2,
                         model_repo: str= &#34;timm&#34;,
                        _target_=None,
                        **kwargs):

    if model_repo == &#34;timm&#34;:
        return build_timm_backbone(backbone_name=backbone_name,
                                   pretrained=pretrained,
                                   num_classes=num_classes,
                                   feature_layer=feature_layer)
    else:
        try:
            print(&#34;Trying non-timm based model loading&#34;)
            cfg = DictConf(dict(
                backbone_name=backbone_name,
                pretrained=pretrained,
                num_classes=1000,
                feature_layer=feature_layer,
                model_repo=model_repo
            ))
            # return hydra.utils.log.info(f&#34;Instantiating &lt;{cfg.model._target_}&gt;&#34;)
            # model: pl.LightningModule = hydra.utils.instantiate(cfg.model, cfg=cfg, _recursive_=False)
        except:
            # TBD Add other pretrained model backends
            raise NotImplementedError(f&#34;Invalid model_repo={model_repo}&#34;)

def build_model_head(num_classes: int=1000,
                     pool_size: int=1,
                     pool_type: str=&#39;avg&#39;,
                     head_type: str=&#39;linear&#39;,
                     feature_size: int=512,
                     hidden_size: Optional[int]=512,
                     dropout_p: Optional[float]=0.3,
                    **kwargs):
    &#34;&#34;&#34;
    
    Returns a nn.Sequential model containing 3 children:
        global_pool -&gt; flatten -&gt; classifier
        
    Available pool_types:
        - &#34;avg&#34;
            global_avg_pool
        - &#34;avgdrop&#34;
            global_avg_pool -&gt; dropout
        - &#34;avgmax&#34;
            [global_avg_pool | global_max_pool]
        &#34;max&#34;
            global_max_pool
            
            
    pool_types to be explored:
        - &#34;maxdrop&#34;
            global_max_pool -&gt; dropout
        - &#34;avgmaxdrop&#34;
            [global_avg_pool | global_max_pool] -&gt; dropout

        
    Available head_types:
        - linear
        
        - custom
    
    &#34;&#34;&#34;

    head = OrderedDict()
    global_pool, feature_size = build_global_pool(pool_type=pool_type,
                                                  pool_size=pool_size,
                                                  feature_size=feature_size,
                                                  dropout_p=dropout_p)
    head[&#34;global_pool&#34;] = global_pool
    head[&#34;flatten&#34;] = Flatten()
    
    classifier_input_feature_size = feature_size*(pool_size**2)
    if head_type==&#39;linear&#39;:
        hidden_size = 0
        head[&#34;classifier&#34;] = nn.Linear(classifier_input_feature_size, num_classes)
    elif head_type==&#39;custom&#39;:
        head[&#34;classifier&#34;] = nn.Sequential(nn.Linear(classifier_input_feature_size, hidden_size),
                                nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False),
                                nn.BatchNorm1d(hidden_size),
                                nn.Linear(hidden_size, num_classes))
    head = nn.Sequential(head)
    print(f&#34;Initializing weights of the model head.&#34;)
    BaseModule.initialize_weights(head)
    return head



def build_model(backbone_cfg: Optional[DictConfig]=None,
                head_cfg: Optional[DictConfig]=None,
                backbone_name=&#39;gluon_seresnext50_32x4d&#39;,
                pretrained: Union[bool, str]=True,
                num_classes: int=1000,
                freeze_backbone: bool=False,
                pool_size: int=1,
                pool_type: str=&#39;avg&#39;,
                head_type: str=&#39;linear&#39;,
                hidden_size: Optional[int]=512,
                dropout_p: Optional[float]=0.3):
    &#34;&#34;&#34;
    
    Creates a classifier lightning module from a backbone model + a classication head.
    
    
    Return:
    ```
            model = nn.Sequential(OrderedDict({
                &#34;backbone&#34;:backbone,
                &#34;head&#34;:head
            }))
    ```

    
    &#34;&#34;&#34;
    if backbone_cfg is None:
        backbone = build_model_backbone(backbone_name=backbone_name,
                                        pretrained=pretrained,
                                        num_classes=num_classes,
                                        freeze_backbone=freeze_backbone,
                                        feature_layer=-2)
    else:
        backbone = build_model_backbone(**backbone_cfg)
    
    feature_size = list(backbone.parameters())[-1].shape[0]
    
    if head_cfg is None:
        head = build_model_head(num_classes=num_classes,
                                pool_size=pool_size,
                                pool_type=pool_type,
                                head_type=head_type,
                                feature_size=feature_size,
                                hidden_size=hidden_size,
                                dropout_p=dropout_p)
    else:
        head = build_model_head(feature_size=feature_size,
                                **head_cfg)
    
    model = nn.Sequential(OrderedDict({
        &#34;backbone&#34;:backbone,
        &#34;head&#34;:head
    }))

    
    
    return model


from imutils.ml.utils.model_utils import log_model_summary


        



## Save for later (commented out on 10-17-21)
# def build_model(model_name: str,
#                 pretrained: bool=False,
#                 progress: bool=True,
#                 num_classes: int=1000,
#                 global_pool_type: str=&#39;avg&#39;,
#                 drop_rate: float=0.0,
#                 **kwargs) -&gt; nn.Module:

    
#     if model_name in resnet.AVAILABLE_MODELS:
#         ModelFactory = resnet.build_model
        
# #     elif &#39;senet&#39; in model_name:
#     elif model_name in senet.AVAILABLE_MODELS:
#         ModelFactory = senet.build_model

# #     elif &#39;efficientnet&#39; in model_name:
#     elif model_name in efficientnet.AVAILABLE_MODELS:
#         ModelFactory = efficientnet.build_model
#     else:
#         print(f&#34;model with name {model_name} has not be implemented yet.&#34;)
#         print(&#34;Available Models:&#34;)
#         pp(AVAILABLE_MODELS)
#         return None
    
#     model = ModelFactory(model_name=model_name,
#                          pretrained=pretrained,
#                          progress=progress,
#                          num_classes=num_classes,
#                          global_pool_type=global_pool_type,
#                          drop_rate=drop_rate,
#                          **kwargs)

#     print(f&#34;[BUILDING MODEL] build_model({model_name}, pretrained={pretrained})&#34;)
    
#     return model
    
#     model.name = model_name
#     model.pretrained = pretrained
#     return model










# 3.a Optional: Register a custom backbone
# This is useful to create new backbone and make them accessible from `ImageClassifier`
# @ImageClassifier.backbones(name=&#34;resnet18&#34;)
# def fn_resnet(pretrained: bool = True):
#     model = torchvision.models.resnet18(pretrained)
#     # remove the last two layers &amp; turn it into a Sequential model
#     backbone = nn.Sequential(*list(model.children())[:-2])
#     num_features = model.fc.in_features
#     # backbones need to return the num_features to build the head
#     return backbone, num_features


# def create_classifier(num_features: int, num_classes: int, pool_type=&#39;avg&#39;, bias: bool=True):
#     global_pool = nn.AdaptiveAvgPool2d(1)
#     flatten_layer = nn.Flatten()
#     linear_layer = nn.Linear(num_features, num_classes, bias=bias)
#     return [global_pool, flatten_layer, linear_layer]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imutils.ml.models.backbones.backbone.build_model"><code class="name flex">
<span>def <span class="ident">build_model</span></span>(<span>backbone_cfg: Optional[omegaconf.dictconfig.DictConfig] = None, head_cfg: Optional[omegaconf.dictconfig.DictConfig] = None, backbone_name='gluon_seresnext50_32x4d', pretrained: Union[bool, str] = True, num_classes: int = 1000, freeze_backbone: bool = False, pool_size: int = 1, pool_type: str = 'avg', head_type: str = 'linear', hidden_size: Optional[int] = 512, dropout_p: Optional[float] = 0.3)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a classifier lightning module from a backbone model + a classication head.</p>
<p>Return:</p>
<pre><code>        model = nn.Sequential(OrderedDict({
            &quot;backbone&quot;:backbone,
            &quot;head&quot;:head
        }))
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model(backbone_cfg: Optional[DictConfig]=None,
                head_cfg: Optional[DictConfig]=None,
                backbone_name=&#39;gluon_seresnext50_32x4d&#39;,
                pretrained: Union[bool, str]=True,
                num_classes: int=1000,
                freeze_backbone: bool=False,
                pool_size: int=1,
                pool_type: str=&#39;avg&#39;,
                head_type: str=&#39;linear&#39;,
                hidden_size: Optional[int]=512,
                dropout_p: Optional[float]=0.3):
    &#34;&#34;&#34;
    
    Creates a classifier lightning module from a backbone model + a classication head.
    
    
    Return:
    ```
            model = nn.Sequential(OrderedDict({
                &#34;backbone&#34;:backbone,
                &#34;head&#34;:head
            }))
    ```

    
    &#34;&#34;&#34;
    if backbone_cfg is None:
        backbone = build_model_backbone(backbone_name=backbone_name,
                                        pretrained=pretrained,
                                        num_classes=num_classes,
                                        freeze_backbone=freeze_backbone,
                                        feature_layer=-2)
    else:
        backbone = build_model_backbone(**backbone_cfg)
    
    feature_size = list(backbone.parameters())[-1].shape[0]
    
    if head_cfg is None:
        head = build_model_head(num_classes=num_classes,
                                pool_size=pool_size,
                                pool_type=pool_type,
                                head_type=head_type,
                                feature_size=feature_size,
                                hidden_size=hidden_size,
                                dropout_p=dropout_p)
    else:
        head = build_model_head(feature_size=feature_size,
                                **head_cfg)
    
    model = nn.Sequential(OrderedDict({
        &#34;backbone&#34;:backbone,
        &#34;head&#34;:head
    }))

    
    
    return model</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.backbones.backbone.build_model_backbone"><code class="name flex">
<span>def <span class="ident">build_model_backbone</span></span>(<span>backbone_name='gluon_seresnext50_32x4d', pretrained: Union[bool, str] = True, num_classes: int = 1000, feature_layer: int = -2, model_repo: str = 'timm', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model_backbone(backbone_name=&#39;gluon_seresnext50_32x4d&#39;,
                         pretrained: Union[bool, str]=True,
                         num_classes: int=1000,
                         feature_layer: int=-2,
                         model_repo: str= &#34;timm&#34;,
                        _target_=None,
                        **kwargs):

    if model_repo == &#34;timm&#34;:
        return build_timm_backbone(backbone_name=backbone_name,
                                   pretrained=pretrained,
                                   num_classes=num_classes,
                                   feature_layer=feature_layer)
    else:
        try:
            print(&#34;Trying non-timm based model loading&#34;)
            cfg = DictConf(dict(
                backbone_name=backbone_name,
                pretrained=pretrained,
                num_classes=1000,
                feature_layer=feature_layer,
                model_repo=model_repo
            ))
            # return hydra.utils.log.info(f&#34;Instantiating &lt;{cfg.model._target_}&gt;&#34;)
            # model: pl.LightningModule = hydra.utils.instantiate(cfg.model, cfg=cfg, _recursive_=False)
        except:
            # TBD Add other pretrained model backends
            raise NotImplementedError(f&#34;Invalid model_repo={model_repo}&#34;)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.backbones.backbone.build_model_head"><code class="name flex">
<span>def <span class="ident">build_model_head</span></span>(<span>num_classes: int = 1000, pool_size: int = 1, pool_type: str = 'avg', head_type: str = 'linear', feature_size: int = 512, hidden_size: Optional[int] = 512, dropout_p: Optional[float] = 0.3, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a nn.Sequential model containing 3 children:
global_pool -&gt; flatten -&gt; classifier</p>
<p>Available pool_types:
- "avg"
global_avg_pool
- "avgdrop"
global_avg_pool -&gt; dropout
- "avgmax"
[global_avg_pool | global_max_pool]
"max"
global_max_pool</p>
<p>pool_types to be explored:
- "maxdrop"
global_max_pool -&gt; dropout
- "avgmaxdrop"
[global_avg_pool | global_max_pool] -&gt; dropout</p>
<p>Available head_types:
- linear</p>
<pre><code>- custom
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model_head(num_classes: int=1000,
                     pool_size: int=1,
                     pool_type: str=&#39;avg&#39;,
                     head_type: str=&#39;linear&#39;,
                     feature_size: int=512,
                     hidden_size: Optional[int]=512,
                     dropout_p: Optional[float]=0.3,
                    **kwargs):
    &#34;&#34;&#34;
    
    Returns a nn.Sequential model containing 3 children:
        global_pool -&gt; flatten -&gt; classifier
        
    Available pool_types:
        - &#34;avg&#34;
            global_avg_pool
        - &#34;avgdrop&#34;
            global_avg_pool -&gt; dropout
        - &#34;avgmax&#34;
            [global_avg_pool | global_max_pool]
        &#34;max&#34;
            global_max_pool
            
            
    pool_types to be explored:
        - &#34;maxdrop&#34;
            global_max_pool -&gt; dropout
        - &#34;avgmaxdrop&#34;
            [global_avg_pool | global_max_pool] -&gt; dropout

        
    Available head_types:
        - linear
        
        - custom
    
    &#34;&#34;&#34;

    head = OrderedDict()
    global_pool, feature_size = build_global_pool(pool_type=pool_type,
                                                  pool_size=pool_size,
                                                  feature_size=feature_size,
                                                  dropout_p=dropout_p)
    head[&#34;global_pool&#34;] = global_pool
    head[&#34;flatten&#34;] = Flatten()
    
    classifier_input_feature_size = feature_size*(pool_size**2)
    if head_type==&#39;linear&#39;:
        hidden_size = 0
        head[&#34;classifier&#34;] = nn.Linear(classifier_input_feature_size, num_classes)
    elif head_type==&#39;custom&#39;:
        head[&#34;classifier&#34;] = nn.Sequential(nn.Linear(classifier_input_feature_size, hidden_size),
                                nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False),
                                nn.BatchNorm1d(hidden_size),
                                nn.Linear(hidden_size, num_classes))
    head = nn.Sequential(head)
    print(f&#34;Initializing weights of the model head.&#34;)
    BaseModule.initialize_weights(head)
    return head</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.backbones.backbone.load_model_checkpoint"><code class="name flex">
<span>def <span class="ident">load_model_checkpoint</span></span>(<span>model, ckpt_path: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_checkpoint(model, ckpt_path: str):
    ckpt_path = glob.glob(hydra.utils.to_absolute_path(ckpt_path))
    model = model.load_state_dict(torch.load(ckpt_path[0]))
    return model</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils.ml.models.backbones" href="index.html">imutils.ml.models.backbones</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imutils.ml.models.backbones.backbone.build_model" href="#imutils.ml.models.backbones.backbone.build_model">build_model</a></code></li>
<li><code><a title="imutils.ml.models.backbones.backbone.build_model_backbone" href="#imutils.ml.models.backbones.backbone.build_model_backbone">build_model_backbone</a></code></li>
<li><code><a title="imutils.ml.models.backbones.backbone.build_model_head" href="#imutils.ml.models.backbones.backbone.build_model_head">build_model_head</a></code></li>
<li><code><a title="imutils.ml.models.backbones.backbone.load_model_checkpoint" href="#imutils.ml.models.backbones.backbone.load_model_checkpoint">load_model_checkpoint</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>