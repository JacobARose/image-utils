<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.ml.models.pl.classifier API documentation</title>
<meta name="description" content="imutils/models/pl/classifier.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.ml.models.pl.classifier</code></h1>
</header>
<section id="section-intro">
<p>imutils/models/pl/classifier.py</p>
<p>Created on: Wednesday March 16th, 2022<br>
Created by: Jacob Alexander Rose</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;

imutils/models/pl/classifier.py


Created on: Wednesday March 16th, 2022  
Created by: Jacob Alexander Rose  

&#34;&#34;&#34;




from typing import Any, Dict, List, Sequence, Tuple, Union, Optional
import hydra
import pytorch_lightning as pl
import torchmetrics
import torch
import torch.nn.functional as F
import wandb
from omegaconf import DictConfig
from torch.optim import Optimizer

import numpy as np
import matplotlib.pyplot as plt

from captum.attr import IntegratedGradients
from captum.attr import NoiseTunnel
from captum.attr import visualization as viz

from imutils.ml.utils.common import iterate_elements_in_batches, render_images

# from src.pl_modules import resnets
# from src.pl_modules import losses
from torchvision import models

from pl_bolts.optimizers.lr_scheduler import linear_warmup_decay


class LitClassifier(pl.LightningModule):
        def __init__(self,
                                 cfg: DictConfig, 
                                 name: str,
                                 num_classes: int, 
                                 final_nl,
                                 loss,
                                 pretrain : bool = True,
                                 self_supervised=False,
                                 *args, **kwargs) -&gt; None:

                super().__init__(*args, **kwargs)
                self.cfg = cfg
                self.save_hyperparameters(cfg)
                self.name = name

                self.pretrain = pretrain
                self.num_classes = num_classes
                self.self_supervised = self_supervised


                if self.name == &#34;resnet18&#34;:
                        self.net = models.resnet18(pretrained=False, num_classes=num_classes)
                elif self.name == &#34;simclr_resnet18&#34;:
                        self.net = models.resnet18(pretrained=False, num_classes=num_classes)
                if self.name == &#39;resnet50&#39;:
                        self.net = models.resnet50(pretrained=True)
                        n_features = self.net.fc.in_features
                        fc = torch.nn.Linear(n_features, n_class)
                        self.net.fc = fc
                else:
                        raise NotImplementedError(&#34;Could not find network {}.&#34;.format(self.net))
                self.net.fc.weight.data.normal_(0, 0.005)
                self.net.fc.bias.data.fill_(0.1)

                self.loss = getattr(losses, loss)


                        
                metric = torchmetrics.Accuracy()
                self.train_accuracy = metric.clone().cuda()
                self.val_accuracy = metric.clone().cuda()
                self.test_accuracy = metric.clone().cuda()

        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
                return self.net(x)

        def step(self, x, y) -&gt; Dict[str, torch.Tensor]:
                if self.self_supervised:
                        z1, z2 = self.shared_step(x)
                        loss = self.loss(z1, z2)
                else:
                        logits = self(x)
                        loss = self.loss(self.final_nl(logits, dim=-1), y)
                return {&#34;logits&#34;: logits, &#34;loss&#34;: loss, &#34;y&#34;: y, &#34;x&#34;: x}

        def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:
                x, y = batch
                out = self.step(x, y)
                return out

        def training_step_end(self, out):
                self.train_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;train_acc&#34;: self.train_accuracy,
                                &#34;train_loss&#34;: out[&#34;loss&#34;].mean(),
                        },
                        on_step=True,
                        on_epoch=False
                )
                return out[&#34;loss&#34;].mean()

        def validation_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch
                out = self.step(x, y)
                return out
        
        def validation_step_end(self, out):
                self.val_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;val_acc&#34;: self.val_accuracy,
                                &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                        },
                )
                return {
                        &#34;image&#34;: out[&#34;x&#34;],
                        &#34;y_true&#34;: out[&#34;y&#34;],
                        &#34;logits&#34;: out[&#34;logits&#34;],
                        &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                }

        def test_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch
                out = self.step(x, y)
                return out

        def test_step_end(self, out):
                self.test_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;test_acc&#34;: self.test_accuracy,
                                &#34;test_loss&#34;: out[&#34;loss&#34;].mean(),
                        },
                )
                return {
                        &#34;image&#34;: out[&#34;x&#34;],
                        &#34;y_true&#34;: out[&#34;y&#34;],
                        &#34;logits&#34;: out[&#34;logits&#34;],
                        &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                }


        def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size.val,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;val_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step)   


        def test_epoch_end(self, outputs: List[Any]) -&gt; None:
                
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size.test,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;test_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step)


        @staticmethod
        def render_image_predictions(
                outputs: List[Any],
                batch_size: int,
                n_elements_to_log: int,
                log_name: str=&#34;image predictions&#34;,
                normalize_visualization: bool=True,
                logger=None,
                global_step: int=0
        ) -&gt; None:
                
                # images_feat_viz = []
                # integrated_gradients = IntegratedGradients(self.forward)
                # noise_tunnel = NoiseTunnel(integrated_gradients)
                
                images = []
                for output_element in iterate_elements_in_batches(
                        outputs, batch_size, n_elements_to_log
                ):  
                        rendered_image = render_images(
                                output_element[&#34;image&#34;],
                                autoshow=False,
                                normalize=normalize_visualization)
                        caption = f&#34;y_pred: {output_element[&#39;logits&#39;].argmax()}  [gt: {output_element[&#39;y_true&#39;]}]&#34;  # noqa      
                        # attributions_ig_nt = noise_tunnel.attribute(output_element[&#34;image&#34;].unsqueeze(0), nt_samples=50,
                                                                                                                # nt_type=&#39;smoothgrad_sq&#39;, target=output_element[&#34;y_true&#34;],
                                                                                                                # internal_batch_size=50)
                        images.append(
                                wandb.Image(
                                        rendered_image,
                                        caption=caption,
                                )
                        )
                if logger is not None:
                        logger.experiment.log(
                                {log_name: images}, 
                                step=global_step
                        )


        def configure_optimizers(
                self,
        ) -&gt; Union[Optimizer, Tuple[Sequence[Optimizer], Sequence[Any]]]:
                &#34;&#34;&#34;
                Choose what optimizers and learning-rate schedulers to use in your optimization.
                Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.
                Return:
                        Any of these 6 options.
                        - Single optimizer.
                        - List or Tuple - List of optimizers.
                        - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
                        - Dictionary, with an &#39;optimizer&#39; key, and (optionally) a &#39;lr_scheduler&#39;
                          key whose value is a single LR scheduler or lr_dict.
                        - Tuple of dictionaries as described, with an optional &#39;frequency&#39; key.
                        - None - Fit will run without any optimizer.
                &#34;&#34;&#34;
                if hasattr(self.cfg.optim.optimizer, &#34;exclude_bn_bias&#34;) and \
                                self.cfg.optim.optimizer.exclude_bn_bias:
                        params = self.exclude_from_wt_decay(self.named_parameters(), weight_decay=self.cfg.optim.optimizer.weight_decay)
                else:
                        params = self.parameters()

                opt = hydra.utils.instantiate(
                        self.cfg.optim.optimizer, 
                        params=params,
                        weight_decay=self.cfg.optim.optimizer.weight_decay
                )
                
                if not self.cfg.optim.use_lr_scheduler:
                        return opt

                # Handle schedulers if requested
                if torch.optim.lr_scheduler.warmup_steps:
                        # Right now this is specific to SimCLR
                        lr_scheduler = {
                                &#34;scheduler&#34;: torch.optim.lr_scheduler.LambdaLR(
                                        opt,
                                        linear_warmup_decay(
                                                self.cfg.optim.lr_scheduler.warmup_steps,
                                                self.cfg.optim.lr_scheduler.total_steps,
                                                cosine=True),
                                ),
                                &#34;interval&#34;: &#34;step&#34;,
                                &#34;frequency&#34;: 1,
                        }
                else:
                        lr_scheduler = self.cfg.optim.lr_scheduler
                scheduler = hydra.utils.instantiate(lr_scheduler, optimizer=opt)
                return [opt], [scheduler]

        @staticmethod
        def exclude_from_wt_decay(named_params: List[Tuple[str, torch.Tensor]],
                                                          weight_decay: float,
                                                          skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)
                                                         ) -&gt; List[Dict[str, Any]]:
                &#34;&#34;&#34;
                Sort named_params into 2 groups: included &amp; excluded from weight decay.
                Includes any params with a name that doesn&#39;t match any pattern in `skip_list`.
                
                Arguments:
                        named_params: List[Tuple[str, torch.Tensor]]
                        weight_decay: float,
                        skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)):          
                &#34;&#34;&#34;
                params = []
                excluded_params = []

                for name, param in named_params:
                        if not param.requires_grad:
                                continue
                        elif any(layer_name in name for layer_name in skip_list):
                                excluded_params.append(param)
                        else:
                                params.append(param)

                return [
                        {&#34;params&#34;: params, &#34;weight_decay&#34;: weight_decay},
                        {
                                &#34;params&#34;: excluded_params,
                                &#34;weight_decay&#34;: 0.0,
                        },
                ]

        def init_metrics(self,
                                         stage: str=&#39;train&#39;,
                                         tag: Optional[str]=None):
                tag = tag or &#34;&#34;
                if not hasattr(self, &#34;all_metrics&#34;):
                        self.all_metrics = {}
                
                if not hasattr(self,&#34;num_classes&#34;) and hasattr(self.hparams, &#34;num_classes&#34;):
                        self.num_classes = self.hparams.num_classes
                
                print(f&#34;self.num_classes={self.num_classes}&#34;)
                if stage in [&#39;train&#39;, &#39;all&#39;]:
                        prefix=f&#39;{tag}_train&#39;.strip(&#34;_&#34;)
                        self.metrics_train = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_train_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;train&#39;)
                        self.all_metrics[&#39;train&#39;] = {&#34;scalar&#34;:self.metrics_train,
                                                                                 &#34;per_class&#34;:self.metrics_train_per_class}
                        
                if stage in [&#39;val&#39;, &#39;all&#39;]:
                        prefix=f&#39;{tag}_val&#39;.strip(&#34;_&#34;)
                        self.metrics_val = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_val_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;val&#39;)
                        self.all_metrics[&#39;val&#39;] = {&#34;scalar&#34;:self.metrics_val,
                                                                           &#34;per_class&#34;:self.metrics_val_per_class}
                        
                if stage in [&#39;test&#39;, &#39;all&#39;]:
                        if isinstance(tag, str):
                                prefix=tag
                        else:
                                prefix = &#34;test&#34;
#                        prefix=f&#39;{tag}_test&#39;.strip(&#34;_&#34;)
                        self.metrics_test = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_test_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=prefix)
                        self.all_metrics[&#39;test&#39;] = {&#34;scalar&#34;:self.metrics_test,
                                                                                &#34;per_class&#34;:self.metrics_test_per_class}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier"><code class="flex name class">
<span>class <span class="ident">LitClassifier</span></span>
<span>(</span><span>cfg:Â omegaconf.dictconfig.DictConfig, name:Â str, num_classes:Â int, final_nl, loss, pretrain:Â boolÂ =Â True, self_supervised=False, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Hooks to be used in LightningModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LitClassifier(pl.LightningModule):
        def __init__(self,
                                 cfg: DictConfig, 
                                 name: str,
                                 num_classes: int, 
                                 final_nl,
                                 loss,
                                 pretrain : bool = True,
                                 self_supervised=False,
                                 *args, **kwargs) -&gt; None:

                super().__init__(*args, **kwargs)
                self.cfg = cfg
                self.save_hyperparameters(cfg)
                self.name = name

                self.pretrain = pretrain
                self.num_classes = num_classes
                self.self_supervised = self_supervised


                if self.name == &#34;resnet18&#34;:
                        self.net = models.resnet18(pretrained=False, num_classes=num_classes)
                elif self.name == &#34;simclr_resnet18&#34;:
                        self.net = models.resnet18(pretrained=False, num_classes=num_classes)
                if self.name == &#39;resnet50&#39;:
                        self.net = models.resnet50(pretrained=True)
                        n_features = self.net.fc.in_features
                        fc = torch.nn.Linear(n_features, n_class)
                        self.net.fc = fc
                else:
                        raise NotImplementedError(&#34;Could not find network {}.&#34;.format(self.net))
                self.net.fc.weight.data.normal_(0, 0.005)
                self.net.fc.bias.data.fill_(0.1)

                self.loss = getattr(losses, loss)


                        
                metric = torchmetrics.Accuracy()
                self.train_accuracy = metric.clone().cuda()
                self.val_accuracy = metric.clone().cuda()
                self.test_accuracy = metric.clone().cuda()

        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
                return self.net(x)

        def step(self, x, y) -&gt; Dict[str, torch.Tensor]:
                if self.self_supervised:
                        z1, z2 = self.shared_step(x)
                        loss = self.loss(z1, z2)
                else:
                        logits = self(x)
                        loss = self.loss(self.final_nl(logits, dim=-1), y)
                return {&#34;logits&#34;: logits, &#34;loss&#34;: loss, &#34;y&#34;: y, &#34;x&#34;: x}

        def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:
                x, y = batch
                out = self.step(x, y)
                return out

        def training_step_end(self, out):
                self.train_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;train_acc&#34;: self.train_accuracy,
                                &#34;train_loss&#34;: out[&#34;loss&#34;].mean(),
                        },
                        on_step=True,
                        on_epoch=False
                )
                return out[&#34;loss&#34;].mean()

        def validation_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch
                out = self.step(x, y)
                return out
        
        def validation_step_end(self, out):
                self.val_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;val_acc&#34;: self.val_accuracy,
                                &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                        },
                )
                return {
                        &#34;image&#34;: out[&#34;x&#34;],
                        &#34;y_true&#34;: out[&#34;y&#34;],
                        &#34;logits&#34;: out[&#34;logits&#34;],
                        &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                }

        def test_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch
                out = self.step(x, y)
                return out

        def test_step_end(self, out):
                self.test_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;test_acc&#34;: self.test_accuracy,
                                &#34;test_loss&#34;: out[&#34;loss&#34;].mean(),
                        },
                )
                return {
                        &#34;image&#34;: out[&#34;x&#34;],
                        &#34;y_true&#34;: out[&#34;y&#34;],
                        &#34;logits&#34;: out[&#34;logits&#34;],
                        &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                }


        def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size.val,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;val_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step)   


        def test_epoch_end(self, outputs: List[Any]) -&gt; None:
                
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size.test,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;test_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step)


        @staticmethod
        def render_image_predictions(
                outputs: List[Any],
                batch_size: int,
                n_elements_to_log: int,
                log_name: str=&#34;image predictions&#34;,
                normalize_visualization: bool=True,
                logger=None,
                global_step: int=0
        ) -&gt; None:
                
                # images_feat_viz = []
                # integrated_gradients = IntegratedGradients(self.forward)
                # noise_tunnel = NoiseTunnel(integrated_gradients)
                
                images = []
                for output_element in iterate_elements_in_batches(
                        outputs, batch_size, n_elements_to_log
                ):  
                        rendered_image = render_images(
                                output_element[&#34;image&#34;],
                                autoshow=False,
                                normalize=normalize_visualization)
                        caption = f&#34;y_pred: {output_element[&#39;logits&#39;].argmax()}  [gt: {output_element[&#39;y_true&#39;]}]&#34;  # noqa      
                        # attributions_ig_nt = noise_tunnel.attribute(output_element[&#34;image&#34;].unsqueeze(0), nt_samples=50,
                                                                                                                # nt_type=&#39;smoothgrad_sq&#39;, target=output_element[&#34;y_true&#34;],
                                                                                                                # internal_batch_size=50)
                        images.append(
                                wandb.Image(
                                        rendered_image,
                                        caption=caption,
                                )
                        )
                if logger is not None:
                        logger.experiment.log(
                                {log_name: images}, 
                                step=global_step
                        )


        def configure_optimizers(
                self,
        ) -&gt; Union[Optimizer, Tuple[Sequence[Optimizer], Sequence[Any]]]:
                &#34;&#34;&#34;
                Choose what optimizers and learning-rate schedulers to use in your optimization.
                Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.
                Return:
                        Any of these 6 options.
                        - Single optimizer.
                        - List or Tuple - List of optimizers.
                        - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
                        - Dictionary, with an &#39;optimizer&#39; key, and (optionally) a &#39;lr_scheduler&#39;
                          key whose value is a single LR scheduler or lr_dict.
                        - Tuple of dictionaries as described, with an optional &#39;frequency&#39; key.
                        - None - Fit will run without any optimizer.
                &#34;&#34;&#34;
                if hasattr(self.cfg.optim.optimizer, &#34;exclude_bn_bias&#34;) and \
                                self.cfg.optim.optimizer.exclude_bn_bias:
                        params = self.exclude_from_wt_decay(self.named_parameters(), weight_decay=self.cfg.optim.optimizer.weight_decay)
                else:
                        params = self.parameters()

                opt = hydra.utils.instantiate(
                        self.cfg.optim.optimizer, 
                        params=params,
                        weight_decay=self.cfg.optim.optimizer.weight_decay
                )
                
                if not self.cfg.optim.use_lr_scheduler:
                        return opt

                # Handle schedulers if requested
                if torch.optim.lr_scheduler.warmup_steps:
                        # Right now this is specific to SimCLR
                        lr_scheduler = {
                                &#34;scheduler&#34;: torch.optim.lr_scheduler.LambdaLR(
                                        opt,
                                        linear_warmup_decay(
                                                self.cfg.optim.lr_scheduler.warmup_steps,
                                                self.cfg.optim.lr_scheduler.total_steps,
                                                cosine=True),
                                ),
                                &#34;interval&#34;: &#34;step&#34;,
                                &#34;frequency&#34;: 1,
                        }
                else:
                        lr_scheduler = self.cfg.optim.lr_scheduler
                scheduler = hydra.utils.instantiate(lr_scheduler, optimizer=opt)
                return [opt], [scheduler]

        @staticmethod
        def exclude_from_wt_decay(named_params: List[Tuple[str, torch.Tensor]],
                                                          weight_decay: float,
                                                          skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)
                                                         ) -&gt; List[Dict[str, Any]]:
                &#34;&#34;&#34;
                Sort named_params into 2 groups: included &amp; excluded from weight decay.
                Includes any params with a name that doesn&#39;t match any pattern in `skip_list`.
                
                Arguments:
                        named_params: List[Tuple[str, torch.Tensor]]
                        weight_decay: float,
                        skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)):          
                &#34;&#34;&#34;
                params = []
                excluded_params = []

                for name, param in named_params:
                        if not param.requires_grad:
                                continue
                        elif any(layer_name in name for layer_name in skip_list):
                                excluded_params.append(param)
                        else:
                                params.append(param)

                return [
                        {&#34;params&#34;: params, &#34;weight_decay&#34;: weight_decay},
                        {
                                &#34;params&#34;: excluded_params,
                                &#34;weight_decay&#34;: 0.0,
                        },
                ]

        def init_metrics(self,
                                         stage: str=&#39;train&#39;,
                                         tag: Optional[str]=None):
                tag = tag or &#34;&#34;
                if not hasattr(self, &#34;all_metrics&#34;):
                        self.all_metrics = {}
                
                if not hasattr(self,&#34;num_classes&#34;) and hasattr(self.hparams, &#34;num_classes&#34;):
                        self.num_classes = self.hparams.num_classes
                
                print(f&#34;self.num_classes={self.num_classes}&#34;)
                if stage in [&#39;train&#39;, &#39;all&#39;]:
                        prefix=f&#39;{tag}_train&#39;.strip(&#34;_&#34;)
                        self.metrics_train = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_train_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;train&#39;)
                        self.all_metrics[&#39;train&#39;] = {&#34;scalar&#34;:self.metrics_train,
                                                                                 &#34;per_class&#34;:self.metrics_train_per_class}
                        
                if stage in [&#39;val&#39;, &#39;all&#39;]:
                        prefix=f&#39;{tag}_val&#39;.strip(&#34;_&#34;)
                        self.metrics_val = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_val_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;val&#39;)
                        self.all_metrics[&#39;val&#39;] = {&#34;scalar&#34;:self.metrics_val,
                                                                           &#34;per_class&#34;:self.metrics_val_per_class}
                        
                if stage in [&#39;test&#39;, &#39;all&#39;]:
                        if isinstance(tag, str):
                                prefix=tag
                        else:
                                prefix = &#34;test&#34;
#                        prefix=f&#39;{tag}_test&#39;.strip(&#34;_&#34;)
                        self.metrics_test = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_test_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=prefix)
                        self.all_metrics[&#39;test&#39;] = {&#34;scalar&#34;:self.metrics_test,
                                                                                &#34;per_class&#34;:self.metrics_test_per_class}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.exclude_from_wt_decay"><code class="name flex">
<span>def <span class="ident">exclude_from_wt_decay</span></span>(<span>named_params:Â List[Tuple[str,Â torch.Tensor]], weight_decay:Â float, skip_list:Â Tuple[str]Â =Â ('bias', 'bn')) â€‘>Â List[Dict[str,Â Any]]</span>
</code></dt>
<dd>
<div class="desc"><p>Sort named_params into 2 groups: included &amp; excluded from weight decay.
Includes any params with a name that doesn't match any pattern in <code>skip_list</code>.</p>
<h2 id="arguments">Arguments</h2>
<p>named_params: List[Tuple[str, torch.Tensor]]
weight_decay: float,
skip_list: Tuple[str]=("bias", "bn")):</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def exclude_from_wt_decay(named_params: List[Tuple[str, torch.Tensor]],
                                                  weight_decay: float,
                                                  skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)
                                                 ) -&gt; List[Dict[str, Any]]:
        &#34;&#34;&#34;
        Sort named_params into 2 groups: included &amp; excluded from weight decay.
        Includes any params with a name that doesn&#39;t match any pattern in `skip_list`.
        
        Arguments:
                named_params: List[Tuple[str, torch.Tensor]]
                weight_decay: float,
                skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)):          
        &#34;&#34;&#34;
        params = []
        excluded_params = []

        for name, param in named_params:
                if not param.requires_grad:
                        continue
                elif any(layer_name in name for layer_name in skip_list):
                        excluded_params.append(param)
                else:
                        params.append(param)

        return [
                {&#34;params&#34;: params, &#34;weight_decay&#34;: weight_decay},
                {
                        &#34;params&#34;: excluded_params,
                        &#34;weight_decay&#34;: 0.0,
                },
        ]</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.render_image_predictions"><code class="name flex">
<span>def <span class="ident">render_image_predictions</span></span>(<span>outputs:Â List[Any], batch_size:Â int, n_elements_to_log:Â int, log_name:Â strÂ =Â 'image predictions', normalize_visualization:Â boolÂ =Â True, logger=None, global_step:Â intÂ =Â 0) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def render_image_predictions(
        outputs: List[Any],
        batch_size: int,
        n_elements_to_log: int,
        log_name: str=&#34;image predictions&#34;,
        normalize_visualization: bool=True,
        logger=None,
        global_step: int=0
) -&gt; None:
        
        # images_feat_viz = []
        # integrated_gradients = IntegratedGradients(self.forward)
        # noise_tunnel = NoiseTunnel(integrated_gradients)
        
        images = []
        for output_element in iterate_elements_in_batches(
                outputs, batch_size, n_elements_to_log
        ):  
                rendered_image = render_images(
                        output_element[&#34;image&#34;],
                        autoshow=False,
                        normalize=normalize_visualization)
                caption = f&#34;y_pred: {output_element[&#39;logits&#39;].argmax()}  [gt: {output_element[&#39;y_true&#39;]}]&#34;  # noqa      
                # attributions_ig_nt = noise_tunnel.attribute(output_element[&#34;image&#34;].unsqueeze(0), nt_samples=50,
                                                                                                        # nt_type=&#39;smoothgrad_sq&#39;, target=output_element[&#34;y_true&#34;],
                                                                                                        # internal_batch_size=50)
                images.append(
                        wandb.Image(
                                rendered_image,
                                caption=caption,
                        )
                )
        if logger is not None:
                logger.experiment.log(
                        {log_name: images}, 
                        step=global_step
                )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self) â€‘>Â Union[torch.optim.optimizer.Optimizer,Â Tuple[Sequence[torch.optim.optimizer.Optimizer],Â Sequence[Any]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you'd need one. But in the case of GANs or similar you might have multiple.</p>
<h2 id="return">Return</h2>
<p>Any of these 6 options.
- Single optimizer.
- List or Tuple - List of optimizers.
- Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
- Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler'
key whose value is a single LR scheduler or lr_dict.
- Tuple of dictionaries as described, with an optional 'frequency' key.
- None - Fit will run without any optimizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(
        self,
) -&gt; Union[Optimizer, Tuple[Sequence[Optimizer], Sequence[Any]]]:
        &#34;&#34;&#34;
        Choose what optimizers and learning-rate schedulers to use in your optimization.
        Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.
        Return:
                Any of these 6 options.
                - Single optimizer.
                - List or Tuple - List of optimizers.
                - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
                - Dictionary, with an &#39;optimizer&#39; key, and (optionally) a &#39;lr_scheduler&#39;
                  key whose value is a single LR scheduler or lr_dict.
                - Tuple of dictionaries as described, with an optional &#39;frequency&#39; key.
                - None - Fit will run without any optimizer.
        &#34;&#34;&#34;
        if hasattr(self.cfg.optim.optimizer, &#34;exclude_bn_bias&#34;) and \
                        self.cfg.optim.optimizer.exclude_bn_bias:
                params = self.exclude_from_wt_decay(self.named_parameters(), weight_decay=self.cfg.optim.optimizer.weight_decay)
        else:
                params = self.parameters()

        opt = hydra.utils.instantiate(
                self.cfg.optim.optimizer, 
                params=params,
                weight_decay=self.cfg.optim.optimizer.weight_decay
        )
        
        if not self.cfg.optim.use_lr_scheduler:
                return opt

        # Handle schedulers if requested
        if torch.optim.lr_scheduler.warmup_steps:
                # Right now this is specific to SimCLR
                lr_scheduler = {
                        &#34;scheduler&#34;: torch.optim.lr_scheduler.LambdaLR(
                                opt,
                                linear_warmup_decay(
                                        self.cfg.optim.lr_scheduler.warmup_steps,
                                        self.cfg.optim.lr_scheduler.total_steps,
                                        cosine=True),
                        ),
                        &#34;interval&#34;: &#34;step&#34;,
                        &#34;frequency&#34;: 1,
                }
        else:
                lr_scheduler = self.cfg.optim.lr_scheduler
        scheduler = hydra.utils.instantiate(lr_scheduler, optimizer=opt)
        return [opt], [scheduler]</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x:Â torch.Tensor) â€‘>Â torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Same as :meth:<code>torch.nn.Module.forward()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Whatever you decide to pass into the forward method.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments are also possible.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Your model's output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        return self.net(x)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.init_metrics"><code class="name flex">
<span>def <span class="ident">init_metrics</span></span>(<span>self, stage:Â strÂ =Â 'train', tag:Â Optional[str]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">        def init_metrics(self,
                                         stage: str=&#39;train&#39;,
                                         tag: Optional[str]=None):
                tag = tag or &#34;&#34;
                if not hasattr(self, &#34;all_metrics&#34;):
                        self.all_metrics = {}
                
                if not hasattr(self,&#34;num_classes&#34;) and hasattr(self.hparams, &#34;num_classes&#34;):
                        self.num_classes = self.hparams.num_classes
                
                print(f&#34;self.num_classes={self.num_classes}&#34;)
                if stage in [&#39;train&#39;, &#39;all&#39;]:
                        prefix=f&#39;{tag}_train&#39;.strip(&#34;_&#34;)
                        self.metrics_train = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_train_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;train&#39;)
                        self.all_metrics[&#39;train&#39;] = {&#34;scalar&#34;:self.metrics_train,
                                                                                 &#34;per_class&#34;:self.metrics_train_per_class}
                        
                if stage in [&#39;val&#39;, &#39;all&#39;]:
                        prefix=f&#39;{tag}_val&#39;.strip(&#34;_&#34;)
                        self.metrics_val = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_val_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;val&#39;)
                        self.all_metrics[&#39;val&#39;] = {&#34;scalar&#34;:self.metrics_val,
                                                                           &#34;per_class&#34;:self.metrics_val_per_class}
                        
                if stage in [&#39;test&#39;, &#39;all&#39;]:
                        if isinstance(tag, str):
                                prefix=tag
                        else:
                                prefix = &#34;test&#34;
#                        prefix=f&#39;{tag}_test&#39;.strip(&#34;_&#34;)
                        self.metrics_test = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
                        self.metrics_test_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=prefix)
                        self.all_metrics[&#39;test&#39;] = {&#34;scalar&#34;:self.metrics_test,
                                                                                &#34;per_class&#34;:self.metrics_test_per_class}</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, x, y) â€‘>Â Dict[str,Â torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, x, y) -&gt; Dict[str, torch.Tensor]:
        if self.self_supervised:
                z1, z2 = self.shared_step(x)
                loss = self.loss(z1, z2)
        else:
                logits = self(x)
                loss = self.loss(self.final_nl(logits, dim=-1), y)
        return {&#34;logits&#34;: logits, &#34;loss&#34;: loss, &#34;y&#34;: y, &#34;x&#34;: x}</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.test_epoch_end"><code class="name flex">
<span>def <span class="ident">test_epoch_end</span></span>(<span>self, outputs:Â List[Any]) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of a test epoch with the output of all test steps.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
test_outs = []
for test_batch in test_data:
    out = test_step(test_batch)
    test_outs.append(out)
test_epoch_end(test_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>outputs</code></strong></dt>
<dd>List of outputs you defined in :meth:<code>test_step_end</code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</dd>
</dl>
<h2 id="return">Return</h2>
<p>None</p>
<h2 id="note">Note</h2>
<p>If you didn't define a :meth:<code>test_step</code>, this won't be called.</p>
<h2 id="examples">Examples</h2>
<p>With a single dataloader:</p>
<p>.. code-block:: python</p>
<pre><code>def test_epoch_end(self, outputs):
    # do something with the outputs of all test batches
    all_test_preds = test_step_outputs.predictions

    some_result = calc_all_results(all_test_preds)
    self.log(some_result)
</code></pre>
<p>With multiple dataloaders, <code>outputs</code> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<p>.. code-block:: python</p>
<pre><code>def test_epoch_end(self, outputs):
    final_value = 0
    for dataloader_outputs in outputs:
        for test_step_out in dataloader_outputs:
            # do something
            final_value += test_step_out

    self.log("final_metric", final_value)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_epoch_end(self, outputs: List[Any]) -&gt; None:
        
        self.render_image_predictions(
                outputs=outputs,
                batch_size=self.cfg.data.datamodule.batch_size.test,
                n_elements_to_log=self.cfg.logging.n_elements_to_log,
                log_name=&#34;test_image_predictions&#34;,
                normalize_visualization=self.cfg.logging.normalize_visualization,
                logger=self.logger,
                global_step=self.global_step)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.test_step"><code class="name flex">
<span>def <span class="ident">test_step</span></span>(<span>self, batch:Â Any, batch_idx:Â int) â€‘>Â Dict[str,Â torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the test set.
In this step you'd normally generate examples or calculate anything of interest
such as accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
test_outs = []
for test_batch in test_data:
    out = test_step(test_batch)
    test_outs.append(out)
test_epoch_end(test_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):</dt>
<dt>The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</dd>
</dl>
<h2 id="return">Return</h2>
<p>Any of.</p>
<ul>
<li>Any object or value</li>
<li><code>None</code> - Testing will skip to the next batch</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one test dataloader:
def test_step(self, batch, batch_idx):
    ...


# if you have multiple test dataloaders:
def test_step(self, batch, batch_idx, dataloader_idx):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single test dataset
def test_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'test_loss': loss, 'test_acc': test_acc})
</code></pre>
<p>If you pass in multiple test dataloaders, :meth:<code>test_step</code> will have an additional argument.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple test dataloaders
def test_step(self, batch, batch_idx, dataloader_idx):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to test you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>test_step</code> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
        x, y = batch
        out = self.step(x, y)
        return out</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.test_step_end"><code class="name flex">
<span>def <span class="ident">test_step_end</span></span>(<span>self, out)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this when testing with dp or ddp2 because :meth:<code>test_step</code> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<h2 id="note">Note</h2>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<pre><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches]
test_step_end(batch_parts_outputs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_parts_outputs</code></strong></dt>
<dd>What you return in :meth:<code>test_step</code> for each batch part.</dd>
</dl>
<h2 id="return">Return</h2>
<p>None or anything</p>
<p>.. code-block:: python</p>
<pre><code># WITHOUT test_step_end
# if used in DP or DDP2, this batch is 1/num_gpus large
def test_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)
    loss = self.softmax(out)
    self.log("test_loss", loss)


# --------------
# with test_step_end to do softmax over the full batch
def test_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    return out


def test_step_end(self, output_results):
    # this out is now the full size of the batch
    all_test_step_outs = output_results.out
    loss = nce_loss(all_test_step_outs)
    self.log("test_loss", loss)
</code></pre>
<p>See Also:
See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_step_end(self, out):
        self.test_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
        self.log_dict(
                {
                        &#34;test_acc&#34;: self.test_accuracy,
                        &#34;test_loss&#34;: out[&#34;loss&#34;].mean(),
                },
        )
        return {
                &#34;image&#34;: out[&#34;x&#34;],
                &#34;y_true&#34;: out[&#34;y&#34;],
                &#34;logits&#34;: out[&#34;logits&#34;],
                &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
        }</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, batch:Â Any, batch_idx:Â int) â€‘>Â torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<h2 id="args">Args</h2>
<p>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):
The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.
batch_idx (<code>int</code>): Integer displaying index of this batch
optimizer_idx (<code>int</code>): When using multiple optimizers, this argument will also be present.
hiddens (<code>Any</code>): Passed in if
:paramref:<code>~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps</code> &gt; 0.</p>
<h2 id="return">Return</h2>
<p>Any of.</p>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code></li>
<li><code>None</code> - Training will skip to the next batch. This is only for automatic optimization.
This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>If you define multiple optimizers, this step will be called with an additional
<code>optimizer_idx</code> parameter.</p>
<p>.. code-block:: python</p>
<pre><code># Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx, optimizer_idx):
    if optimizer_idx == 0:
        # do training_step with encoder
        ...
    if optimizer_idx == 1:
        # do training_step with decoder
        ...
</code></pre>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<p>.. code-block:: python</p>
<pre><code># Truncated back-propagation through time
def training_step(self, batch, batch_idx, hiddens):
    # hiddens are the hidden states from the previous truncated backprop step
    out, hiddens = self.lstm(data, hiddens)
    loss = ...
    return {"loss": loss, "hiddens": hiddens}
</code></pre>
<h2 id="note">Note</h2>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:
        x, y = batch
        out = self.step(x, y)
        return out</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.training_step_end"><code class="name flex">
<span>def <span class="ident">training_step_end</span></span>(<span>self, out)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this when training with dp or ddp2 because :meth:<code>training_step</code> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<h2 id="note">Note</h2>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don't have to change your code</p>
<p>.. code-block:: python</p>
<pre><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]
training_step_end(batch_parts_outputs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_parts_outputs</code></strong></dt>
<dd>What you return in <code>training_step</code> for each batch part.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Anything</p>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<p>.. code-block:: python</p>
<pre><code>def training_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)

    # softmax uses only a portion of the batch in the denominator
    loss = self.softmax(out)
    loss = nce_loss(loss)
    return loss
</code></pre>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<p>.. code-block:: python</p>
<pre><code>def training_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    return {"pred": out}


def training_step_end(self, training_step_outputs):
    gpu_0_pred = training_step_outputs[0]["pred"]
    gpu_1_pred = training_step_outputs[1]["pred"]
    gpu_n_pred = training_step_outputs[n]["pred"]

    # this softmax now uses the full batch
    loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred])
    return loss
</code></pre>
<p>See Also:
See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step_end(self, out):
        self.train_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
        self.log_dict(
                {
                        &#34;train_acc&#34;: self.train_accuracy,
                        &#34;train_loss&#34;: out[&#34;loss&#34;].mean(),
                },
                on_step=True,
                on_epoch=False
        )
        return out[&#34;loss&#34;].mean()</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.validation_epoch_end"><code class="name flex">
<span>def <span class="ident">validation_epoch_end</span></span>(<span>self, outputs:Â List[Any]) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
        &#34;&#34;&#34;
        
        &#34;&#34;&#34;
        self.render_image_predictions(
                outputs=outputs,
                batch_size=self.cfg.data.datamodule.batch_size.val,
                n_elements_to_log=self.cfg.logging.n_elements_to_log,
                log_name=&#34;val_image_predictions&#34;,
                normalize_visualization=self.cfg.logging.normalize_visualization,
                logger=self.logger,
                global_step=self.global_step)   </code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, batch:Â Any, batch_idx:Â int) â€‘>Â Dict[str,Â torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set.
In this step you'd might generate examples or calculate anything of interest like accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    val_outs.append(out)
validation_epoch_end(val_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):</dt>
<dt>The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of this batch</dd>
<dt><strong><code>dataloader_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the dataloader that produced this batch
(only if multiple val dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>Any object or value</li>
<li><code>None</code> - Validation will skip to the next batch</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># pseudocode of order
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    if defined("validation_step_end"):
        out = validation_step_end(out)
    val_outs.append(out)
val_outs = validation_epoch_end(val_outs)
</code></pre>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
        x, y = batch
        out = self.step(x, y)
        return out</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.validation_step_end"><code class="name flex">
<span>def <span class="ident">validation_step_end</span></span>(<span>self, out)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this when validating with dp or ddp2 because :meth:<code>validation_step</code> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<h2 id="note">Note</h2>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<pre><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches]
validation_step_end(batch_parts_outputs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_parts_outputs</code></strong></dt>
<dd>What you return in :meth:<code>validation_step</code>
for each batch part.</dd>
</dl>
<h2 id="return">Return</h2>
<p>None or anything</p>
<p>.. code-block:: python</p>
<pre><code># WITHOUT validation_step_end
# if used in DP or DDP2, this batch is 1/num_gpus large
def validation_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    loss = self.softmax(out)
    loss = nce_loss(loss)
    self.log("val_loss", loss)


# --------------
# with validation_step_end to do softmax over the full batch
def validation_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)
    return out


def validation_step_end(self, val_step_outputs):
    for out in val_step_outputs:
        ...
</code></pre>
<p>See Also:
See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step_end(self, out):
        self.val_accuracy(self.final_nl(out[&#34;logits&#34;], dim=-1), out[&#34;y&#34;])
        self.log_dict(
                {
                        &#34;val_acc&#34;: self.val_accuracy,
                        &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                },
        )
        return {
                &#34;image&#34;: out[&#34;x&#34;],
                &#34;y_true&#34;: out[&#34;y&#34;],
                &#34;logits&#34;: out[&#34;logits&#34;],
                &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
        }</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils.ml.models.pl" href="index.html">imutils.ml.models.pl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imutils.ml.models.pl.classifier.LitClassifier" href="#imutils.ml.models.pl.classifier.LitClassifier">LitClassifier</a></code></h4>
<ul class="">
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.configure_optimizers" href="#imutils.ml.models.pl.classifier.LitClassifier.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.dump_patches" href="#imutils.ml.models.pl.classifier.LitClassifier.dump_patches">dump_patches</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.exclude_from_wt_decay" href="#imutils.ml.models.pl.classifier.LitClassifier.exclude_from_wt_decay">exclude_from_wt_decay</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.forward" href="#imutils.ml.models.pl.classifier.LitClassifier.forward">forward</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.init_metrics" href="#imutils.ml.models.pl.classifier.LitClassifier.init_metrics">init_metrics</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.render_image_predictions" href="#imutils.ml.models.pl.classifier.LitClassifier.render_image_predictions">render_image_predictions</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.step" href="#imutils.ml.models.pl.classifier.LitClassifier.step">step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.test_epoch_end" href="#imutils.ml.models.pl.classifier.LitClassifier.test_epoch_end">test_epoch_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.test_step" href="#imutils.ml.models.pl.classifier.LitClassifier.test_step">test_step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.test_step_end" href="#imutils.ml.models.pl.classifier.LitClassifier.test_step_end">test_step_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.training" href="#imutils.ml.models.pl.classifier.LitClassifier.training">training</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.training_step" href="#imutils.ml.models.pl.classifier.LitClassifier.training_step">training_step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.training_step_end" href="#imutils.ml.models.pl.classifier.LitClassifier.training_step_end">training_step_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.validation_epoch_end" href="#imutils.ml.models.pl.classifier.LitClassifier.validation_epoch_end">validation_epoch_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.validation_step" href="#imutils.ml.models.pl.classifier.LitClassifier.validation_step">validation_step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.validation_step_end" href="#imutils.ml.models.pl.classifier.LitClassifier.validation_step_end">validation_step_end</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>