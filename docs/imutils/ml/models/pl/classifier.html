<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.ml.models.pl.classifier API documentation</title>
<meta name="description" content="imutils/models/pl/classifier.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.ml.models.pl.classifier</code></h1>
</header>
<section id="section-intro">
<p>imutils/models/pl/classifier.py</p>
<p>Created on: Wednesday March 16th, 2022<br>
Created by: Jacob Alexander Rose</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;

imutils/models/pl/classifier.py


Created on: Wednesday March 16th, 2022  
Created by: Jacob Alexander Rose  

&#34;&#34;&#34;


from icecream import ic
from rich import print as pp
from typing import Any, Dict, List, Sequence, Tuple, Union, Optional, Callable
import hydra
import pytorch_lightning as pl
import torchmetrics
import torch
import torch.nn.functional as F
import wandb
from omegaconf import DictConfig, OmegaConf
from torch.optim import Optimizer

import numpy as np
import matplotlib.pyplot as plt
        
from captum.attr import IntegratedGradients
from captum.attr import NoiseTunnel
from captum.attr import visualization as viz

from imutils.ml.utils.common import iterate_elements_in_batches, render_images
from imutils.ml.utils.metric_utils import get_scalar_metrics
from torchvision import models
# from pl_bolts.optimizers.lr_scheduler import linear_warmup_decay
from imutils.ml.models.base import BaseModule, BaseLightningModule
from imutils.ml.models.backbones.backbone import build_model
from imutils.ml.utils.experiment_utils import resolve_config
# from imutils.ml.utils.model_utils import log_model_summary

from imutils.ml.utils.toolbox.nn.loss import LabelSmoothingLoss

# from imutils.ml import losses
# nn = losses.nn

__all__ = [&#34;LitClassifier&#34;]



class LitClassifier(BaseLightningModule): #pl.LightningModule):
        def __init__(self,
                                 cfg: DictConfig=None,
                                 model_cfg: DictConfig=None, 
                                 name: str=None,
                                 num_classes: int=None, 
                                 loss_func: Union[Callable, str]=None,
                                 # pretrain : bool = True,
                                 # self_supervised=False,
                                 *args, **kwargs) -&gt; None:
                super().__init__(*args, **kwargs)
                # import pdb; pdb.set_trace()
                cfg = resolve_config(cfg)
                self.save_hyperparameters(cfg, ignore=[&#39;loss_func&#39;])
                self.cfg = cfg
                model_cfg = cfg.get(&#34;model_cfg&#34;, {})
                self.model_cfg = model_cfg or {}
                self.lr = cfg.hp.lr
                self.batch_size = cfg.hp.batch_size
                self.num_classes = num_classes or self.model_cfg.head.get(&#34;num_classes&#34;)
                self.name = name or self.model_cfg.get(&#34;name&#34;)
                
                self.setup_loss(loss_func)
                self.setup_metrics()
                self.net = build_model(backbone_cfg=self.model_cfg.backbone,
                                                           head_cfg=self.model_cfg.head)
                # self.backbone

                if self.cfg.train.freeze_backbone:
                        self.freeze_up_to(layer=self.cfg.train.get(&#34;freeze_backbone_up_to&#34;),
                                                          submodule=&#34;backbone&#34;,
                                                          verbose=False)        # def on_fit_start(self):

                
                if self.cfg.logging.log_model_summary:
                        self.summarize_model(f&#34;{self.name}/init&#34;)

#       def on_fit_start(self):
                
                # if self.cfg.logging.log_model_summary:
                #       self.summarize_model()

        def setup_loss(self,
                                   loss_func: Optional[Union[Callable, str]]=None):
                if isinstance(loss_func, Callable):
                        self.loss = loss_func
                else:
                        self.loss = hydra.utils.instantiate(self.model_cfg.loss)

        # def summarize_model(self):
        #       cfg = self.cfg
        #       input_size = (1, *OmegaConf.to_container(cfg.model_cfg.input_shape, resolve=True))
        #       model_summary = log_model_summary(model=self.net,
        #                                       input_size=input_size,
        #                                       full_summary=True,
        #                                       working_dir=cfg.checkpoint_dir,
        #                                       model_name = cfg.model_cfg.name,
        #                                       verbose=1)


        def setup_metrics(self):
                
                self.train_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                           average=&#34;macro&#34;,
                                                                                           prefix=&#34;train&#34;)
                self.val_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                           average=&#34;macro&#34;,
                                                                                           prefix=&#34;val&#34;)
                self.test_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                           average=&#34;macro&#34;,
                                                                                           prefix=&#34;test&#34;)
                

        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
                return self.net(x)

        def step(self, x, y) -&gt; Dict[str, torch.Tensor]:
                &#34;&#34;&#34;
                TODO: remove the &#34;x&#34; from common method self.step() outputs &amp; benchmark reduction in GPU memory leaks
                &#34;&#34;&#34;
                # if self.self_supervised:
                #       z1, z2 = self.shared_step(x)
                #       loss = self.loss(z1, z2)
                # else:
                logits = self(x)
                                
                loss = self.loss(logits, y)
                
                # pp(f&#34;logits.shape: {logits.shape}, y.shape: {y.shape}, loss.shape: {loss.shape}&#34;)
                # pp(f&#34;logits.min(): {logits.min()}, logits.max(): {logits.max()}, logits.mean(axis=-1): {logits.mean(axis=-1)}, logits.std(axis=-1): {logits.std(axis=-1)}&#34;)
                # _y = y.type(torch.cuda.FloatTensor)
                # pp(f&#34;y.min(): {_y.min()}, y.max(): {_y.max()}, y.mean(axis=-1): {_y.mean(axis=-1)}, y.std(axis=-1): {_y.std(axis=-1)}&#34;)
                # pp(f&#34;loss.min(): {loss.min()}, loss.max(): {loss.max()}, loss.mean(axis=-1): {loss.mean(axis=-1)}, loss.std(axis=-1): {loss.std(axis=-1)}&#34;)

                return {&#34;logits&#34;: logits, &#34;loss&#34;: loss, &#34;y&#34;: y, &#34;x&#34;: x}

        def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:

                x, y = batch[:2]
                # import pdb; pdb.set_trace()
                out = self.step(x, y)
                
                # print(&#34;self.training_step: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}

        def training_step_end(self, out):
                # print(&#34;self.training_step_end: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                self.train_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
                
                batch_size=self.batch_size #len(out[&#34;y&#34;])
                loss = out[&#34;loss&#34;].mean()
                log_dict = {
                        &#34;train_loss&#34;: loss,
                        **self.train_metric
                }
                self.log_dict(
                        log_dict,
                        on_step=True,
                        on_epoch=True,
                        prog_bar=True,
                        batch_size=batch_size
                )
                self.print(f&#34;training_step_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, loss: {loss}&#34;)
                return loss

        def validation_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch[:2]
                out = self.step(x, y)
                # self.
                # print(&#34;self.validation_step: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}
        
        def validation_step_end(self, out):
                # self.
                # print(&#34;self.validation_step_end: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                self.val_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
                batch_size=self.batch_size #len(out[&#34;y&#34;])
                loss = out[&#34;loss&#34;].mean()
                
                log_dict = {
                        &#34;val_loss&#34;: loss,
                        **self.val_metric
                }
                # if &#34;val/F1_top1&#34; in self.val_metric.keys():
                #       log_dict[&#34;val_F1&#34;] = self.val_metric[&#34;val/F1_top1&#34;]
                #       log_dict.update({k:v for k,v in self.val_metric.items() if k != &#34;val/F1_top1&#34;})
                # elif &#34;val_macro_F1&#34; in self.val_metric.keys():
                #       log_dict[&#34;val_F1&#34;] = self.val_metric[&#34;val_macro_F1&#34;]
                #       log_dict.update({k:v for k,v in self.val_metric.items() if k != &#34;val_macro_F1&#34;})
                # else:
                # log_dict.update(self.val_metric)
                # self.log(&#34;val_macro_F1&#34;, self.val_metric[&#34;val_macro_F1&#34;])
                self.log_dict(log_dict,
                                          on_step=True, # False, #
                                          on_epoch=True,
                                          prog_bar=True,
                                          batch_size=batch_size)
                self.print(f&#34;validation_step_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, loss: {loss}&#34;)

                # self.log_dict(self.val_metric,
                #                         on_step=True,
                #                         on_epoch=True,
                #                         # prog_bar=True,
                #                         batch_size=batch_size)
                
                # return {
                #       # &#34;image&#34;: out[&#34;x&#34;],
                #       &#34;y_true&#34;: out[&#34;y&#34;],
                #       &#34;logits&#34;: out[&#34;logits&#34;],
                #       &#34;val_loss&#34;: loss,
                # }

        def test_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch[:2]
                out = self.step(x, y)
                return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}

        def test_step_end(self, out):
                batch_size=len(out[&#34;y&#34;])
                self.test_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;test_loss&#34;: out[&#34;loss&#34;].mean(),
                                **self.test_metric,
                        },
                        batch_size=batch_size
                )
                return {
                        # &#34;image&#34;: out[&#34;x&#34;],
                        &#34;y_true&#34;: out[&#34;y&#34;],
                        &#34;logits&#34;: out[&#34;logits&#34;],
                        &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                }

        def predict_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                if len(batch)==3:
                        x, y, metadata = batch[:3]
                        image_idx = metadata.get(&#34;image_id&#34;)
                else:
                        x, y = batch[:2]
                        image_idx = torch.arange(0, len(x)) + batch_idx*self.batch_size

                y_logit = self(x)
                return {&#34;image_id&#34;:image_idx,
                                &#34;y_logit&#34;:y_logit}

        def training_epoch_end(self, outputs: List[Any]) -&gt; None:
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                info = {k: v.shape for k,v in outputs[0].items()}
                # print(&#34;self.training_epoch_end: &#34;, f&#34;device:{torch.cuda.current_device()}, len(outputs)={len(outputs)}, info: {info}&#34;)

                # losses = []
                # for o in outputs:
                #       losses.append(o[&#34;loss&#34;])
                # losses = torch.stack(losses)
                losses = torch.stack([o[&#34;loss&#34;] for o in outputs])
                self.print(f&#34;training_epoch_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, losses: {losses}&#34;)
                # print(f&#34;self.validation_epoch_end (torch.stack the losses): losses.shape = {losses.shape}&#34;)
                


        def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                info = {k: v.shape for k,v in outputs[0].items()}
                print(&#34;self.validation_epoch_end: &#34;, f&#34;device:{torch.cuda.current_device()}, len(outputs)={len(outputs)}, info: {info}&#34;)
                
                # losses = torch.stack([o[&#34;loss&#34;] for o in outputs])
                losses = []
                y = []
                logits = []
                for o in outputs:
                        losses.append(o[&#34;loss&#34;])
                        y.append(o[&#34;y&#34;])
                        logits = [o[&#34;logits&#34;]]
                losses = torch.stack(losses)
                y = torch.cat(y)
                logits = torch.cat(logits)
                
                self.print(f&#34;validation_epoch_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, losses: {losses}&#34;)
                # print(f&#34;self.validation_epoch_end (torch.stack the losses): losses.shape = {losses.shape}, y.shape = {y.shape}, logits.shape = {logits.shape}&#34;)
                
                if &#34;image&#34; not in outputs:
                        # print(f&#34;Skipping val render_image_predictions due to missing &#39;image&#39; key in epoch outputs.&#34;)
                        return
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;val_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step,
                        commit=False)


        def test_epoch_end(self, outputs: List[Any]) -&gt; None:
                if &#34;image&#34; not in outputs:
                        # print(f&#34;Skipping test render_image_predictions due to missing &#39;image&#39; key in epoch outputs.&#34;)
                        return
                
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;test_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step,
                        commit=False)


        @staticmethod
        def render_image_predictions(
                outputs: List[Any],
                batch_size: int,
                n_elements_to_log: int,
                log_name: str=&#34;image predictions&#34;,
                normalize_visualization: bool=True,
                logger=None,
                global_step: int=0,
                commit: bool=False
        ) -&gt; None:
                
                # images_feat_viz = []
                # integrated_gradients = IntegratedGradients(self.forward)
                # noise_tunnel = NoiseTunnel(integrated_gradients)
                
                images = []
                for output_element in iterate_elements_in_batches(
                        outputs, batch_size, n_elements_to_log
                ):  
                        rendered_image = render_images(
                                output_element[&#34;image&#34;],
                                autoshow=False,
                                normalize=normalize_visualization)
                        caption = f&#34;y_pred: {output_element[&#39;logits&#39;].argmax()}  [gt: {output_element[&#39;y_true&#39;]}]&#34;  # noqa      
                        # attributions_ig_nt = noise_tunnel.attribute(output_element[&#34;image&#34;].unsqueeze(0), nt_samples=50,
                                                                                                                # nt_type=&#39;smoothgrad_sq&#39;, target=output_element[&#34;y_true&#34;],
                                                                                                                # internal_batch_size=50)
                        images.append(
                                wandb.Image(
                                        rendered_image,
                                        caption=caption,
                                )
                        )
                if logger is not None:
                        logger.experiment.log(
                                {log_name: images,
                                 &#34;global_step&#34;:global_step},
                                commit=commit
                        )


        def configure_optimizers(
                self,
        ) -&gt; Union[Optimizer, Tuple[Sequence[Optimizer], Sequence[Any]]]:
                &#34;&#34;&#34;
                Choose what optimizers and learning-rate schedulers to use in your optimization.
                Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.
                Return:
                        Any of these 6 options.
                        - Single optimizer.
                        - List or Tuple - List of optimizers.
                        - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
                        - Dictionary, with an &#39;optimizer&#39; key, and (optionally) a &#39;lr_scheduler&#39;
                          key whose value is a single LR scheduler or lr_dict.
                        - Tuple of dictionaries as described, with an optional &#39;frequency&#39; key.
                        - None - Fit will run without any optimizer.
                &#34;&#34;&#34;
                # if hasattr(self.cfg.optim, &#34;exclude_bn_bias&#34;) and \
                                # self.cfg.optim.get(&#34;exclude_bn_bias&#34;, False):
                if self.cfg.optim.get(&#34;exclude_bn_bias&#34;, False):
                        params = self.exclude_from_wt_decay(self.named_parameters(), weight_decay=self.cfg.optim.optimizer.weight_decay)
                else:
                        params = self.parameters()

                # pp(OmegaConf.to_container(self.cfg.optim.optimizer))
                pp(OmegaConf.to_container(self.cfg.optim.optimizer, resolve=True))
                opt = hydra.utils.instantiate(
                        OmegaConf.to_container(self.cfg.optim.optimizer),
                        params=params,
                        weight_decay=self.cfg.optim.optimizer.weight_decay,
                        _convert_=&#34;partial&#34;
                )

                out = opt
                if self.cfg.optim.use_lr_scheduler:
                        lr_scheduler = self.cfg.optim.lr_scheduler
                        scheduler = hydra.utils.instantiate(lr_scheduler, optimizer=opt)
                        out = ([opt], [scheduler])
                return out

        @staticmethod
        def exclude_from_wt_decay(named_params: List[Tuple[str, torch.Tensor]],
                                                          weight_decay: float,
                                                          skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)
                                                         ) -&gt; List[Dict[str, Any]]:
                &#34;&#34;&#34;
                Sort named_params into 2 groups: included &amp; excluded from weight decay.
                Includes any params with a name that doesn&#39;t match any pattern in `skip_list`.
                
                Arguments:
                        named_params: List[Tuple[str, torch.Tensor]]
                        weight_decay: float,
                        skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)):          
                &#34;&#34;&#34;
                params = []
                excluded_params = []

                for name, param in named_params:
                        if not param.requires_grad:
                                continue
                        elif any(layer_name in name for layer_name in skip_list):
                                excluded_params.append(param)
                        else:
                                params.append(param)

                return [
                        {&#34;params&#34;: params, &#34;weight_decay&#34;: weight_decay},
                        {
                                &#34;params&#34;: excluded_params,
                                &#34;weight_decay&#34;: 0.0,
                        },
                ]
        
# [TODO] Uncomment &amp; benchmark this
# source: https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html#set-grads-to-none
        # def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
        #        optimizer.zero_grad(set_to_none=True)
        
        

#       def init_metrics(self,
#                                        stage: str=&#39;train&#39;,
#                                        tag: Optional[str]=None):
#               tag = tag or &#34;&#34;
#               if not hasattr(self, &#34;all_metrics&#34;):
#                       self.all_metrics = {}
                
#               if not hasattr(self,&#34;num_classes&#34;) and hasattr(self.hparams, &#34;num_classes&#34;):
#                       self.num_classes = self.hparams.num_classes
                
#               print(f&#34;self.num_classes={self.num_classes}&#34;)
#               if stage in [&#39;train&#39;, &#39;all&#39;]:
#                       prefix=f&#39;{tag}_train&#39;.strip(&#34;_&#34;)
#                       self.metrics_train = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
#                       self.metrics_train_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;train&#39;)
#                       self.all_metrics[&#39;train&#39;] = {&#34;scalar&#34;:self.metrics_train,
#                                                                                &#34;per_class&#34;:self.metrics_train_per_class}
                        
#               if stage in [&#39;val&#39;, &#39;all&#39;]:
#                       prefix=f&#39;{tag}_val&#39;.strip(&#34;_&#34;)
#                       self.metrics_val = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
#                       self.metrics_val_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=&#39;val&#39;)
#                       self.all_metrics[&#39;val&#39;] = {&#34;scalar&#34;:self.metrics_val,
#                                                                          &#34;per_class&#34;:self.metrics_val_per_class}
                        
#               if stage in [&#39;test&#39;, &#39;all&#39;]:
#                       if isinstance(tag, str):
#                               prefix=tag
#                       else:
#                               prefix = &#34;test&#34;
# #                      prefix=f&#39;{tag}_test&#39;.strip(&#34;_&#34;)
#                       self.metrics_test = get_scalar_metrics(num_classes=self.num_classes, average=&#39;macro&#39;, prefix=prefix)
#                       self.metrics_test_per_class = get_per_class_metrics(num_classes=self.num_classes, prefix=prefix)
#                       self.all_metrics[&#39;test&#39;] = {&#34;scalar&#34;:self.metrics_test,
#                                                                               &#34;per_class&#34;:self.metrics_test_per_class}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier"><code class="flex name class">
<span>class <span class="ident">LitClassifier</span></span>
<span>(</span><span>cfg:Â omegaconf.dictconfig.DictConfigÂ =Â None, model_cfg:Â omegaconf.dictconfig.DictConfigÂ =Â None, name:Â strÂ =Â None, num_classes:Â intÂ =Â None, loss_func:Â Union[Callable,Â str]Â =Â None, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements some more custom boiler plate for custom lightning modules</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LitClassifier(BaseLightningModule): #pl.LightningModule):
        def __init__(self,
                                 cfg: DictConfig=None,
                                 model_cfg: DictConfig=None, 
                                 name: str=None,
                                 num_classes: int=None, 
                                 loss_func: Union[Callable, str]=None,
                                 # pretrain : bool = True,
                                 # self_supervised=False,
                                 *args, **kwargs) -&gt; None:
                super().__init__(*args, **kwargs)
                # import pdb; pdb.set_trace()
                cfg = resolve_config(cfg)
                self.save_hyperparameters(cfg, ignore=[&#39;loss_func&#39;])
                self.cfg = cfg
                model_cfg = cfg.get(&#34;model_cfg&#34;, {})
                self.model_cfg = model_cfg or {}
                self.lr = cfg.hp.lr
                self.batch_size = cfg.hp.batch_size
                self.num_classes = num_classes or self.model_cfg.head.get(&#34;num_classes&#34;)
                self.name = name or self.model_cfg.get(&#34;name&#34;)
                
                self.setup_loss(loss_func)
                self.setup_metrics()
                self.net = build_model(backbone_cfg=self.model_cfg.backbone,
                                                           head_cfg=self.model_cfg.head)
                # self.backbone

                if self.cfg.train.freeze_backbone:
                        self.freeze_up_to(layer=self.cfg.train.get(&#34;freeze_backbone_up_to&#34;),
                                                          submodule=&#34;backbone&#34;,
                                                          verbose=False)        # def on_fit_start(self):

                
                if self.cfg.logging.log_model_summary:
                        self.summarize_model(f&#34;{self.name}/init&#34;)

#       def on_fit_start(self):
                
                # if self.cfg.logging.log_model_summary:
                #       self.summarize_model()

        def setup_loss(self,
                                   loss_func: Optional[Union[Callable, str]]=None):
                if isinstance(loss_func, Callable):
                        self.loss = loss_func
                else:
                        self.loss = hydra.utils.instantiate(self.model_cfg.loss)

        # def summarize_model(self):
        #       cfg = self.cfg
        #       input_size = (1, *OmegaConf.to_container(cfg.model_cfg.input_shape, resolve=True))
        #       model_summary = log_model_summary(model=self.net,
        #                                       input_size=input_size,
        #                                       full_summary=True,
        #                                       working_dir=cfg.checkpoint_dir,
        #                                       model_name = cfg.model_cfg.name,
        #                                       verbose=1)


        def setup_metrics(self):
                
                self.train_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                           average=&#34;macro&#34;,
                                                                                           prefix=&#34;train&#34;)
                self.val_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                           average=&#34;macro&#34;,
                                                                                           prefix=&#34;val&#34;)
                self.test_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                           average=&#34;macro&#34;,
                                                                                           prefix=&#34;test&#34;)
                

        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
                return self.net(x)

        def step(self, x, y) -&gt; Dict[str, torch.Tensor]:
                &#34;&#34;&#34;
                TODO: remove the &#34;x&#34; from common method self.step() outputs &amp; benchmark reduction in GPU memory leaks
                &#34;&#34;&#34;
                # if self.self_supervised:
                #       z1, z2 = self.shared_step(x)
                #       loss = self.loss(z1, z2)
                # else:
                logits = self(x)
                                
                loss = self.loss(logits, y)
                
                # pp(f&#34;logits.shape: {logits.shape}, y.shape: {y.shape}, loss.shape: {loss.shape}&#34;)
                # pp(f&#34;logits.min(): {logits.min()}, logits.max(): {logits.max()}, logits.mean(axis=-1): {logits.mean(axis=-1)}, logits.std(axis=-1): {logits.std(axis=-1)}&#34;)
                # _y = y.type(torch.cuda.FloatTensor)
                # pp(f&#34;y.min(): {_y.min()}, y.max(): {_y.max()}, y.mean(axis=-1): {_y.mean(axis=-1)}, y.std(axis=-1): {_y.std(axis=-1)}&#34;)
                # pp(f&#34;loss.min(): {loss.min()}, loss.max(): {loss.max()}, loss.mean(axis=-1): {loss.mean(axis=-1)}, loss.std(axis=-1): {loss.std(axis=-1)}&#34;)

                return {&#34;logits&#34;: logits, &#34;loss&#34;: loss, &#34;y&#34;: y, &#34;x&#34;: x}

        def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:

                x, y = batch[:2]
                # import pdb; pdb.set_trace()
                out = self.step(x, y)
                
                # print(&#34;self.training_step: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}

        def training_step_end(self, out):
                # print(&#34;self.training_step_end: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                self.train_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
                
                batch_size=self.batch_size #len(out[&#34;y&#34;])
                loss = out[&#34;loss&#34;].mean()
                log_dict = {
                        &#34;train_loss&#34;: loss,
                        **self.train_metric
                }
                self.log_dict(
                        log_dict,
                        on_step=True,
                        on_epoch=True,
                        prog_bar=True,
                        batch_size=batch_size
                )
                self.print(f&#34;training_step_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, loss: {loss}&#34;)
                return loss

        def validation_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch[:2]
                out = self.step(x, y)
                # self.
                # print(&#34;self.validation_step: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}
        
        def validation_step_end(self, out):
                # self.
                # print(&#34;self.validation_step_end: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
                self.val_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
                batch_size=self.batch_size #len(out[&#34;y&#34;])
                loss = out[&#34;loss&#34;].mean()
                
                log_dict = {
                        &#34;val_loss&#34;: loss,
                        **self.val_metric
                }
                # if &#34;val/F1_top1&#34; in self.val_metric.keys():
                #       log_dict[&#34;val_F1&#34;] = self.val_metric[&#34;val/F1_top1&#34;]
                #       log_dict.update({k:v for k,v in self.val_metric.items() if k != &#34;val/F1_top1&#34;})
                # elif &#34;val_macro_F1&#34; in self.val_metric.keys():
                #       log_dict[&#34;val_F1&#34;] = self.val_metric[&#34;val_macro_F1&#34;]
                #       log_dict.update({k:v for k,v in self.val_metric.items() if k != &#34;val_macro_F1&#34;})
                # else:
                # log_dict.update(self.val_metric)
                # self.log(&#34;val_macro_F1&#34;, self.val_metric[&#34;val_macro_F1&#34;])
                self.log_dict(log_dict,
                                          on_step=True, # False, #
                                          on_epoch=True,
                                          prog_bar=True,
                                          batch_size=batch_size)
                self.print(f&#34;validation_step_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, loss: {loss}&#34;)

                # self.log_dict(self.val_metric,
                #                         on_step=True,
                #                         on_epoch=True,
                #                         # prog_bar=True,
                #                         batch_size=batch_size)
                
                # return {
                #       # &#34;image&#34;: out[&#34;x&#34;],
                #       &#34;y_true&#34;: out[&#34;y&#34;],
                #       &#34;logits&#34;: out[&#34;logits&#34;],
                #       &#34;val_loss&#34;: loss,
                # }

        def test_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                x, y = batch[:2]
                out = self.step(x, y)
                return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}

        def test_step_end(self, out):
                batch_size=len(out[&#34;y&#34;])
                self.test_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
                self.log_dict(
                        {
                                &#34;test_loss&#34;: out[&#34;loss&#34;].mean(),
                                **self.test_metric,
                        },
                        batch_size=batch_size
                )
                return {
                        # &#34;image&#34;: out[&#34;x&#34;],
                        &#34;y_true&#34;: out[&#34;y&#34;],
                        &#34;logits&#34;: out[&#34;logits&#34;],
                        &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
                }

        def predict_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
                if len(batch)==3:
                        x, y, metadata = batch[:3]
                        image_idx = metadata.get(&#34;image_id&#34;)
                else:
                        x, y = batch[:2]
                        image_idx = torch.arange(0, len(x)) + batch_idx*self.batch_size

                y_logit = self(x)
                return {&#34;image_id&#34;:image_idx,
                                &#34;y_logit&#34;:y_logit}

        def training_epoch_end(self, outputs: List[Any]) -&gt; None:
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                info = {k: v.shape for k,v in outputs[0].items()}
                # print(&#34;self.training_epoch_end: &#34;, f&#34;device:{torch.cuda.current_device()}, len(outputs)={len(outputs)}, info: {info}&#34;)

                # losses = []
                # for o in outputs:
                #       losses.append(o[&#34;loss&#34;])
                # losses = torch.stack(losses)
                losses = torch.stack([o[&#34;loss&#34;] for o in outputs])
                self.print(f&#34;training_epoch_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, losses: {losses}&#34;)
                # print(f&#34;self.validation_epoch_end (torch.stack the losses): losses.shape = {losses.shape}&#34;)
                


        def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                info = {k: v.shape for k,v in outputs[0].items()}
                print(&#34;self.validation_epoch_end: &#34;, f&#34;device:{torch.cuda.current_device()}, len(outputs)={len(outputs)}, info: {info}&#34;)
                
                # losses = torch.stack([o[&#34;loss&#34;] for o in outputs])
                losses = []
                y = []
                logits = []
                for o in outputs:
                        losses.append(o[&#34;loss&#34;])
                        y.append(o[&#34;y&#34;])
                        logits = [o[&#34;logits&#34;]]
                losses = torch.stack(losses)
                y = torch.cat(y)
                logits = torch.cat(logits)
                
                self.print(f&#34;validation_epoch_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, losses: {losses}&#34;)
                # print(f&#34;self.validation_epoch_end (torch.stack the losses): losses.shape = {losses.shape}, y.shape = {y.shape}, logits.shape = {logits.shape}&#34;)
                
                if &#34;image&#34; not in outputs:
                        # print(f&#34;Skipping val render_image_predictions due to missing &#39;image&#39; key in epoch outputs.&#34;)
                        return
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;val_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step,
                        commit=False)


        def test_epoch_end(self, outputs: List[Any]) -&gt; None:
                if &#34;image&#34; not in outputs:
                        # print(f&#34;Skipping test render_image_predictions due to missing &#39;image&#39; key in epoch outputs.&#34;)
                        return
                
                self.render_image_predictions(
                        outputs=outputs,
                        batch_size=self.cfg.data.datamodule.batch_size,
                        n_elements_to_log=self.cfg.logging.n_elements_to_log,
                        log_name=&#34;test_image_predictions&#34;,
                        normalize_visualization=self.cfg.logging.normalize_visualization,
                        logger=self.logger,
                        global_step=self.global_step,
                        commit=False)


        @staticmethod
        def render_image_predictions(
                outputs: List[Any],
                batch_size: int,
                n_elements_to_log: int,
                log_name: str=&#34;image predictions&#34;,
                normalize_visualization: bool=True,
                logger=None,
                global_step: int=0,
                commit: bool=False
        ) -&gt; None:
                
                # images_feat_viz = []
                # integrated_gradients = IntegratedGradients(self.forward)
                # noise_tunnel = NoiseTunnel(integrated_gradients)
                
                images = []
                for output_element in iterate_elements_in_batches(
                        outputs, batch_size, n_elements_to_log
                ):  
                        rendered_image = render_images(
                                output_element[&#34;image&#34;],
                                autoshow=False,
                                normalize=normalize_visualization)
                        caption = f&#34;y_pred: {output_element[&#39;logits&#39;].argmax()}  [gt: {output_element[&#39;y_true&#39;]}]&#34;  # noqa      
                        # attributions_ig_nt = noise_tunnel.attribute(output_element[&#34;image&#34;].unsqueeze(0), nt_samples=50,
                                                                                                                # nt_type=&#39;smoothgrad_sq&#39;, target=output_element[&#34;y_true&#34;],
                                                                                                                # internal_batch_size=50)
                        images.append(
                                wandb.Image(
                                        rendered_image,
                                        caption=caption,
                                )
                        )
                if logger is not None:
                        logger.experiment.log(
                                {log_name: images,
                                 &#34;global_step&#34;:global_step},
                                commit=commit
                        )


        def configure_optimizers(
                self,
        ) -&gt; Union[Optimizer, Tuple[Sequence[Optimizer], Sequence[Any]]]:
                &#34;&#34;&#34;
                Choose what optimizers and learning-rate schedulers to use in your optimization.
                Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.
                Return:
                        Any of these 6 options.
                        - Single optimizer.
                        - List or Tuple - List of optimizers.
                        - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
                        - Dictionary, with an &#39;optimizer&#39; key, and (optionally) a &#39;lr_scheduler&#39;
                          key whose value is a single LR scheduler or lr_dict.
                        - Tuple of dictionaries as described, with an optional &#39;frequency&#39; key.
                        - None - Fit will run without any optimizer.
                &#34;&#34;&#34;
                # if hasattr(self.cfg.optim, &#34;exclude_bn_bias&#34;) and \
                                # self.cfg.optim.get(&#34;exclude_bn_bias&#34;, False):
                if self.cfg.optim.get(&#34;exclude_bn_bias&#34;, False):
                        params = self.exclude_from_wt_decay(self.named_parameters(), weight_decay=self.cfg.optim.optimizer.weight_decay)
                else:
                        params = self.parameters()

                # pp(OmegaConf.to_container(self.cfg.optim.optimizer))
                pp(OmegaConf.to_container(self.cfg.optim.optimizer, resolve=True))
                opt = hydra.utils.instantiate(
                        OmegaConf.to_container(self.cfg.optim.optimizer),
                        params=params,
                        weight_decay=self.cfg.optim.optimizer.weight_decay,
                        _convert_=&#34;partial&#34;
                )

                out = opt
                if self.cfg.optim.use_lr_scheduler:
                        lr_scheduler = self.cfg.optim.lr_scheduler
                        scheduler = hydra.utils.instantiate(lr_scheduler, optimizer=opt)
                        out = ([opt], [scheduler])
                return out

        @staticmethod
        def exclude_from_wt_decay(named_params: List[Tuple[str, torch.Tensor]],
                                                          weight_decay: float,
                                                          skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)
                                                         ) -&gt; List[Dict[str, Any]]:
                &#34;&#34;&#34;
                Sort named_params into 2 groups: included &amp; excluded from weight decay.
                Includes any params with a name that doesn&#39;t match any pattern in `skip_list`.
                
                Arguments:
                        named_params: List[Tuple[str, torch.Tensor]]
                        weight_decay: float,
                        skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)):          
                &#34;&#34;&#34;
                params = []
                excluded_params = []

                for name, param in named_params:
                        if not param.requires_grad:
                                continue
                        elif any(layer_name in name for layer_name in skip_list):
                                excluded_params.append(param)
                        else:
                                params.append(param)

                return [
                        {&#34;params&#34;: params, &#34;weight_decay&#34;: weight_decay},
                        {
                                &#34;params&#34;: excluded_params,
                                &#34;weight_decay&#34;: 0.0,
                        },
                ]
        
# [TODO] Uncomment &amp; benchmark this
# source: https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html#set-grads-to-none
        # def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
        #        optimizer.zero_grad(set_to_none=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imutils.ml.models.base.BaseLightningModule" href="../base.html#imutils.ml.models.base.BaseLightningModule">BaseLightningModule</a></li>
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.exclude_from_wt_decay"><code class="name flex">
<span>def <span class="ident">exclude_from_wt_decay</span></span>(<span>named_params:Â List[Tuple[str,Â torch.Tensor]], weight_decay:Â float, skip_list:Â Tuple[str]Â =Â ('bias', 'bn')) â€‘>Â List[Dict[str,Â Any]]</span>
</code></dt>
<dd>
<div class="desc"><p>Sort named_params into 2 groups: included &amp; excluded from weight decay.
Includes any params with a name that doesn't match any pattern in <code>skip_list</code>.</p>
<h2 id="arguments">Arguments</h2>
<p>named_params: List[Tuple[str, torch.Tensor]]
weight_decay: float,
skip_list: Tuple[str]=("bias", "bn")):</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def exclude_from_wt_decay(named_params: List[Tuple[str, torch.Tensor]],
                                                  weight_decay: float,
                                                  skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)
                                                 ) -&gt; List[Dict[str, Any]]:
        &#34;&#34;&#34;
        Sort named_params into 2 groups: included &amp; excluded from weight decay.
        Includes any params with a name that doesn&#39;t match any pattern in `skip_list`.
        
        Arguments:
                named_params: List[Tuple[str, torch.Tensor]]
                weight_decay: float,
                skip_list: Tuple[str]=(&#34;bias&#34;, &#34;bn&#34;)):          
        &#34;&#34;&#34;
        params = []
        excluded_params = []

        for name, param in named_params:
                if not param.requires_grad:
                        continue
                elif any(layer_name in name for layer_name in skip_list):
                        excluded_params.append(param)
                else:
                        params.append(param)

        return [
                {&#34;params&#34;: params, &#34;weight_decay&#34;: weight_decay},
                {
                        &#34;params&#34;: excluded_params,
                        &#34;weight_decay&#34;: 0.0,
                },
        ]</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.render_image_predictions"><code class="name flex">
<span>def <span class="ident">render_image_predictions</span></span>(<span>outputs:Â List[Any], batch_size:Â int, n_elements_to_log:Â int, log_name:Â strÂ =Â 'image predictions', normalize_visualization:Â boolÂ =Â True, logger=None, global_step:Â intÂ =Â 0, commit:Â boolÂ =Â False) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def render_image_predictions(
        outputs: List[Any],
        batch_size: int,
        n_elements_to_log: int,
        log_name: str=&#34;image predictions&#34;,
        normalize_visualization: bool=True,
        logger=None,
        global_step: int=0,
        commit: bool=False
) -&gt; None:
        
        # images_feat_viz = []
        # integrated_gradients = IntegratedGradients(self.forward)
        # noise_tunnel = NoiseTunnel(integrated_gradients)
        
        images = []
        for output_element in iterate_elements_in_batches(
                outputs, batch_size, n_elements_to_log
        ):  
                rendered_image = render_images(
                        output_element[&#34;image&#34;],
                        autoshow=False,
                        normalize=normalize_visualization)
                caption = f&#34;y_pred: {output_element[&#39;logits&#39;].argmax()}  [gt: {output_element[&#39;y_true&#39;]}]&#34;  # noqa      
                # attributions_ig_nt = noise_tunnel.attribute(output_element[&#34;image&#34;].unsqueeze(0), nt_samples=50,
                                                                                                        # nt_type=&#39;smoothgrad_sq&#39;, target=output_element[&#34;y_true&#34;],
                                                                                                        # internal_batch_size=50)
                images.append(
                        wandb.Image(
                                rendered_image,
                                caption=caption,
                        )
                )
        if logger is not None:
                logger.experiment.log(
                        {log_name: images,
                         &#34;global_step&#34;:global_step},
                        commit=commit
                )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self) â€‘>Â Union[torch.optim.optimizer.Optimizer,Â Tuple[Sequence[torch.optim.optimizer.Optimizer],Â Sequence[Any]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you'd need one. But in the case of GANs or similar you might have multiple.</p>
<h2 id="return">Return</h2>
<p>Any of these 6 options.
- Single optimizer.
- List or Tuple - List of optimizers.
- Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
- Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler'
key whose value is a single LR scheduler or lr_dict.
- Tuple of dictionaries as described, with an optional 'frequency' key.
- None - Fit will run without any optimizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(
        self,
) -&gt; Union[Optimizer, Tuple[Sequence[Optimizer], Sequence[Any]]]:
        &#34;&#34;&#34;
        Choose what optimizers and learning-rate schedulers to use in your optimization.
        Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.
        Return:
                Any of these 6 options.
                - Single optimizer.
                - List or Tuple - List of optimizers.
                - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).
                - Dictionary, with an &#39;optimizer&#39; key, and (optionally) a &#39;lr_scheduler&#39;
                  key whose value is a single LR scheduler or lr_dict.
                - Tuple of dictionaries as described, with an optional &#39;frequency&#39; key.
                - None - Fit will run without any optimizer.
        &#34;&#34;&#34;
        # if hasattr(self.cfg.optim, &#34;exclude_bn_bias&#34;) and \
                        # self.cfg.optim.get(&#34;exclude_bn_bias&#34;, False):
        if self.cfg.optim.get(&#34;exclude_bn_bias&#34;, False):
                params = self.exclude_from_wt_decay(self.named_parameters(), weight_decay=self.cfg.optim.optimizer.weight_decay)
        else:
                params = self.parameters()

        # pp(OmegaConf.to_container(self.cfg.optim.optimizer))
        pp(OmegaConf.to_container(self.cfg.optim.optimizer, resolve=True))
        opt = hydra.utils.instantiate(
                OmegaConf.to_container(self.cfg.optim.optimizer),
                params=params,
                weight_decay=self.cfg.optim.optimizer.weight_decay,
                _convert_=&#34;partial&#34;
        )

        out = opt
        if self.cfg.optim.use_lr_scheduler:
                lr_scheduler = self.cfg.optim.lr_scheduler
                scheduler = hydra.utils.instantiate(lr_scheduler, optimizer=opt)
                out = ([opt], [scheduler])
        return out</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.predict_step"><code class="name flex">
<span>def <span class="ident">predict_step</span></span>(<span>self, batch:Â Any, batch_idx:Â int) â€‘>Â Dict[str,Â torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Step function called during :meth:<code>~pytorch_lightning.trainer.trainer.Trainer.predict</code>. By default, it
calls :meth:<code>~pytorch_lightning.core.lightning.LightningModule.forward</code>. Override to add any processing
logic.</p>
<p>The :meth:<code>~pytorch_lightning.core.lightning.LightningModule.predict_step</code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use :class:<code>~pytorch_lightning.callbacks.BasePredictionWriter</code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The :class:<code>~pytorch_lightning.callbacks.BasePredictionWriter</code> should be used while using a spawn
based accelerator. This happens for <code>Trainer(strategy="ddp_spawn")</code>
or training on 8 TPU cores with <code>Trainer(tpu_cores=8)</code> as predictions won't be returned.</p>
<p>Example ::</p>
<pre><code>class MyModel(LightningModule):

    def predicts_step(self, batch, batch_idx, dataloader_idx):
        return self(batch)

dm = ...
model = MyModel()
trainer = Trainer(gpus=2)
predictions = trainer.predict(model, dm)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>Current batch</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>Index of current batch</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>Index of the current dataloader</dd>
</dl>
<h2 id="return">Return</h2>
<p>Predicted output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
        if len(batch)==3:
                x, y, metadata = batch[:3]
                image_idx = metadata.get(&#34;image_id&#34;)
        else:
                x, y = batch[:2]
                image_idx = torch.arange(0, len(x)) + batch_idx*self.batch_size

        y_logit = self(x)
        return {&#34;image_id&#34;:image_idx,
                        &#34;y_logit&#34;:y_logit}</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.setup_loss"><code class="name flex">
<span>def <span class="ident">setup_loss</span></span>(<span>self, loss_func:Â Union[Callable,Â str,Â None]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_loss(self,
                           loss_func: Optional[Union[Callable, str]]=None):
        if isinstance(loss_func, Callable):
                self.loss = loss_func
        else:
                self.loss = hydra.utils.instantiate(self.model_cfg.loss)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.setup_metrics"><code class="name flex">
<span>def <span class="ident">setup_metrics</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_metrics(self):
        
        self.train_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                   average=&#34;macro&#34;,
                                                                                   prefix=&#34;train&#34;)
        self.val_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                   average=&#34;macro&#34;,
                                                                                   prefix=&#34;val&#34;)
        self.test_metric = get_scalar_metrics(num_classes=self.num_classes,
                                                                                   average=&#34;macro&#34;,
                                                                                   prefix=&#34;test&#34;)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, x, y) â€‘>Â Dict[str,Â torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>TODO: remove the "x" from common method self.step() outputs &amp; benchmark reduction in GPU memory leaks</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, x, y) -&gt; Dict[str, torch.Tensor]:
        &#34;&#34;&#34;
        TODO: remove the &#34;x&#34; from common method self.step() outputs &amp; benchmark reduction in GPU memory leaks
        &#34;&#34;&#34;
        # if self.self_supervised:
        #       z1, z2 = self.shared_step(x)
        #       loss = self.loss(z1, z2)
        # else:
        logits = self(x)
                        
        loss = self.loss(logits, y)
        
        # pp(f&#34;logits.shape: {logits.shape}, y.shape: {y.shape}, loss.shape: {loss.shape}&#34;)
        # pp(f&#34;logits.min(): {logits.min()}, logits.max(): {logits.max()}, logits.mean(axis=-1): {logits.mean(axis=-1)}, logits.std(axis=-1): {logits.std(axis=-1)}&#34;)
        # _y = y.type(torch.cuda.FloatTensor)
        # pp(f&#34;y.min(): {_y.min()}, y.max(): {_y.max()}, y.mean(axis=-1): {_y.mean(axis=-1)}, y.std(axis=-1): {_y.std(axis=-1)}&#34;)
        # pp(f&#34;loss.min(): {loss.min()}, loss.max(): {loss.max()}, loss.mean(axis=-1): {loss.mean(axis=-1)}, loss.std(axis=-1): {loss.std(axis=-1)}&#34;)

        return {&#34;logits&#34;: logits, &#34;loss&#34;: loss, &#34;y&#34;: y, &#34;x&#34;: x}</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.test_epoch_end"><code class="name flex">
<span>def <span class="ident">test_epoch_end</span></span>(<span>self, outputs:Â List[Any]) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of a test epoch with the output of all test steps.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
test_outs = []
for test_batch in test_data:
    out = test_step(test_batch)
    test_outs.append(out)
test_epoch_end(test_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>outputs</code></strong></dt>
<dd>List of outputs you defined in :meth:<code>test_step_end</code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</dd>
</dl>
<h2 id="return">Return</h2>
<p>None</p>
<h2 id="note">Note</h2>
<p>If you didn't define a :meth:<code>test_step</code>, this won't be called.</p>
<h2 id="examples">Examples</h2>
<p>With a single dataloader:</p>
<p>.. code-block:: python</p>
<pre><code>def test_epoch_end(self, outputs):
    # do something with the outputs of all test batches
    all_test_preds = test_step_outputs.predictions

    some_result = calc_all_results(all_test_preds)
    self.log(some_result)
</code></pre>
<p>With multiple dataloaders, <code>outputs</code> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<p>.. code-block:: python</p>
<pre><code>def test_epoch_end(self, outputs):
    final_value = 0
    for dataloader_outputs in outputs:
        for test_step_out in dataloader_outputs:
            # do something
            final_value += test_step_out

    self.log("final_metric", final_value)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_epoch_end(self, outputs: List[Any]) -&gt; None:
        if &#34;image&#34; not in outputs:
                # print(f&#34;Skipping test render_image_predictions due to missing &#39;image&#39; key in epoch outputs.&#34;)
                return
        
        self.render_image_predictions(
                outputs=outputs,
                batch_size=self.cfg.data.datamodule.batch_size,
                n_elements_to_log=self.cfg.logging.n_elements_to_log,
                log_name=&#34;test_image_predictions&#34;,
                normalize_visualization=self.cfg.logging.normalize_visualization,
                logger=self.logger,
                global_step=self.global_step,
                commit=False)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.test_step"><code class="name flex">
<span>def <span class="ident">test_step</span></span>(<span>self, batch:Â Any, batch_idx:Â int) â€‘>Â Dict[str,Â torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the test set.
In this step you'd normally generate examples or calculate anything of interest
such as accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
test_outs = []
for test_batch in test_data:
    out = test_step(test_batch)
    test_outs.append(out)
test_epoch_end(test_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):</dt>
<dt>The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</dd>
</dl>
<h2 id="return">Return</h2>
<p>Any of.</p>
<ul>
<li>Any object or value</li>
<li><code>None</code> - Testing will skip to the next batch</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one test dataloader:
def test_step(self, batch, batch_idx):
    ...


# if you have multiple test dataloaders:
def test_step(self, batch, batch_idx, dataloader_idx):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single test dataset
def test_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'test_loss': loss, 'test_acc': test_acc})
</code></pre>
<p>If you pass in multiple test dataloaders, :meth:<code>test_step</code> will have an additional argument.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple test dataloaders
def test_step(self, batch, batch_idx, dataloader_idx):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to test you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>test_step</code> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
        x, y = batch[:2]
        out = self.step(x, y)
        return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.test_step_end"><code class="name flex">
<span>def <span class="ident">test_step_end</span></span>(<span>self, out)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this when testing with dp or ddp2 because :meth:<code>test_step</code> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<h2 id="note">Note</h2>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<pre><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches]
test_step_end(batch_parts_outputs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_parts_outputs</code></strong></dt>
<dd>What you return in :meth:<code>test_step</code> for each batch part.</dd>
</dl>
<h2 id="return">Return</h2>
<p>None or anything</p>
<p>.. code-block:: python</p>
<pre><code># WITHOUT test_step_end
# if used in DP or DDP2, this batch is 1/num_gpus large
def test_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)
    loss = self.softmax(out)
    self.log("test_loss", loss)


# --------------
# with test_step_end to do softmax over the full batch
def test_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    return out


def test_step_end(self, output_results):
    # this out is now the full size of the batch
    all_test_step_outs = output_results.out
    loss = nce_loss(all_test_step_outs)
    self.log("test_loss", loss)
</code></pre>
<p>See Also:
See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_step_end(self, out):
        batch_size=len(out[&#34;y&#34;])
        self.test_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
        self.log_dict(
                {
                        &#34;test_loss&#34;: out[&#34;loss&#34;].mean(),
                        **self.test_metric,
                },
                batch_size=batch_size
        )
        return {
                # &#34;image&#34;: out[&#34;x&#34;],
                &#34;y_true&#34;: out[&#34;y&#34;],
                &#34;logits&#34;: out[&#34;logits&#34;],
                &#34;val_loss&#34;: out[&#34;loss&#34;].mean(),
        }</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.training_epoch_end"><code class="name flex">
<span>def <span class="ident">training_epoch_end</span></span>(<span>self, outputs:Â List[Any]) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_epoch_end(self, outputs: List[Any]) -&gt; None:
        &#34;&#34;&#34;
        
        &#34;&#34;&#34;
        info = {k: v.shape for k,v in outputs[0].items()}
        # print(&#34;self.training_epoch_end: &#34;, f&#34;device:{torch.cuda.current_device()}, len(outputs)={len(outputs)}, info: {info}&#34;)

        # losses = []
        # for o in outputs:
        #       losses.append(o[&#34;loss&#34;])
        # losses = torch.stack(losses)
        losses = torch.stack([o[&#34;loss&#34;] for o in outputs])
        self.print(f&#34;training_epoch_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, losses: {losses}&#34;)
        # print(f&#34;self.validation_epoch_end (torch.stack the losses): losses.shape = {losses.shape}&#34;)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, batch:Â Any, batch_idx:Â int) â€‘>Â torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<h2 id="args">Args</h2>
<p>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):
The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.
batch_idx (<code>int</code>): Integer displaying index of this batch
optimizer_idx (<code>int</code>): When using multiple optimizers, this argument will also be present.
hiddens (<code>Any</code>): Passed in if
:paramref:<code>~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps</code> &gt; 0.</p>
<h2 id="return">Return</h2>
<p>Any of.</p>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code></li>
<li><code>None</code> - Training will skip to the next batch. This is only for automatic optimization.
This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>If you define multiple optimizers, this step will be called with an additional
<code>optimizer_idx</code> parameter.</p>
<p>.. code-block:: python</p>
<pre><code># Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx, optimizer_idx):
    if optimizer_idx == 0:
        # do training_step with encoder
        ...
    if optimizer_idx == 1:
        # do training_step with decoder
        ...
</code></pre>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<p>.. code-block:: python</p>
<pre><code># Truncated back-propagation through time
def training_step(self, batch, batch_idx, hiddens):
    # hiddens are the hidden states from the previous truncated backprop step
    out, hiddens = self.lstm(data, hiddens)
    loss = ...
    return {"loss": loss, "hiddens": hiddens}
</code></pre>
<h2 id="note">Note</h2>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:

        x, y = batch[:2]
        # import pdb; pdb.set_trace()
        out = self.step(x, y)
        
        # print(&#34;self.training_step: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
        return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.training_step_end"><code class="name flex">
<span>def <span class="ident">training_step_end</span></span>(<span>self, out)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this when training with dp or ddp2 because :meth:<code>training_step</code> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<h2 id="note">Note</h2>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don't have to change your code</p>
<p>.. code-block:: python</p>
<pre><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]
training_step_end(batch_parts_outputs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_parts_outputs</code></strong></dt>
<dd>What you return in <code>training_step</code> for each batch part.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Anything</p>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<p>.. code-block:: python</p>
<pre><code>def training_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)

    # softmax uses only a portion of the batch in the denominator
    loss = self.softmax(out)
    loss = nce_loss(loss)
    return loss
</code></pre>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<p>.. code-block:: python</p>
<pre><code>def training_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    return {"pred": out}


def training_step_end(self, training_step_outputs):
    gpu_0_pred = training_step_outputs[0]["pred"]
    gpu_1_pred = training_step_outputs[1]["pred"]
    gpu_n_pred = training_step_outputs[n]["pred"]

    # this softmax now uses the full batch
    loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred])
    return loss
</code></pre>
<p>See Also:
See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step_end(self, out):
        # print(&#34;self.training_step_end: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
        self.train_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
        
        batch_size=self.batch_size #len(out[&#34;y&#34;])
        loss = out[&#34;loss&#34;].mean()
        log_dict = {
                &#34;train_loss&#34;: loss,
                **self.train_metric
        }
        self.log_dict(
                log_dict,
                on_step=True,
                on_epoch=True,
                prog_bar=True,
                batch_size=batch_size
        )
        self.print(f&#34;training_step_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, loss: {loss}&#34;)
        return loss</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.validation_epoch_end"><code class="name flex">
<span>def <span class="ident">validation_epoch_end</span></span>(<span>self, outputs:Â List[Any]) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
        &#34;&#34;&#34;
        
        &#34;&#34;&#34;
        info = {k: v.shape for k,v in outputs[0].items()}
        print(&#34;self.validation_epoch_end: &#34;, f&#34;device:{torch.cuda.current_device()}, len(outputs)={len(outputs)}, info: {info}&#34;)
        
        # losses = torch.stack([o[&#34;loss&#34;] for o in outputs])
        losses = []
        y = []
        logits = []
        for o in outputs:
                losses.append(o[&#34;loss&#34;])
                y.append(o[&#34;y&#34;])
                logits = [o[&#34;logits&#34;]]
        losses = torch.stack(losses)
        y = torch.cat(y)
        logits = torch.cat(logits)
        
        self.print(f&#34;validation_epoch_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, losses: {losses}&#34;)
        # print(f&#34;self.validation_epoch_end (torch.stack the losses): losses.shape = {losses.shape}, y.shape = {y.shape}, logits.shape = {logits.shape}&#34;)
        
        if &#34;image&#34; not in outputs:
                # print(f&#34;Skipping val render_image_predictions due to missing &#39;image&#39; key in epoch outputs.&#34;)
                return
        self.render_image_predictions(
                outputs=outputs,
                batch_size=self.cfg.data.datamodule.batch_size,
                n_elements_to_log=self.cfg.logging.n_elements_to_log,
                log_name=&#34;val_image_predictions&#34;,
                normalize_visualization=self.cfg.logging.normalize_visualization,
                logger=self.logger,
                global_step=self.global_step,
                commit=False)</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, batch:Â Any, batch_idx:Â int) â€‘>Â Dict[str,Â torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set.
In this step you'd might generate examples or calculate anything of interest like accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    val_outs.append(out)
validation_epoch_end(val_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):</dt>
<dt>The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of this batch</dd>
<dt><strong><code>dataloader_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the dataloader that produced this batch
(only if multiple val dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>Any object or value</li>
<li><code>None</code> - Validation will skip to the next batch</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># pseudocode of order
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    if defined("validation_step_end"):
        out = validation_step_end(out)
    val_outs.append(out)
val_outs = validation_epoch_end(val_outs)
</code></pre>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, batch: Any, batch_idx: int) -&gt; Dict[str, torch.Tensor]:
        x, y = batch[:2]
        out = self.step(x, y)
        # self.
        # print(&#34;self.validation_step: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
        return {k: out[k] for k in [&#34;logits&#34;, &#34;loss&#34;, &#34;y&#34;]}</code></pre>
</details>
</dd>
<dt id="imutils.ml.models.pl.classifier.LitClassifier.validation_step_end"><code class="name flex">
<span>def <span class="ident">validation_step_end</span></span>(<span>self, out)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this when validating with dp or ddp2 because :meth:<code>validation_step</code> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<h2 id="note">Note</h2>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<pre><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches]
validation_step_end(batch_parts_outputs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_parts_outputs</code></strong></dt>
<dd>What you return in :meth:<code>validation_step</code>
for each batch part.</dd>
</dl>
<h2 id="return">Return</h2>
<p>None or anything</p>
<p>.. code-block:: python</p>
<pre><code># WITHOUT validation_step_end
# if used in DP or DDP2, this batch is 1/num_gpus large
def validation_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    loss = self.softmax(out)
    loss = nce_loss(loss)
    self.log("val_loss", loss)


# --------------
# with validation_step_end to do softmax over the full batch
def validation_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)
    return out


def validation_step_end(self, val_step_outputs):
    for out in val_step_outputs:
        ...
</code></pre>
<p>See Also:
See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step_end(self, out):
        # self.
        # print(&#34;self.validation_step_end: &#34;, f&#34;device:{torch.cuda.current_device()}, y.shape:{out[&#39;y&#39;].shape}, logits.shape:{out[&#39;logits&#39;].shape}, loss.shape:{out[&#39;loss&#39;].shape}&#34;)
        self.val_metric(out[&#34;logits&#34;], out[&#34;y&#34;])
        batch_size=self.batch_size #len(out[&#34;y&#34;])
        loss = out[&#34;loss&#34;].mean()
        
        log_dict = {
                &#34;val_loss&#34;: loss,
                **self.val_metric
        }
        # if &#34;val/F1_top1&#34; in self.val_metric.keys():
        #       log_dict[&#34;val_F1&#34;] = self.val_metric[&#34;val/F1_top1&#34;]
        #       log_dict.update({k:v for k,v in self.val_metric.items() if k != &#34;val/F1_top1&#34;})
        # elif &#34;val_macro_F1&#34; in self.val_metric.keys():
        #       log_dict[&#34;val_F1&#34;] = self.val_metric[&#34;val_macro_F1&#34;]
        #       log_dict.update({k:v for k,v in self.val_metric.items() if k != &#34;val_macro_F1&#34;})
        # else:
        # log_dict.update(self.val_metric)
        # self.log(&#34;val_macro_F1&#34;, self.val_metric[&#34;val_macro_F1&#34;])
        self.log_dict(log_dict,
                                  on_step=True, # False, #
                                  on_epoch=True,
                                  prog_bar=True,
                                  batch_size=batch_size)
        self.print(f&#34;validation_step_end -&gt; self.current_epoch: {self.current_epoch}, self.global_step: {self.global_step}, loss: {loss}&#34;)

        # self.log_dict(self.val_metric,
        #                         on_step=True,
        #                         on_epoch=True,
        #                         # prog_bar=True,
        #                         batch_size=batch_size)
        
        # return {
        #       # &#34;image&#34;: out[&#34;x&#34;],
        #       &#34;y_true&#34;: out[&#34;y&#34;],
        #       &#34;logits&#34;: out[&#34;logits&#34;],
        #       &#34;val_loss&#34;: loss,
        # }</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="imutils.ml.models.base.BaseLightningModule" href="../base.html#imutils.ml.models.base.BaseLightningModule">BaseLightningModule</a></b></code>:
<ul class="hlist">
<li><code><a title="imutils.ml.models.base.BaseLightningModule.forward" href="../base.html#imutils.ml.models.base.BaseLightningModule.forward">forward</a></code></li>
<li><code><a title="imutils.ml.models.base.BaseLightningModule.on_train_epoch_start" href="../base.html#imutils.ml.models.base.BaseLightningModule.on_train_epoch_start">on_train_epoch_start</a></code></li>
<li><code><a title="imutils.ml.models.base.BaseLightningModule.on_train_start" href="../base.html#imutils.ml.models.base.BaseLightningModule.on_train_start">on_train_start</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils.ml.models.pl" href="index.html">imutils.ml.models.pl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imutils.ml.models.pl.classifier.LitClassifier" href="#imutils.ml.models.pl.classifier.LitClassifier">LitClassifier</a></code></h4>
<ul class="">
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.configure_optimizers" href="#imutils.ml.models.pl.classifier.LitClassifier.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.dump_patches" href="#imutils.ml.models.pl.classifier.LitClassifier.dump_patches">dump_patches</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.exclude_from_wt_decay" href="#imutils.ml.models.pl.classifier.LitClassifier.exclude_from_wt_decay">exclude_from_wt_decay</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.predict_step" href="#imutils.ml.models.pl.classifier.LitClassifier.predict_step">predict_step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.render_image_predictions" href="#imutils.ml.models.pl.classifier.LitClassifier.render_image_predictions">render_image_predictions</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.setup_loss" href="#imutils.ml.models.pl.classifier.LitClassifier.setup_loss">setup_loss</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.setup_metrics" href="#imutils.ml.models.pl.classifier.LitClassifier.setup_metrics">setup_metrics</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.step" href="#imutils.ml.models.pl.classifier.LitClassifier.step">step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.test_epoch_end" href="#imutils.ml.models.pl.classifier.LitClassifier.test_epoch_end">test_epoch_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.test_step" href="#imutils.ml.models.pl.classifier.LitClassifier.test_step">test_step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.test_step_end" href="#imutils.ml.models.pl.classifier.LitClassifier.test_step_end">test_step_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.training" href="#imutils.ml.models.pl.classifier.LitClassifier.training">training</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.training_epoch_end" href="#imutils.ml.models.pl.classifier.LitClassifier.training_epoch_end">training_epoch_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.training_step" href="#imutils.ml.models.pl.classifier.LitClassifier.training_step">training_step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.training_step_end" href="#imutils.ml.models.pl.classifier.LitClassifier.training_step_end">training_step_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.validation_epoch_end" href="#imutils.ml.models.pl.classifier.LitClassifier.validation_epoch_end">validation_epoch_end</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.validation_step" href="#imutils.ml.models.pl.classifier.LitClassifier.validation_step">validation_step</a></code></li>
<li><code><a title="imutils.ml.models.pl.classifier.LitClassifier.validation_step_end" href="#imutils.ml.models.pl.classifier.LitClassifier.validation_step_end">validation_step_end</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>