<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.ml.pretrain.lr_tuner API documentation</title>
<meta name="description" content="image-utils/imutils/ml/pretrain/lr_tuner.py â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.ml.pretrain.lr_tuner</code></h1>
</header>
<section id="section-intro">
<p>image-utils/imutils/ml/pretrain/lr_tuner.py</p>
<p>originally located at:
lightning_hydra_classifiers/scripts/pretrain/lr_tuner.py</p>
<p>Created on: Friday Sept 3rd, 2021
Moved on: Monday March 28th, 2022
Author: Jacob A Rose</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;

image-utils/imutils/ml/pretrain/lr_tuner.py

originally located at:
        lightning_hydra_classifiers/scripts/pretrain/lr_tuner.py


Created on: Friday Sept 3rd, 2021
Moved on: Monday March 28th, 2022
Author: Jacob A Rose


&#34;&#34;&#34;


import pytorch_lightning as pl
import argparse
from omegaconf import DictConfig, OmegaConf
import os
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import wandb
from rich import print as pp
from typing import *


from imutils.ml.utils.etl_utils import ETL
from imutils.ml.utils import template_utils
############################################
logger = template_utils.get_logger(name=__name__)


__all__ = [&#34;run&#34;]



from dataclasses import dataclass, asdict
import hydra
from pytorch_lightning.utilities import rank_zero_only

@dataclass
class LRTunerConfig:

        min_lr: float = 1e-08
        max_lr: float = 1.0
        num_training: int = 100
        mode: str = &#39;exponential&#39;
        early_stop_threshold: float = 4.0

DEFAULT_CONFIG = OmegaConf.structured(LRTunerConfig())


# @rank_zero_only
def run(cfg: DictConfig,
                datamodule=None,
                model=None) -&gt; DictConfig:
        &#34;&#34;&#34;
        WIP implementation that encloses trainer instantiation within the pretrain stage to ensure proper release of memory prior to multi-GPU training.
        
        - TODO: Figure out heuristic for scaling lr found using lr_tuner from 1-GPU to multi-GPUs
        
        
        ToDO: Consider how to override the optimizer scheduler temporarily.
        
        &#34;&#34;&#34;
        import pytorch_lightning as pl
        import imutils
        import imutils.ml.models.pl.classifier
        use_lr_scheduler = cfg.optim.use_lr_scheduler
        trainer_args = argparse.Namespace(**cfg.pretrain.lr_tuner.pl_trainer)
        if trainer_args.auto_lr_find is False:
                hydra.utils.log.info(&#34;Skipping pretrain.lr_tuner stage b/c trainer_args.auto_lr_find==False&#34;)
                return cfg

        import torch

        hparams_path = cfg.pretrain.lr_tuner.get(&#34;hparams_path&#34;, &#34;lr_tuner_hparams.yaml&#34;)
        if os.path.isfile(hparams_path):
                best_hparams = ETL.config_from_yaml(hparams_path)
                best_lr = best_hparams[&#39;lr&#39;]
                
                hydra.utils.log.info(f&#34;device:{torch.cuda.current_device()}&#34;)
                hydra.utils.log.info(f&#34;Loaded best lr_tuner hparams: {best_hparams} from file: {hparams_path}&#34;)
                cfg = _update_cfg_lr(cfg,
                                                         lr=best_lr,
                                                         use_lr_scheduler=use_lr_scheduler)
                return cfg

        if datamodule is None:
                hydra.utils.log.info(f&#34;Instantiating &lt;{cfg.data.datamodule._target_}&gt;&#34;)
                datamodule: pl.LightningDataModule = hydra.utils.instantiate(
                        cfg.data.datamodule, _recursive_=False
                )
                datamodule.setup()

        from imutils.ml.utils.experiment_utils import configure_loss_func

        loss_func = configure_loss_func(cfg, targets=datamodule.train_dataset.df.y)

        if model is None:
                hydra.utils.log.info(f&#34;Instantiating &lt;{cfg.model_cfg._target_}&gt; prior to pretrain.lr_tuner&#34;)
                # model: pl.LightningModule = hydra.utils.instantiate(cfg.model, cfg=cfg, _recursive_=False)
                model = imutils.ml.models.pl.classifier.LitClassifier(cfg=cfg, #model_cfg=cfg.model_cfg,
                                                                                                                          loss_func=loss_func)

        hydra.utils.log.info(&#34;INITIATING STAGE: pretrain.lr_finder using cfg settings:&#34;)
        template_utils.print_config(cfg, fields=[&#34;pretrain&#34;, &#34;model_cfg&#34;, &#34;optim&#34;])
        trainer = pl.Trainer.from_argparse_args(args=trainer_args)
        
        ## lr_tuner execution
        tuner_args = OmegaConf.to_container(cfg.pretrain.lr_tuner.tuner, resolve=True)

        lr_finder = trainer.tuner.lr_find(model, datamodule=datamodule, **tuner_args)
        new_lr = lr_finder.suggestion()
        
        if cfg.train.pl_trainer.devices &gt; 1:
                num_gpus = cfg.train.pl_trainer.devices
                hydra.utils.log.info(f&#34;NOTICE: Since devices={num_gpus}, scaling new lr by the same amount [EXPERIMENTAL].&#34;)
                hydra.utils.log.info(f&#34;BEFORE: {new_lr}&#34;)
                new_lr = new_lr*num_gpus
                hydra.utils.log.info(f&#34;AFTER: {new_lr}&#34;)
        
        cfg = _update_cfg_lr(cfg,
                                                 lr=new_lr,
                                                 use_lr_scheduler=use_lr_scheduler)
        best_hparams = {&#34;lr&#34;:new_lr}
        os.makedirs(os.path.dirname(hparams_path), exist_ok=True)
        ETL.config2yaml(best_hparams, hparams_path)
        
        hydra.utils.log.info(f&#34;device:{torch.cuda.current_device()}&#34;)
        hydra.utils.log.info(f&#34;Saved best lr_tuner hparams: {best_hparams} to file: {hparams_path}&#34;)
        print(f&#34;SUCCESS: Updated config:&#34;)
        print(&#34;\n&#34;.join([&#34;=&#34; * 80, f&#34;Learning rate updated to {new_lr}&#34;,&#34;=&#34; * 80]))
        template_utils.print_config(cfg, fields=[&#34;pretrain&#34;, &#34;model_cfg&#34;, &#34;optim&#34;])
        

        return cfg


def _update_cfg_lr(cfg,
                                   lr: float,
                                   use_lr_scheduler: bool=False):
        cfg.optim.use_lr_scheduler = use_lr_scheduler
        # if cfg.optim.lr_scheduler is not None:
        if cfg.optim.use_lr_scheduler:
                # cfg.optim.use_lr_scheduler = True
                cfg.optim.lr_scheduler.warmup_start_lr = lr
                hydra.utils.log.info(&#34;[MODE 1] UPDATING WARMUP STARTING LEARNING RATE W/ SCHEDULE&#34;)
                hydra.utils.log.info(&#34;RESULTS of lr_tuner stage -- Using an lr_scheduler --&gt;\n&#34; \
                                                         + f&#34; Updated cfg.optim.lr_scheduler.warmup_start_lr={cfg.optim.lr_scheduler.warmup_start_lr}&#34;)
        else:
                cfg.hp.lr = lr
                cfg.optim.optimizer.lr = lr
                # model.lr = new_lr
                hydra.utils.log.info(&#34;[MODE 2] UPDATING BASE LEARNING RATE W/O SCHEDULE&#34;)
                hydra.utils.log.info(&#34;RESULTS of lr_tuner stage -- Not using an lr_scheduler --&gt;\n&#34; \
                                                         + f&#34; Updated cfg.optim.optimizer.lr={cfg.optim.optimizer.lr}&#34;)

        return cfg

# def run_lr_tuner(trainer: pl.Trainer,
#                                 model: pl.LightningModule,
#                                 datamodule: pl.LightningDataModule,
#                                 config: argparse.Namespace,
#                                 results_dir: str,
#                                 group: str=None,
#                                 run: Optional=None):
#                                 # strict_resume: bool=False):
# #                               run=None):
#        &#34;&#34;&#34;
#        Learning rate tuner
        
#        Adapted and refactored from &#34;lightning-hydra-classifiers/lightning_hydra_classifiers/scripts/train_basic.py&#34;
#        &#34;&#34;&#34;
#        tuner_config = OmegaConf.create(DEFAULT_CONFIG)

#        try:
#                cfg = asdict(config)
#        except TypeError:
#                cfg = OmegaConf.to_container(config, resolve=True)
#        finally:
#                cfg = dict(config)
        
#        if &#34;pretrain&#34; in cfg:
#                logger.info(f&#34;Proceeding with overrides merged with default parameters&#34;)
# #              logger.info(f&#34;overrides: {config.lr_tuner}&#34;)
# #              logger.info(f&#34;defaults: {tuner_config}&#34;)
#                tuner_config = OmegaConf.merge(DEFAULT_CONFIG, cfg[&#34;pretrain&#34;])
#        else:
#                for k, v in DEFAULT_CONFIG.items():
#                        if k in cfg:
#                                tuner_config.update({k:config[k]})

#                config.pretrain = OmegaConf.create(tuner_config)


#        results_path = str(Path(results_dir, &#34;results.csv&#34;))
#        hparams_path = str(Path(results_dir, &#34;hparams.yaml&#34;))
#        if os.path.isfile(hparams_path):
                
#                best_hparams = ETL.config_from_yaml(hparams_path)
#                results = None
#                if os.path.isfile(results_path):
#                        results = ETL.df_from_csv(results_path)
                
#                best_lr = best_hparams[&#39;lr&#39;]
#                if hasattr(model, &#34;config&#34;):
#                        model.config.lr = best_lr


#                model.hparams.lr = best_lr
#                config.model.lr = best_lr
# #              config.model.optimizer.lr = model.config.lr
                
#                assert config.model.lr == best_lr

#                logger.info(f&#39;[FOUND] Previously completed trial. Results located in file:\n`{results_path}`&#39;)
#                logger.info(f&#39;[LOADING] Previous results + avoiding repetition of tuning procedure.&#39;)
#                logger.info(f&#39;Proceeding with learning rate, lr = {config.model.lr:.3e}&#39;)
#                logger.info(&#39;Model hparams =&#39;)
#                pp(best_hparams)
#                suggestion = {&#34;lr&#34;: config.model.lr,
#                                          &#34;loss&#34;: None}
#                return suggestion, results, config
        
#        if run is None:
#                run = wandb.init(job_type = &#34;lr_tune&#34;,
#                                                 config=cfg,
#                                                 group=group,
#                                                 reinit=True)
#                logger.info(f&#34;[Initiating Stage] lr_tuner&#34;)
#                lr_tuner = trainer.tuner.lr_find(model,
#                                                                                 datamodule,
#                                                                                 **cfg.get(&#34;pretrain&#34;, {}))
#                lr_tuner_results = lr_tuner.results
#                best_lr = lr_tuner.suggestion()
#                suggestion = {&#34;lr&#34;: best_lr,
#                                          &#34;loss&#34;:lr_tuner_results[&#39;loss&#39;][lr_tuner._optimal_idx]}
                
#                if hasattr(model, &#34;config&#34;):
#                        model.config.lr = suggestion[&#39;lr&#39;]
#                model.hparams.lr = suggestion[&#39;lr&#39;]
#                config.model.lr = model.hparams.lr
# #              config.model.optimizer.lr = model.hparams.lr
#                model.hparams.update(config.model)
#                best_hparams = OmegaConf.create({&#34;optimized_hparam_key&#34;: &#34;lr&#34;,
#                                                                                 &#34;lr&#34;:best_lr,
#                                                                                 &#34;batch_size&#34;:config.data.batch_size,
#                                                                                 &#34;image_size&#34;:config.data.image_size,
#                                                                                 &#34;lr_tuner_config&#34;:config.pretrain}) #.lr_tuner})
#                results_dir = Path(results_path).parent
#                os.makedirs(results_dir, exist_ok=True)
#                ETL.config2yaml(best_hparams, hparams_path)
#                logger.info(f&#39;Saved best lr value (along w/ batch_size, image_size) to file located at: {str(hparams_path)}&#39;) # {str(results_dir / &#34;hparams.yaml&#34;)}&#39;)
#                logger.info(f&#39;File contents expected to contain: \n{dict(best_hparams)}&#39;)      

#                fig = lr_tuner.plot(suggest=True)
#                plot_fname = &#39;lr_tuner_results_loss-vs-lr.png&#39;
#                plot_path = results_dir / plot_fname

#                plt.suptitle(f&#34;Suggested lr={best_lr:.4e} |\n| Searched {lr_tuner.num_training} lr values $\in$ [{lr_tuner.lr_min},{lr_tuner.lr_max}] |\n| bsz = {config.data.batch_size}&#34;, fontsize=&#39;small&#39;)
#                plt.tight_layout()
#                plt.subplots_adjust(bottom=0.2, top=0.8)
#                plt.savefig(plot_path)
                
#                if run is not None:
#        #               run.summary[&#39;lr_finder/plot&#39;] = wandb.Image(fig, caption=plot_fname)
#                        run.log({&#39;lr_finder/plot&#39;: wandb.Image(str(plot_path), caption=plot_fname)})
#                        run.log({&#39;lr_finder/best/loss&#39;: suggestion[&#34;loss&#34;]})
#                        run.log({&#39;lr_finder/best/lr&#39;: suggestion[&#34;lr&#34;]})
#                        run.log({&#39;lr_finder/batch_size&#39;: config.data.batch_size})
#                        run.log({&#39;image_size&#39;: config.data.image_size})
#                        run.log({&#39;lr_finder/hparams&#39;: OmegaConf.to_container(best_hparams)})
                        
#                        df = pd.DataFrame(lr_tuner.results)
#                        try:
#                                ETL.df2csv(df, results_path)
#                                run.log({&#34;lr_finder/results&#34;:wandb.Table(dataframe=df)})
#                        except Exception as e:
#                                if hasattr(df, &#34;to_pandas&#34;):
#                                        run.log({&#34;lr_finder/results&#34;:wandb.Table(dataframe=df.to_pandas())})

#        logger.info(f&#39;FINISHED: `run_lr_tuner(config)`&#39;)
#        logger.info(f&#39;Proceeding with:\n&#39;)
#        logger.info(f&#39;Learning rate = {config.model.lr:.3e}&#39;)
#        logger.info(f&#39;Batch size = {config.data.batch_size}&#39;)
        
#        return suggestion, lr_tuner_results, config</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imutils.ml.pretrain.lr_tuner.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>cfg:Â omegaconf.dictconfig.DictConfig, datamodule=None, model=None) â€‘>Â omegaconf.dictconfig.DictConfig</span>
</code></dt>
<dd>
<div class="desc"><p>WIP implementation that encloses trainer instantiation within the pretrain stage to ensure proper release of memory prior to multi-GPU training.</p>
<ul>
<li>TODO: Figure out heuristic for scaling lr found using lr_tuner from 1-GPU to multi-GPUs</li>
</ul>
<p>ToDO: Consider how to override the optimizer scheduler temporarily.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(cfg: DictConfig,
                datamodule=None,
                model=None) -&gt; DictConfig:
        &#34;&#34;&#34;
        WIP implementation that encloses trainer instantiation within the pretrain stage to ensure proper release of memory prior to multi-GPU training.
        
        - TODO: Figure out heuristic for scaling lr found using lr_tuner from 1-GPU to multi-GPUs
        
        
        ToDO: Consider how to override the optimizer scheduler temporarily.
        
        &#34;&#34;&#34;
        import pytorch_lightning as pl
        import imutils
        import imutils.ml.models.pl.classifier
        use_lr_scheduler = cfg.optim.use_lr_scheduler
        trainer_args = argparse.Namespace(**cfg.pretrain.lr_tuner.pl_trainer)
        if trainer_args.auto_lr_find is False:
                hydra.utils.log.info(&#34;Skipping pretrain.lr_tuner stage b/c trainer_args.auto_lr_find==False&#34;)
                return cfg

        import torch

        hparams_path = cfg.pretrain.lr_tuner.get(&#34;hparams_path&#34;, &#34;lr_tuner_hparams.yaml&#34;)
        if os.path.isfile(hparams_path):
                best_hparams = ETL.config_from_yaml(hparams_path)
                best_lr = best_hparams[&#39;lr&#39;]
                
                hydra.utils.log.info(f&#34;device:{torch.cuda.current_device()}&#34;)
                hydra.utils.log.info(f&#34;Loaded best lr_tuner hparams: {best_hparams} from file: {hparams_path}&#34;)
                cfg = _update_cfg_lr(cfg,
                                                         lr=best_lr,
                                                         use_lr_scheduler=use_lr_scheduler)
                return cfg

        if datamodule is None:
                hydra.utils.log.info(f&#34;Instantiating &lt;{cfg.data.datamodule._target_}&gt;&#34;)
                datamodule: pl.LightningDataModule = hydra.utils.instantiate(
                        cfg.data.datamodule, _recursive_=False
                )
                datamodule.setup()

        from imutils.ml.utils.experiment_utils import configure_loss_func

        loss_func = configure_loss_func(cfg, targets=datamodule.train_dataset.df.y)

        if model is None:
                hydra.utils.log.info(f&#34;Instantiating &lt;{cfg.model_cfg._target_}&gt; prior to pretrain.lr_tuner&#34;)
                # model: pl.LightningModule = hydra.utils.instantiate(cfg.model, cfg=cfg, _recursive_=False)
                model = imutils.ml.models.pl.classifier.LitClassifier(cfg=cfg, #model_cfg=cfg.model_cfg,
                                                                                                                          loss_func=loss_func)

        hydra.utils.log.info(&#34;INITIATING STAGE: pretrain.lr_finder using cfg settings:&#34;)
        template_utils.print_config(cfg, fields=[&#34;pretrain&#34;, &#34;model_cfg&#34;, &#34;optim&#34;])
        trainer = pl.Trainer.from_argparse_args(args=trainer_args)
        
        ## lr_tuner execution
        tuner_args = OmegaConf.to_container(cfg.pretrain.lr_tuner.tuner, resolve=True)

        lr_finder = trainer.tuner.lr_find(model, datamodule=datamodule, **tuner_args)
        new_lr = lr_finder.suggestion()
        
        if cfg.train.pl_trainer.devices &gt; 1:
                num_gpus = cfg.train.pl_trainer.devices
                hydra.utils.log.info(f&#34;NOTICE: Since devices={num_gpus}, scaling new lr by the same amount [EXPERIMENTAL].&#34;)
                hydra.utils.log.info(f&#34;BEFORE: {new_lr}&#34;)
                new_lr = new_lr*num_gpus
                hydra.utils.log.info(f&#34;AFTER: {new_lr}&#34;)
        
        cfg = _update_cfg_lr(cfg,
                                                 lr=new_lr,
                                                 use_lr_scheduler=use_lr_scheduler)
        best_hparams = {&#34;lr&#34;:new_lr}
        os.makedirs(os.path.dirname(hparams_path), exist_ok=True)
        ETL.config2yaml(best_hparams, hparams_path)
        
        hydra.utils.log.info(f&#34;device:{torch.cuda.current_device()}&#34;)
        hydra.utils.log.info(f&#34;Saved best lr_tuner hparams: {best_hparams} to file: {hparams_path}&#34;)
        print(f&#34;SUCCESS: Updated config:&#34;)
        print(&#34;\n&#34;.join([&#34;=&#34; * 80, f&#34;Learning rate updated to {new_lr}&#34;,&#34;=&#34; * 80]))
        template_utils.print_config(cfg, fields=[&#34;pretrain&#34;, &#34;model_cfg&#34;, &#34;optim&#34;])
        

        return cfg</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils.ml.pretrain" href="index.html">imutils.ml.pretrain</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imutils.ml.pretrain.lr_tuner.run" href="#imutils.ml.pretrain.lr_tuner.run">run</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>