<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imutils.ml.utils.label_utils API documentation</title>
<meta name="description" content="imutils/ml/utils/label_utils.py
Created on: Thursday, March 25th, 2022
Author: Jacob A Rose â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imutils.ml.utils.label_utils</code></h1>
</header>
<section id="section-intro">
<p>imutils/ml/utils/label_utils.py
Created on: Thursday, March 25th, 2022
Author: Jacob A Rose</p>
<p>Description: Moved LabelEncoder class to imutils and refactored to be more closely tied to sklearn's LabelEncoder class.</p>
<p>Original location:
lightning_hydra_classifiers/utils/common_utils.py
Created on: Wednesday, July 14th, 2021
Author: Jacob A Rose</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
imutils/ml/utils/label_utils.py
Created on: Thursday, March 25th, 2022
Author: Jacob A Rose

Description: Moved LabelEncoder class to imutils and refactored to be more closely tied to sklearn&#39;s LabelEncoder class.

Original location:
        lightning_hydra_classifiers/utils/common_utils.py
        Created on: Wednesday, July 14th, 2021
        Author: Jacob A Rose


&#34;&#34;&#34;

import os
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import numbers
from typing import Union, List, Any, Tuple, Dict, Optional, Sequence
import collections
import seaborn as sns
from sklearn.model_selection import train_test_split
import json
from sklearn import preprocessing


from imutils.ml.utils.toolbox.nn import functional as BF
from imutils.ml.utils.toolbox.nn.loss import LabelSmoothingLoss


from imutils.ml.utils import template_utils
logging = template_utils.get_logger(__name__)




__all__ = [&#34;LabelEncoder&#34;] #, &#34;trainval_split&#34;, &#34;trainvaltest_split&#34;, &#34;DataSplitter&#34;,
                   # &#34;plot_split_distributions&#34;, &#34;plot_class_distributions&#34;, &#34;filter_df_by_threshold&#34;,
                   # &#34;compute_class_counts&#34;, &#34;Batch&#34;]

from collections import namedtuple
Batch = namedtuple(&#34;Batch&#34;, (&#34;image&#34;, &#34;target&#34;, &#34;metadata&#34;))


class LabelEncoder(object):
        &#34;&#34;&#34;
        Label encoder for tag labels.
        
        len(idx2class) &lt;= len(class2idx)
        num_classes == len(idx2class) &lt;= len(class2idx)
        &#34;&#34;&#34;
        def __init__(self,
                                 class2idx: Dict[str,int]=None,
                                 replacements: Optional[Dict[str,str]]=None):
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                

                self.class2idx = class2idx or {}
                self.replacements = replacements or {}
                assert len(self.classes) == len(self.idx2class) &lt;= len(self.class2idx)
                self.num_samples = 0
                self.verbose=False

        def copy(self, encoder: &#34;LabelEncoder&#34;) -&gt; &#34;LabelEncoder&#34;:
                &#34;&#34;&#34;
                Instantiate a new LabelEncoder instance containing all attributes of this one.
                &#34;&#34;&#34;
                return LabelEncoder(class2idx=self.class2idx,
                                                        replacements=self.replacements)


        @classmethod
        def from_sklearn(cls,
                                         encoder: preprocessing.LabelEncoder=None,
                                         replacements: Optional[Dict[str,str]]=None):
                &#34;&#34;&#34;
                Returns a new instance of this custom LabelEncoder from a fitted sklearn LabelEncoder. Useful for interoperability.
                &#34;&#34;&#34;
                
                class_list = getattr(encoder, &#34;classes_&#34;, [])
                class2idx = {label: idx for idx, label in enumerate(class_list)}
                
                custom_encoder = cls(class2idx=class2idx,
                                                         replacements=replacements)
                
                return custom_encoder


        def to_sklearn(self) -&gt; preprocessing.LabelEncoder:
                &#34;&#34;&#34;
                Returns an sklearn LabelEncoder after fitting it to any existing labels contained in this custom Label Encoder. Useful for interoperability.
                &#34;&#34;&#34;
                sklearn_encoder = preprocessing.LabelEncoder()
                sklearn_encoder.fit(self.classes)
                
                return sklearn_encoder


        @property
        def idx2class(self) -&gt; Dict[Any, Any]:
                return {v: k for k, v in self.class2idx.items() if k not in self.replacements.keys()}
        
        @property
        def classes(self) -&gt; List[Any]:
                return [k for k in self.class2idx.keys() if k not in self.replacements.keys()]

        def __len__(self):
                return len(self.idx2class)

        @property
        def num_classes(self) -&gt; int:
                return len(self)

        def fit(self, y):
                &#34;&#34;&#34;
                In the case that this label encoder already has 1+ labels, this assigns any new classes integers above any already used.
                In the case that this has never been used, it collects any new labels and assigns them inteegers starting with 0.
                In both cases, if a label exists in the keys of self.replacements, then they will be replaced by the corresponding value before being considered for use in the encoder.
                
                Arguments:
                        y: Sequence
                
                &#34;&#34;&#34;
                
                counts = collections.Counter(y)
                self.num_samples += sum(counts.values())
                
                classes = sorted(list(counts.keys()))
                new_classes = sorted([label for label in classes if (label not in self.classes) and (label not in self.replacements.keys())])
                replace_classes = sorted([label for label in classes if label in self.replacements.keys()])
                
                old_num_classes = len(self)             
                old_highest_class = None
                idx = 0
                if len(self.idx2class) &gt; 0:
                        old_highest_class = max(self.idx2class.keys())
                        idx = old_highest_class + 1
                        
                for label in new_classes:
                        self.class2idx[label] = idx
                        idx += 1
                for label in replace_classes:
                        if self.replacements[label] in self.class2idx:
                                self.class2idx[label] = self.class2idx[self.replacements[label]]
                        else:
                                logging.warning(
                                        f&#34;[Warning]: label {label} marked for replacement, but its replacement label has yet to be assigned an int encoding in class2idx.&#34;
                                )
                        
                new_classes = [c for c in new_classes if c not in self.replacements.keys()]
                if len(new_classes):
                        logging.debug(f&#34;[FITTING] {len(y)} samples with {len(classes)} classes, adding {len(new_classes)} new class labels. Latest num_classes = {len(self)}&#34;)
                assert len(self) == (old_num_classes + len(new_classes)), f&#34;len(self)={len(self)}, (old_num_classes={old_num_classes}, len(new_classes)={len(new_classes)})&#34;
                assert np.all([label in self.idx2class.values() for label in new_classes])
                return self

        def transform(self, y):
                &#34;&#34;&#34;Wrapper for inter-operability with sklearn&#34;&#34;&#34;
                return self.encode(y)
        def inv_transform(self, y):
                &#34;&#34;&#34;Wrapper for inter-operability with sklearn&#34;&#34;&#34;
                return self.decode(y)



        def encode(self, y, strict: bool=True):
                if not hasattr(y,&#34;__len__&#34;):
                        y = [y]
                if strict:
                        return np.array([self.class2idx[label] for label in y])
                return np.array([self.class2idx.get(label, -1) for label in y])

        def decode(self, y, strict: bool=True):
                if not hasattr(y,&#34;__len__&#34;):
                        y = [y]
                if strict:
                        return np.array([self.idx2class[label] for label in y])
                return np.array([self.idx2class.get(label, &#34;unknown&#34;) for label in y])

        def decode_topk(self, y, strict: bool=True):
                &#34;&#34;&#34;
                Provide a 2-D array of integer labels representing top-k predicted classes.
                The top-k for each sample is stored in each column of the corresponding row.
                
                Arguments:
                        y: Union[np.ndarray, List]
                                y can be any of the following:
                                
                                        - List of lists of integer labels (e.g. y=[[0,3,4], 
                                                                                                                           [3,7,5]])
                                        - List of 1-D np.ndarrays of integer labels (e.g. y=[np.ndarray([0,3,4]),
                                                                                                                                                 np.ndarray([3,7,5])]
                                        - List of 2-D np.ndarrays of integer labels, with samples incremented along rows and top-k integer predictions in the columns.
                                        (e.g. y=np.ndarray
                                                        [np.ndarray([0,3,4]),
                                                         np.ndarray([3,7,5])])
                                                                                                                                                 
                        strict: bool=True
                &#34;&#34;&#34;
                
                topk_labels = []
                if isinstance(y, np.ndarray):
                        if y.ndim == 2:
                                topk = y.shape[1]
                        else:
                                topk = 1
                if isinstance(y, list):
                        if isinstance(y[0], np.ndarray):
                                topk = y[0].shape[0]
                        elif isinstance(y[0], list):
                                topk = len(y[0])

                for k in range(topk):
                        # labels_k = datamodule.train_dataset.label_encoder.inverse_transform(y_logits_top5.indices[:,k])
                        labels_k = self.decode(y[:,k], strict=strict)
                        topk_labels.append(labels_k)

                return np.vstack(topk_labels).T


        def save(self, fp):
                with open(fp, &#34;w&#34;) as fp:
                        contents = self.getstate() # {&#34;class2idx&#34;: self.class2idx}
                        json.dump(contents, fp, indent=4, sort_keys=False)

        @classmethod
        def load(cls, fp):
                with open(fp, &#34;r&#34;) as fp:
                        kwargs = json.load(fp=fp)
                return cls(**kwargs)
        
        def getstate(self):
                return {&#34;class2idx&#34;: self.class2idx,
                                &#34;replacements&#34;: self.replacements}

        def calculate_class_counts(y: Sequence) -&gt; Tuple[np.ndarray]:
                &#34;&#34;&#34;
                Returns a Tuple of np.ndarrays, the first has unique classes, the second has class counts.
                &#34;&#34;&#34;
                
                classes, counts = BF.class_counts(y)
                return classes, counts
        
        
        def __repr__(self):
                disp = f&#34;&#34;&#34;&lt;{str(type(self)).strip(&#34;&#39;&gt;&#34;).split(&#39;.&#39;)[-1]}&gt;:\n&#34;&#34;&#34;
                disp += f&#34;      num_classes: {len(self)}\n&#34;
                disp += f&#34;      fit on num_samples: {self.num_samples}&#34;
                return disp

        def __str__(self):
                msg = f&#34;&lt;LabelEncoder(num_classes={len(self)})&gt;&#34;
                if len(self.replacements) &gt; 0:
                        msg += &#34;\n&#34; + f&#34;&lt;num_replaced_classes={len(self.replacements)}&gt;&#34;
                return msg














#################################
#################################
####################################################


def trainval_split(x: Union[List[Any],np.ndarray]=None,
                                   y: Union[List[Any],np.ndarray]=None,
                                   val_train_split: float=0.2,
                                   random_state: int=None,
                                   stratify: bool=True
                                   ) -&gt; Dict[str,Tuple[np.ndarray]]:
        &#34;&#34;&#34;
        Wrapper function to split data into 3 stratified subsets specified by `splits`.
        
        User specifies absolute fraction of total requested for each subset (e.g. splits=[0.5, 0.2, 0.3])
        
        Function calculates adjusted fractions necessary in order to use sklearn&#39;s builtin train_test_split function over a sequence of 2 steps.
        
        Step 1: Separate test set from the rest of the data (constituting the union of train + val)
        
        Step 2: Separate the train and val sets from the remainder produced by step 1.

        Output:
                Dict: {&#39;train&#39;:(x_train, y_train),
                                &#39;val&#39;:(x_val_y_val),
                                &#39;test&#39;:(x_test, y_test)}
                                
        Example:
                &gt;&gt; data = torch.data.Dataset(...)
                &gt;&gt; y = data.targets
                &gt;&gt; data_splits = trainvaltest_split(x=None,
                                                                                        y=y,
                                                                                        splits=(0.5, 0.2, 0.3),
                                                                                        random_state=0,
                                                                                        stratify=True)
        
        &#34;&#34;&#34;
        
        train_split = 1.0 - val_train_split
        
        if stratify and (y is None):
                raise ValueError(&#34;If y is not provided, stratify must be set to False.&#34;)
        
        y = np.array(y)
        if x is None:
                x = np.arange(len(y))
        else:
                x = np.array(x)
        
        stratify_y = y if stratify else None    
        x_train, x_val, y_train, y_val = train_test_split(x, y,
                                                                                                          test_size=val_train_split, 
                                                                                                          random_state=random_state,
                                                                                                          stratify=stratify_y)

        x = np.concatenate((x_train, x_val)).tolist()
        assert len(set(x)) == len(x), f&#34;[Warning] Check for possible data leakage. len(set(x))={len(set(x))} != len(x)={len(x)}&#34;
        
        logging.debug(f&#34;x_train.shape={x_train.shape}, y_train.shape={y_train.shape}&#34;)
        logging.debug(f&#34;x_val.shape={x_val.shape}, y_val.shape={y_val.shape}&#34;)
        logging.debug(f&#39;Absolute splits: {[train_split, val_train_split]}&#39;)
        
        return {&#34;train&#34;:(x_train, y_train),
                        &#34;val&#34;:(x_val, y_val)}



####################################################

def trainvaltest_split(x: Union[List[Any],np.ndarray]=None,
                                           y: Union[List[Any],np.ndarray]=None,
                                           splits: List[float]=(0.5, 0.2, 0.3),
                                           random_state: int=None,
                                           stratify: bool=True
                                           ) -&gt; Dict[str,Tuple[np.ndarray]]:
        &#34;&#34;&#34;
        Wrapper function to split data into 3 stratified subsets specified by `splits`.
        
        User specifies absolute fraction of total requested for each subset (e.g. splits=[0.5, 0.2, 0.3])
        
        Function calculates adjusted fractions necessary in order to use sklearn&#39;s builtin train_test_split function over a sequence of 2 steps.
        
        Step 1: Separate test set from the rest of the data (constituting the union of train + val)
        
        Step 2: Separate the train and val sets from the remainder produced by step 1.

        Output:
                Dict: {&#39;train&#39;:(x_train, y_train),
                                &#39;val&#39;:(x_val_y_val),
                                &#39;test&#39;:(x_test, y_test)}
                                
        Example:
                &gt;&gt; data = torch.data.Dataset(...)
                &gt;&gt; y = data.targets
                &gt;&gt; data_splits = trainvaltest_split(x=None,
                                                                                        y=y,
                                                                                        splits=(0.5, 0.2, 0.3),
                                                                                        random_state=0,
                                                                                        stratify=True)
        
        &#34;&#34;&#34;
        
        
        assert len(splits) == 3, &#34;Must provide eactly 3 float values for `splits`&#34;
        assert np.isclose(np.sum(splits), 1.0), f&#34;Sum of all splits values {splits} = {np.sum(splits)} must be 1.0&#34;
        
        train_split, val_split, test_split = splits
        val_relative_split = val_split/(train_split + val_split)
        train_relative_split = train_split/(train_split + val_split)
        
        
        if stratify and (y is None):
                raise ValueError(&#34;If y is not provided, stratify must be set to False.&#34;)
        
        y = np.array(y)
        if x is None:
                x = np.arange(len(y))
        else:
                x = np.array(x)
        
        stratify_y = y if stratify else None    
        x_train_val, x_test, y_train_val, y_test = train_test_split(x, y,
                                                                                                                test_size=test_split, 
                                                                                                                random_state=random_state,
                                                                                                                stratify=y)
        
        stratify_y_train = y_train_val if stratify else None
        x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val,
                                                                                                          test_size=val_relative_split,
                                                                                                          random_state=random_state, 
                                                                                                          stratify=y_train_val)
        
        x = np.concatenate((x_train, x_val, x_test)).tolist()
        assert len(set(x)) == len(x), f&#34;[Warning] Check for possible data leakage. len(set(x))={len(set(x))} != len(x)={len(x)}&#34;
        
        logging.debug(f&#34;x_train.shape={x_train.shape}, y_train.shape={y_train.shape}&#34;)
        logging.debug(f&#34;x_val.shape={x_val.shape}, y_val.shape={y_val.shape}&#34;)
        logging.debug(f&#34;x_test.shape={x_test.shape}, y_test.shape={y_test.shape}&#34;)
        logging.debug(f&#39;Absolute splits: {[train_split, val_split, test_split]}&#39;)
        logging.debug(f&#39;Relative splits: [{train_relative_split:.2f}, {val_relative_split:.2f}, {test_split}]&#39;)
        
        return {&#34;train&#34;:(x_train, y_train),
                        &#34;val&#34;:(x_val, y_val),
                        &#34;test&#34;:(x_test, y_test)}


#############################################################
#############################################################


class DataSplitter:

        @classmethod
        def create_trainvaltest_splits(cls,
                                                                   data: &#34;torchdata.Dataset&#34;,
                                                                   val_split: float=0.2,
                                                                   test_split: Optional[Union[str, float]]=None, #0.3,
                                                                   shuffle: bool=True,
                                                                   seed: int=3654,
                                                                   stratify: bool=True,
                                                                   plot_distributions: bool=False) -&gt; Tuple[&#34;FossilDataset&#34;]:
                
                if (test_split == &#34;test&#34;) or (test_split is None):
                        train_split = 1 - val_split
                        if hasattr(data, f&#34;test_dataset&#34;):
                                data = getattr(data, f&#34;train_dataset&#34;)                  
                elif isinstance(test_split, float):
                        train_split = 1 - (test_split + val_split)
                else:
                        raise ValueError(f&#34;Invalid split arguments: val_train_split={val_train_split}, test_split={test_split}&#34;)


                splits=(train_split, val_split, test_split)
                splits = list(filter(lambda x: isinstance(x, float), splits))
                y = data.targets

                if len(splits)==2:
                        data_splits = trainval_split(x=None,
                                                                                 y=y,
                                                                                 val_train_split=splits[-1],
                                                                                 random_state=seed,
                                                                                 stratify=stratify)

                else:
                        data_splits = trainvaltest_split(x=None,
                                                                                         y=y,
                                                                                         splits=splits,
                                                                                         random_state=seed,
                                                                                         stratify=stratify)

                dataset_splits={}
                for split, (split_idx, split_y) in data_splits.items():
                        print(split, len(split_idx))
                        dataset_splits[split] = data.filter(indices=split_idx, subset_key=split)
                
                
                label_encoder = LabelEncoder()
                label_encoder.fit(dataset_splits[&#34;train&#34;].targets)
                
                for d in [*list(dataset_splits.values()), data]:
                        d.label_encoder = label_encoder
                return dataset_splits


#############################################################
#############################################################


def plot_class_distributions(targets: List[Any], 
                                                         sort_by: Optional[Union[str, bool, Sequence]]=&#34;count&#34;,
                                                         ax=None,
                                                         xticklabels: bool=True,
                                                         hist_kwargs: Optional[Dict]=None):
        &#34;&#34;&#34;
        Example:
                counts = plot_class_distributions(targets=data.targets, sort=True)
        &#34;&#34;&#34;
        
        counts = compute_class_counts(targets,
                                                                  sort_by=sort_by)
                                                
        keys = list(counts.keys())
        values = list(counts.values())

        if ax is None:
                plt.figure(figsize=(20,12))
        ax = sns.histplot(x=keys, weights=values, discrete=True, ax=ax, **hist_kwargs)
        plt.sca(ax)
        if xticklabels:
                xtick_fontsize = &#34;medium&#34;
                if len(keys) &gt; 100:
                        xtick_fontsize = &#34;x-small&#34;
                elif len(keys) &gt; 75:
                        xtick_fontsize = &#34;small&#34;
                plt.xticks(
                        rotation=90, #45, 
                        horizontalalignment=&#39;right&#39;,
                        fontweight=&#39;light&#39;,
                        fontsize=xtick_fontsize
                )
                if len(keys) &gt; 100:
                        for label in ax.xaxis.get_ticklabels()[::2]:
                                label.set_visible(False)
                
        else:
                ax.set_xticklabels([])
        
        return counts


#############################################################
#############################################################


def plot_split_distributions(data_splits: Dict[str, &#34;CommonDataset&#34;],
                                                         use_one_axis: bool=False,
                                                         hist_kwargs: Optional[Dict]=None):
        &#34;&#34;&#34;
        Create 3 vertically-stacked count plots of train, val, and test dataset class label distributions
        
        Arguments:
                data_splits: Dict[str, &#34;CommonDataset&#34;],
                        Dictionary mapping str split names to Dataset objects that at least have a Dataset.targets attribute for labels.
                use_one_axis: bool=False
                        If true, Plot all subsets to the same axis overlayed on top of each other. If False, plot them in individual subplots in the same figure.
                hist_kwargs: Optional[Dict]=None
                        Optional additional kwargs to be passed to sns.histplot(**hist_kwargs)
        
        &#34;&#34;&#34;
        assert isinstance(data_splits, dict)
        num_splits = len(data_splits)
        
        if use_one_axis:
                rows, cols = 1, 1
                fig, ax = plt.subplots(rows, cols, figsize=(20*cols,10*rows))
                ax = [ax]*num_splits
        else:
                if num_splits &lt;= 3:
                        rows = num_splits
                        cols = 1
                else:
                        rows = int(num_splits // 2)
                        cols = int(num_splits % 2)
                        
                fig, ax = plt.subplots(rows, cols, figsize=(20*cols,10*rows))   
                if hasattr(ax, &#34;flatten&#34;):
                        ax = ax.flatten()
        
        train_key = [k for k,v in data_splits.items() if &#34;train&#34; in k]
        sort_by = True
        if len(train_key)==1:
                sort_by = compute_class_counts(data_splits[train_key[0]].targets,
                                                                           sort_by=&#34;count&#34;)
                logging.info(f&#39;Sorting by count for {train_key} subset, and applying order to all other subsets&#39;)
#                logging.info(f&#34;len(sort_by)={len(sort_by)}&#34;)

        num_classes = len(set(list(data_splits.values())[0].targets))   
        xticklabels=False
        num_samples = 0
        counts = {}
        for i, (k, v) in enumerate(data_splits.items()):
                if i == num_splits-1:
                        xticklabels=True
                counts[k] = plot_class_distributions(targets=v.targets, 
                                                                                         sort_by=sort_by,
                                                                                         ax = ax[i],
                                                                                         xticklabels=xticklabels,
                                                                                         hist_kwargs=hist_kwargs)
                num_nonzero_classes = len([name for name, count_i in counts[k].items() if count_i &gt; 0])
                
                title = f&#34;{k} [n={len(v)}&#34;
                if num_nonzero_classes &lt; num_classes:
                        title += f&#34;, num_classes@(count &gt; 0) = {num_nonzero_classes}-out-of-{num_classes} classes in dataset&#34;
                title += &#34;]&#34;
                plt.gca().set_title(title, fontsize=&#39;large&#39;)
                
                num_samples += len(v)
        
        suptitle = &#39;-&#39;.join(list(data_splits.keys())) + f&#34;_splits (total samples={num_samples}, total classes = {num_classes})&#34;
        
        plt.suptitle(suptitle, fontsize=&#39;x-large&#39;)
        plt.subplots_adjust(bottom=0.1, top=0.94, wspace=None, hspace=0.08)
        
        return fig, ax


#############################################################
#############################################################



#############################################################
#############################################################


def filter_df_by_threshold(df: pd.DataFrame,
                                                   threshold: int,
                                                   y_col: str=&#39;family&#39;):
        &#34;&#34;&#34;
        Filter rare classes from dataset in a pd.DataFrame
        
        Input:
                df (pd.DataFrame):
                        Must contain at least 1 column with name given by `y_col`
                threshold (int):
                        Exclude any rows from df that contain a `y_col` value with fewer than `threshold` members in all of df.
                y_col (str): default=&#34;family&#34;
                        The column in df to look for rare classes to exclude.
        Output:
                (pd.DataFrame):
                        Returns a dataframe with the same number of columns as df, and an equal or lower number of rows.
        &#34;&#34;&#34;
        return df.groupby(y_col).filter(lambda x: len(x) &gt;= threshold)


#############################################################
#############################################################



def compute_class_counts(targets: Sequence,
                                                 sort_by: Optional[Union[str, bool, Sequence]]=&#34;count&#34;
                                                ) -&gt; Dict[str, int]:
        
        counts = collections.Counter(targets)
#        if hasattr(sort_by, &#34;__len__&#34;):
        if isinstance(sort_by, dict):
                counts = {k: counts[k] for k in sort_by.keys()}
        if isinstance(sort_by, list):
                counts = {k: counts[k] for k in sort_by}
        elif (sort_by == &#34;count&#34;):
                counts = dict(sorted(counts.items(), key = lambda x:x[1], reverse=True))
        elif (sort_by is True):
                counts = dict(sorted(counts.items(), key = lambda x:x[0], reverse=True))
                
        return counts

#############################################################
#############################################################














#############################################################
#############################################################
############################################################
############################################################

def check_random_state(seed):
        &#34;&#34;&#34;Turn seed into a np.random.RandomState instance
        
        Source: scikit-learn
        
        Parameters
        ----------
        seed : None, int or instance of RandomState
                If seed is None, return the RandomState singleton used by np.random.
                If seed is an int, return a new RandomState instance seeded with seed.
                If seed is already a RandomState instance, return it.
                Otherwise raise ValueError.
        &#34;&#34;&#34;
        if seed is None or seed is np.random:
                return np.random.mtrand._rand
        if isinstance(seed, numbers.Integral):
                return np.random.RandomState(seed)
        if isinstance(seed, np.random.RandomState):
                return seed
        raise ValueError(&#39;%r cannot be used to seed a numpy.random.RandomState&#39;
                                         &#39; instance&#39; % seed)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imutils.ml.utils.label_utils.LabelEncoder"><code class="flex name class">
<span>class <span class="ident">LabelEncoder</span></span>
<span>(</span><span>class2idx:Â Dict[str,Â int]Â =Â None, replacements:Â Optional[Dict[str,Â str]]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Label encoder for tag labels.</p>
<p>len(idx2class) &lt;= len(class2idx)
num_classes == len(idx2class) &lt;= len(class2idx)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LabelEncoder(object):
        &#34;&#34;&#34;
        Label encoder for tag labels.
        
        len(idx2class) &lt;= len(class2idx)
        num_classes == len(idx2class) &lt;= len(class2idx)
        &#34;&#34;&#34;
        def __init__(self,
                                 class2idx: Dict[str,int]=None,
                                 replacements: Optional[Dict[str,str]]=None):
                &#34;&#34;&#34;
                
                &#34;&#34;&#34;
                

                self.class2idx = class2idx or {}
                self.replacements = replacements or {}
                assert len(self.classes) == len(self.idx2class) &lt;= len(self.class2idx)
                self.num_samples = 0
                self.verbose=False

        def copy(self, encoder: &#34;LabelEncoder&#34;) -&gt; &#34;LabelEncoder&#34;:
                &#34;&#34;&#34;
                Instantiate a new LabelEncoder instance containing all attributes of this one.
                &#34;&#34;&#34;
                return LabelEncoder(class2idx=self.class2idx,
                                                        replacements=self.replacements)


        @classmethod
        def from_sklearn(cls,
                                         encoder: preprocessing.LabelEncoder=None,
                                         replacements: Optional[Dict[str,str]]=None):
                &#34;&#34;&#34;
                Returns a new instance of this custom LabelEncoder from a fitted sklearn LabelEncoder. Useful for interoperability.
                &#34;&#34;&#34;
                
                class_list = getattr(encoder, &#34;classes_&#34;, [])
                class2idx = {label: idx for idx, label in enumerate(class_list)}
                
                custom_encoder = cls(class2idx=class2idx,
                                                         replacements=replacements)
                
                return custom_encoder


        def to_sklearn(self) -&gt; preprocessing.LabelEncoder:
                &#34;&#34;&#34;
                Returns an sklearn LabelEncoder after fitting it to any existing labels contained in this custom Label Encoder. Useful for interoperability.
                &#34;&#34;&#34;
                sklearn_encoder = preprocessing.LabelEncoder()
                sklearn_encoder.fit(self.classes)
                
                return sklearn_encoder


        @property
        def idx2class(self) -&gt; Dict[Any, Any]:
                return {v: k for k, v in self.class2idx.items() if k not in self.replacements.keys()}
        
        @property
        def classes(self) -&gt; List[Any]:
                return [k for k in self.class2idx.keys() if k not in self.replacements.keys()]

        def __len__(self):
                return len(self.idx2class)

        @property
        def num_classes(self) -&gt; int:
                return len(self)

        def fit(self, y):
                &#34;&#34;&#34;
                In the case that this label encoder already has 1+ labels, this assigns any new classes integers above any already used.
                In the case that this has never been used, it collects any new labels and assigns them inteegers starting with 0.
                In both cases, if a label exists in the keys of self.replacements, then they will be replaced by the corresponding value before being considered for use in the encoder.
                
                Arguments:
                        y: Sequence
                
                &#34;&#34;&#34;
                
                counts = collections.Counter(y)
                self.num_samples += sum(counts.values())
                
                classes = sorted(list(counts.keys()))
                new_classes = sorted([label for label in classes if (label not in self.classes) and (label not in self.replacements.keys())])
                replace_classes = sorted([label for label in classes if label in self.replacements.keys()])
                
                old_num_classes = len(self)             
                old_highest_class = None
                idx = 0
                if len(self.idx2class) &gt; 0:
                        old_highest_class = max(self.idx2class.keys())
                        idx = old_highest_class + 1
                        
                for label in new_classes:
                        self.class2idx[label] = idx
                        idx += 1
                for label in replace_classes:
                        if self.replacements[label] in self.class2idx:
                                self.class2idx[label] = self.class2idx[self.replacements[label]]
                        else:
                                logging.warning(
                                        f&#34;[Warning]: label {label} marked for replacement, but its replacement label has yet to be assigned an int encoding in class2idx.&#34;
                                )
                        
                new_classes = [c for c in new_classes if c not in self.replacements.keys()]
                if len(new_classes):
                        logging.debug(f&#34;[FITTING] {len(y)} samples with {len(classes)} classes, adding {len(new_classes)} new class labels. Latest num_classes = {len(self)}&#34;)
                assert len(self) == (old_num_classes + len(new_classes)), f&#34;len(self)={len(self)}, (old_num_classes={old_num_classes}, len(new_classes)={len(new_classes)})&#34;
                assert np.all([label in self.idx2class.values() for label in new_classes])
                return self

        def transform(self, y):
                &#34;&#34;&#34;Wrapper for inter-operability with sklearn&#34;&#34;&#34;
                return self.encode(y)
        def inv_transform(self, y):
                &#34;&#34;&#34;Wrapper for inter-operability with sklearn&#34;&#34;&#34;
                return self.decode(y)



        def encode(self, y, strict: bool=True):
                if not hasattr(y,&#34;__len__&#34;):
                        y = [y]
                if strict:
                        return np.array([self.class2idx[label] for label in y])
                return np.array([self.class2idx.get(label, -1) for label in y])

        def decode(self, y, strict: bool=True):
                if not hasattr(y,&#34;__len__&#34;):
                        y = [y]
                if strict:
                        return np.array([self.idx2class[label] for label in y])
                return np.array([self.idx2class.get(label, &#34;unknown&#34;) for label in y])

        def decode_topk(self, y, strict: bool=True):
                &#34;&#34;&#34;
                Provide a 2-D array of integer labels representing top-k predicted classes.
                The top-k for each sample is stored in each column of the corresponding row.
                
                Arguments:
                        y: Union[np.ndarray, List]
                                y can be any of the following:
                                
                                        - List of lists of integer labels (e.g. y=[[0,3,4], 
                                                                                                                           [3,7,5]])
                                        - List of 1-D np.ndarrays of integer labels (e.g. y=[np.ndarray([0,3,4]),
                                                                                                                                                 np.ndarray([3,7,5])]
                                        - List of 2-D np.ndarrays of integer labels, with samples incremented along rows and top-k integer predictions in the columns.
                                        (e.g. y=np.ndarray
                                                        [np.ndarray([0,3,4]),
                                                         np.ndarray([3,7,5])])
                                                                                                                                                 
                        strict: bool=True
                &#34;&#34;&#34;
                
                topk_labels = []
                if isinstance(y, np.ndarray):
                        if y.ndim == 2:
                                topk = y.shape[1]
                        else:
                                topk = 1
                if isinstance(y, list):
                        if isinstance(y[0], np.ndarray):
                                topk = y[0].shape[0]
                        elif isinstance(y[0], list):
                                topk = len(y[0])

                for k in range(topk):
                        # labels_k = datamodule.train_dataset.label_encoder.inverse_transform(y_logits_top5.indices[:,k])
                        labels_k = self.decode(y[:,k], strict=strict)
                        topk_labels.append(labels_k)

                return np.vstack(topk_labels).T


        def save(self, fp):
                with open(fp, &#34;w&#34;) as fp:
                        contents = self.getstate() # {&#34;class2idx&#34;: self.class2idx}
                        json.dump(contents, fp, indent=4, sort_keys=False)

        @classmethod
        def load(cls, fp):
                with open(fp, &#34;r&#34;) as fp:
                        kwargs = json.load(fp=fp)
                return cls(**kwargs)
        
        def getstate(self):
                return {&#34;class2idx&#34;: self.class2idx,
                                &#34;replacements&#34;: self.replacements}

        def calculate_class_counts(y: Sequence) -&gt; Tuple[np.ndarray]:
                &#34;&#34;&#34;
                Returns a Tuple of np.ndarrays, the first has unique classes, the second has class counts.
                &#34;&#34;&#34;
                
                classes, counts = BF.class_counts(y)
                return classes, counts
        
        
        def __repr__(self):
                disp = f&#34;&#34;&#34;&lt;{str(type(self)).strip(&#34;&#39;&gt;&#34;).split(&#39;.&#39;)[-1]}&gt;:\n&#34;&#34;&#34;
                disp += f&#34;      num_classes: {len(self)}\n&#34;
                disp += f&#34;      fit on num_samples: {self.num_samples}&#34;
                return disp

        def __str__(self):
                msg = f&#34;&lt;LabelEncoder(num_classes={len(self)})&gt;&#34;
                if len(self.replacements) &gt; 0:
                        msg += &#34;\n&#34; + f&#34;&lt;num_replaced_classes={len(self.replacements)}&gt;&#34;
                return msg</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.from_sklearn"><code class="name flex">
<span>def <span class="ident">from_sklearn</span></span>(<span>encoder:Â sklearn.preprocessing._label.LabelEncoderÂ =Â None, replacements:Â Optional[Dict[str,Â str]]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new instance of this custom LabelEncoder from a fitted sklearn LabelEncoder. Useful for interoperability.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_sklearn(cls,
                                 encoder: preprocessing.LabelEncoder=None,
                                 replacements: Optional[Dict[str,str]]=None):
        &#34;&#34;&#34;
        Returns a new instance of this custom LabelEncoder from a fitted sklearn LabelEncoder. Useful for interoperability.
        &#34;&#34;&#34;
        
        class_list = getattr(encoder, &#34;classes_&#34;, [])
        class2idx = {label: idx for idx, label in enumerate(class_list)}
        
        custom_encoder = cls(class2idx=class2idx,
                                                 replacements=replacements)
        
        return custom_encoder</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>fp)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, fp):
        with open(fp, &#34;r&#34;) as fp:
                kwargs = json.load(fp=fp)
        return cls(**kwargs)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.classes"><code class="name">var <span class="ident">classes</span> :Â List[Any]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def classes(self) -&gt; List[Any]:
        return [k for k in self.class2idx.keys() if k not in self.replacements.keys()]</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.idx2class"><code class="name">var <span class="ident">idx2class</span> :Â Dict[Any,Â Any]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def idx2class(self) -&gt; Dict[Any, Any]:
        return {v: k for k, v in self.class2idx.items() if k not in self.replacements.keys()}</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.num_classes"><code class="name">var <span class="ident">num_classes</span> :Â int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_classes(self) -&gt; int:
        return len(self)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.calculate_class_counts"><code class="name flex">
<span>def <span class="ident">calculate_class_counts</span></span>(<span>y:Â Sequence[+T_co]) â€‘>Â Tuple[numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a Tuple of np.ndarrays, the first has unique classes, the second has class counts.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_class_counts(y: Sequence) -&gt; Tuple[np.ndarray]:
        &#34;&#34;&#34;
        Returns a Tuple of np.ndarrays, the first has unique classes, the second has class counts.
        &#34;&#34;&#34;
        
        classes, counts = BF.class_counts(y)
        return classes, counts</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self, encoder:Â <a title="imutils.ml.utils.label_utils.LabelEncoder" href="#imutils.ml.utils.label_utils.LabelEncoder">LabelEncoder</a>) â€‘>Â <a title="imutils.ml.utils.label_utils.LabelEncoder" href="#imutils.ml.utils.label_utils.LabelEncoder">LabelEncoder</a></span>
</code></dt>
<dd>
<div class="desc"><p>Instantiate a new LabelEncoder instance containing all attributes of this one.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self, encoder: &#34;LabelEncoder&#34;) -&gt; &#34;LabelEncoder&#34;:
        &#34;&#34;&#34;
        Instantiate a new LabelEncoder instance containing all attributes of this one.
        &#34;&#34;&#34;
        return LabelEncoder(class2idx=self.class2idx,
                                                replacements=self.replacements)</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, y, strict:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self, y, strict: bool=True):
        if not hasattr(y,&#34;__len__&#34;):
                y = [y]
        if strict:
                return np.array([self.idx2class[label] for label in y])
        return np.array([self.idx2class.get(label, &#34;unknown&#34;) for label in y])</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.decode_topk"><code class="name flex">
<span>def <span class="ident">decode_topk</span></span>(<span>self, y, strict:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"><p>Provide a 2-D array of integer labels representing top-k predicted classes.
The top-k for each sample is stored in each column of the corresponding row.</p>
<h2 id="arguments">Arguments</h2>
<p>y: Union[np.ndarray, List]
y can be any of the following:</p>
<pre><code>            - List of lists of integer labels (e.g. y=[[0,3,4], 
                                                                                               [3,7,5]])
            - List of 1-D np.ndarrays of integer labels (e.g. y=[np.ndarray([0,3,4]),
                                                                                                                     np.ndarray([3,7,5])]
            - List of 2-D np.ndarrays of integer labels, with samples incremented along rows and top-k integer predictions in the columns.
            (e.g. y=np.ndarray
                            [np.ndarray([0,3,4]),
                             np.ndarray([3,7,5])])
</code></pre>
<p>strict: bool=True</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode_topk(self, y, strict: bool=True):
        &#34;&#34;&#34;
        Provide a 2-D array of integer labels representing top-k predicted classes.
        The top-k for each sample is stored in each column of the corresponding row.
        
        Arguments:
                y: Union[np.ndarray, List]
                        y can be any of the following:
                        
                                - List of lists of integer labels (e.g. y=[[0,3,4], 
                                                                                                                   [3,7,5]])
                                - List of 1-D np.ndarrays of integer labels (e.g. y=[np.ndarray([0,3,4]),
                                                                                                                                         np.ndarray([3,7,5])]
                                - List of 2-D np.ndarrays of integer labels, with samples incremented along rows and top-k integer predictions in the columns.
                                (e.g. y=np.ndarray
                                                [np.ndarray([0,3,4]),
                                                 np.ndarray([3,7,5])])
                                                                                                                                         
                strict: bool=True
        &#34;&#34;&#34;
        
        topk_labels = []
        if isinstance(y, np.ndarray):
                if y.ndim == 2:
                        topk = y.shape[1]
                else:
                        topk = 1
        if isinstance(y, list):
                if isinstance(y[0], np.ndarray):
                        topk = y[0].shape[0]
                elif isinstance(y[0], list):
                        topk = len(y[0])

        for k in range(topk):
                # labels_k = datamodule.train_dataset.label_encoder.inverse_transform(y_logits_top5.indices[:,k])
                labels_k = self.decode(y[:,k], strict=strict)
                topk_labels.append(labels_k)

        return np.vstack(topk_labels).T</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, y, strict:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, y, strict: bool=True):
        if not hasattr(y,&#34;__len__&#34;):
                y = [y]
        if strict:
                return np.array([self.class2idx[label] for label in y])
        return np.array([self.class2idx.get(label, -1) for label in y])</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, y)</span>
</code></dt>
<dd>
<div class="desc"><p>In the case that this label encoder already has 1+ labels, this assigns any new classes integers above any already used.
In the case that this has never been used, it collects any new labels and assigns them inteegers starting with 0.
In both cases, if a label exists in the keys of self.replacements, then they will be replaced by the corresponding value before being considered for use in the encoder.</p>
<h2 id="arguments">Arguments</h2>
<p>y: Sequence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, y):
        &#34;&#34;&#34;
        In the case that this label encoder already has 1+ labels, this assigns any new classes integers above any already used.
        In the case that this has never been used, it collects any new labels and assigns them inteegers starting with 0.
        In both cases, if a label exists in the keys of self.replacements, then they will be replaced by the corresponding value before being considered for use in the encoder.
        
        Arguments:
                y: Sequence
        
        &#34;&#34;&#34;
        
        counts = collections.Counter(y)
        self.num_samples += sum(counts.values())
        
        classes = sorted(list(counts.keys()))
        new_classes = sorted([label for label in classes if (label not in self.classes) and (label not in self.replacements.keys())])
        replace_classes = sorted([label for label in classes if label in self.replacements.keys()])
        
        old_num_classes = len(self)             
        old_highest_class = None
        idx = 0
        if len(self.idx2class) &gt; 0:
                old_highest_class = max(self.idx2class.keys())
                idx = old_highest_class + 1
                
        for label in new_classes:
                self.class2idx[label] = idx
                idx += 1
        for label in replace_classes:
                if self.replacements[label] in self.class2idx:
                        self.class2idx[label] = self.class2idx[self.replacements[label]]
                else:
                        logging.warning(
                                f&#34;[Warning]: label {label} marked for replacement, but its replacement label has yet to be assigned an int encoding in class2idx.&#34;
                        )
                
        new_classes = [c for c in new_classes if c not in self.replacements.keys()]
        if len(new_classes):
                logging.debug(f&#34;[FITTING] {len(y)} samples with {len(classes)} classes, adding {len(new_classes)} new class labels. Latest num_classes = {len(self)}&#34;)
        assert len(self) == (old_num_classes + len(new_classes)), f&#34;len(self)={len(self)}, (old_num_classes={old_num_classes}, len(new_classes)={len(new_classes)})&#34;
        assert np.all([label in self.idx2class.values() for label in new_classes])
        return self</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.getstate"><code class="name flex">
<span>def <span class="ident">getstate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getstate(self):
        return {&#34;class2idx&#34;: self.class2idx,
                        &#34;replacements&#34;: self.replacements}</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.inv_transform"><code class="name flex">
<span>def <span class="ident">inv_transform</span></span>(<span>self, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for inter-operability with sklearn</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inv_transform(self, y):
        &#34;&#34;&#34;Wrapper for inter-operability with sklearn&#34;&#34;&#34;
        return self.decode(y)</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, fp)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, fp):
        with open(fp, &#34;w&#34;) as fp:
                contents = self.getstate() # {&#34;class2idx&#34;: self.class2idx}
                json.dump(contents, fp, indent=4, sort_keys=False)</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.to_sklearn"><code class="name flex">
<span>def <span class="ident">to_sklearn</span></span>(<span>self) â€‘>Â sklearn.preprocessing._label.LabelEncoder</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an sklearn LabelEncoder after fitting it to any existing labels contained in this custom Label Encoder. Useful for interoperability.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_sklearn(self) -&gt; preprocessing.LabelEncoder:
        &#34;&#34;&#34;
        Returns an sklearn LabelEncoder after fitting it to any existing labels contained in this custom Label Encoder. Useful for interoperability.
        &#34;&#34;&#34;
        sklearn_encoder = preprocessing.LabelEncoder()
        sklearn_encoder.fit(self.classes)
        
        return sklearn_encoder</code></pre>
</details>
</dd>
<dt id="imutils.ml.utils.label_utils.LabelEncoder.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for inter-operability with sklearn</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, y):
        &#34;&#34;&#34;Wrapper for inter-operability with sklearn&#34;&#34;&#34;
        return self.encode(y)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imutils.ml.utils" href="index.html">imutils.ml.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imutils.ml.utils.label_utils.LabelEncoder" href="#imutils.ml.utils.label_utils.LabelEncoder">LabelEncoder</a></code></h4>
<ul class="">
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.calculate_class_counts" href="#imutils.ml.utils.label_utils.LabelEncoder.calculate_class_counts">calculate_class_counts</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.classes" href="#imutils.ml.utils.label_utils.LabelEncoder.classes">classes</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.copy" href="#imutils.ml.utils.label_utils.LabelEncoder.copy">copy</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.decode" href="#imutils.ml.utils.label_utils.LabelEncoder.decode">decode</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.decode_topk" href="#imutils.ml.utils.label_utils.LabelEncoder.decode_topk">decode_topk</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.encode" href="#imutils.ml.utils.label_utils.LabelEncoder.encode">encode</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.fit" href="#imutils.ml.utils.label_utils.LabelEncoder.fit">fit</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.from_sklearn" href="#imutils.ml.utils.label_utils.LabelEncoder.from_sklearn">from_sklearn</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.getstate" href="#imutils.ml.utils.label_utils.LabelEncoder.getstate">getstate</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.idx2class" href="#imutils.ml.utils.label_utils.LabelEncoder.idx2class">idx2class</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.inv_transform" href="#imutils.ml.utils.label_utils.LabelEncoder.inv_transform">inv_transform</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.load" href="#imutils.ml.utils.label_utils.LabelEncoder.load">load</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.num_classes" href="#imutils.ml.utils.label_utils.LabelEncoder.num_classes">num_classes</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.save" href="#imutils.ml.utils.label_utils.LabelEncoder.save">save</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.to_sklearn" href="#imutils.ml.utils.label_utils.LabelEncoder.to_sklearn">to_sklearn</a></code></li>
<li><code><a title="imutils.ml.utils.label_utils.LabelEncoder.transform" href="#imutils.ml.utils.label_utils.LabelEncoder.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>