
defaults:
    - _self_
    - pretrain@pretrain.lr_tuner: lr_tuner
    - data/datamodule@data: base_datamodule
    - callbacks@train.callbacks: default #base_callbacks #
    - aug@data.datamodule.transform_cfg: default_image_aug_conf
    - train/pl_trainer/strategy@train.pl_trainer: ddp # dp
    # - override train/callbacks: default

# metadata specialised for each experiment
core:
    name: "${data.datamodule.name}__${model_cfg.name}"
    version: 0.0.1
    experiments_root_dir: "/media/data_cifs/projects/prj_fossils/users/jacob/experiments/2022/herbarium2022"
    tags:
        - herbarium2022
        - kaggle

execution_list:
    auto_lr_tune: true
    model_fit: true


hp:
    batch_size: 128
    preprocess_size: 256
    resolution: 224
    num_channels: 3
    warmup_epochs: 3
    max_epochs: 30
    lr: 1e-3    # 0.001
    freeze_backbone: false
    freeze_backbone_up_to: -1

seed: 42
run_output_dir: ${hydra:run.dir}
checkpoint_dir: "${run_output_dir}/ckpts"
# defaults:
#    - override hydra/launcher: joblib


model_cfg:
    _target_: imutils.ml.models.pl.classifier.LitClassifier
    backbone:
        model_repo: timm
        name: resnetv2_50
        pretrained: true
        freeze_backbone: ${hp.freeze_backbone}
    head:
        num_classes: 15501
        pool_size: 1
        pool_type: 'avg'
        head_type: 'linear'
        hidden_size: 512
        dropout_p: 0.3

    name: ${.backbone.name}
    loss:
        _target_: torch.nn.CrossEntropyLoss
        # Note: label_smoothing isnt an option in CELoss until some time after pytorch version 1.8.1
        # label_smoothing: 0.0
    resolution: ${int:${hp.resolution}}
    num_channels: ${int:${hp.num_channels}}
    input_shape: 
        - ${int:${hp.num_channels}}
        - ${int:${hp.resolution}}
        - ${int:${hp.resolution}}

    # callbacks:
    #     class_counts_stats: null
    #     image_stats_accumulator: null
    #     log_per_class_metrics_to_wandb: null


optim:
    optimizer:
        _target_: torch.optim.Adam
        lr: ${hp.lr} #1e-3    # 0.001
        betas: [ 0.9, 0.999 ]
        eps: 1e-08
        weight_decay: 0.0
    exclude_bn_bias: True

    use_lr_scheduler: True
    lr_scheduler:
        _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
        warmup_epochs: ${hp.warmup_epochs}
        max_epochs: ${hp.max_epochs}
        warmup_start_lr: 1e-04
        eta_min: 1e-06
        last_epoch: -1
    
    
    # lr_scheduler:
    #     _target_: torch.optim.lr_scheduler.LambdaLR
        # warmup_steps: 2734    #    (70000 // 256) * 10
        # total_steps: 7000000    # 70000 * 100



train:
    # callbacks: ${callbacks/defaults}
    # reproducibility
    deterministic: False
    random_seed: ${seed}

    # training

    pl_trainer:
        _target_: pytorch_lightning.Trainer
        fast_dev_run: False # Enable this for debug purposes
        accelerator: "gpu"
        devices: 1
        precision: 16
        enable_model_summary: true
        # log_gpu_memory: true
        log_every_n_steps: 15
        max_epochs: ${hp.max_epochs}
        # max_steps: ${optim.lr_scheduler.total_steps}
        accumulate_grad_batches: 1
        # num_sanity_val_steps: 2
        # gradient_clip_val: 10000000.0    # 10.

    freeze_backbone: ${hp.freeze_backbone} #TODO: see if this option can be completely replaced with hp.freeze_backbone+model.backbone.freeze_backbone
    freeze_backbone_up_to: ${hp.freeze_backbone}

    # monitor_metric: 'val_loss'
    # monitor_metric_mode: 'min'

#     early_stopping:
#         patience: 42
#         verbose: False

#     model_checkpoints:
#         save_top_k: 2
#         verbose: True # False


logging:

    log_model_summary: true
    log_dataset_summary: true

    n_elements_to_log: 32
    normalize_visualization: True

    # log frequency
    val_check_interval: 1.0
    progress_bar_refresh_rate: 20

    wandb:
        project: herbarium2022
        entity: jrose
        name: "${core.name}"
        log_model: true
        tags: ${core.tags}
        # watch:
        #     log: 'all'
        #     log_freq: 10

    # lr_monitor:
    #     logging_interval: step
    #     log_momentum: False


hydra:
    run:
        dir: ${core.experiments_root_dir}/hydra_experiments/${now:%Y-%m-%d}/${now:%H-%M-%S}

    sweep:
        dir: ${core.experiments_root_dir}/hydra_experiments/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}/
        subdir: ${hydra.job.override_dirname}/seed=${seed} #${hydra.job.num}_${hydra.job.id}

    job:
        config:
          override_dirname:
            exclude_keys:
              - seed
        env_set:
            WANDB_START_METHOD: thread

    # launcher:
    #     n_jobs: 4
    #     batch_size: auto
