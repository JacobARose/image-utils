# @package _global_

defaults:
    - _self_
    - optim@optim: base
    - pretrain@pretrain.lr_tuner: lr_tuner
    # - data/datamodule@data: base_datamodule
    - data/datamodule@data: herbarium2022-res_512_datamodule
    - aug@data.datamodule.transform_cfg: default_image_aug
    - model_cfg@model_cfg: base
    - train: base
    - train/pl_trainer/strategy@train.pl_trainer: ddp # dp
    - callbacks@train.callbacks: default #base_callbacks #
    - experiments: null
    
    # - override train/callbacks: default

# metadata specialised for each experiment
core:
    dataset_name: ${data.datamodule.name}
    backbone_name: ${model_cfg.name}
    name: "${.dataset_name}__${.backbone_name}"
    version: 0.0.1
    experiments_root_dir: "/media/data_cifs/projects/prj_fossils/users/jacob/experiments/2022/herbarium2022"
    tags:
        - herbarium2022
        - kaggle

seed: 42
run_output_dir: ${hydra:run.dir}
checkpoint_dir: "${run_output_dir}/ckpts"
ckpt_path: null

execution_list:
    print_cfg: true
    auto_lr_tune: true
    model_fit: true


hp:
    num_classes: ${data.num_classes.${data.datamodule.label_col}}
    batch_size: 128
    preprocess_size: 256
    resolution: 224
    num_channels: 3
    to_grayscale: false
    warmup_epochs: 3
    max_epochs: 30
    lr: 1e-3    # 0.001
    freeze_backbone: false
    freeze_backbone_up_to: -1
    load_from_checkpoint: false




# defaults:
#    - override hydra/launcher: joblib


# model_cfg:
#     _target_: imutils.ml.models.pl.classifier.LitClassifier
#     backbone:
#         model_repo: timm
#         name: resnetv2_50
#         pretrained: true
#         freeze_backbone: ${hp.freeze_backbone}
#     head:
#         num_classes: 15501
#         pool_size: 1
#         pool_type: 'avg'
#         head_type: 'linear'
#         hidden_size: 512
#         dropout_p: 0.3

#     name: ${.backbone.name}
#     loss:
#         _target_: torch.nn.CrossEntropyLoss
#         # Note: label_smoothing isnt an option in CELoss until some time after pytorch version 1.8.1
#         # label_smoothing: 0.0
#     resolution: ${int:${hp.resolution}}
#     num_channels: ${int:${hp.num_channels}}
#     input_shape: 
#         - ${int:${hp.num_channels}}
#         - ${int:${hp.resolution}}
#         - ${int:${hp.resolution}}

    # callbacks:
    #     class_counts_stats: null
    #     image_stats_accumulator: null
    #     log_per_class_metrics_to_wandb: null


# optim:
#     optimizer:
#         _target_: torch.optim.Adam
#         lr: ${hp.lr} #1e-3    # 0.001
#         betas: [ 0.9, 0.999 ]
#         eps: 1e-08
#         weight_decay: 0.0
#     exclude_bn_bias: True

#     use_lr_scheduler: True
#     lr_scheduler:
#         _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
#         warmup_epochs: ${hp.warmup_epochs}
#         max_epochs: ${hp.max_epochs}
#         warmup_start_lr: 1e-04 #overridden if execution_list.auto_lr_tune=True
#         eta_min: 1e-06
#         last_epoch: -1


# train:
#     # callbacks: ${callbacks/defaults}
#     deterministic: False
#     random_seed: ${seed}

#     pl_trainer:
#         _target_: pytorch_lightning.Trainer
#         fast_dev_run: False # Enable this for debug purposes
#         accelerator: "gpu"
#         devices: 1
#         precision: 16
#         enable_model_summary: true
#         log_every_n_steps: 15
#         max_epochs: ${hp.max_epochs}
#         accumulate_grad_batches: 1

#     freeze_backbone: ${hp.freeze_backbone} #TODO: see if this option can be completely replaced with hp.freeze_backbone+model.backbone.freeze_backbone
#     freeze_backbone_up_to: ${hp.freeze_backbone}



logging:

    log_model_summary: true
    log_dataset_summary: true

    max_batches_to_log: 2
    n_elements_to_log: 64
    normalize_visualization: True

    # log frequency
    val_check_interval: 1.0
    progress_bar_refresh_rate: 10

    wandb:
        project: herbarium2022
        entity: jrose
        name: "${core.name}"
        group: "${data.datamodule.name}"
        log_model: true
        tags: ${core.tags}



hydra:
    run:
        dir: ${core.experiments_root_dir}/hydra_experiments/${now:%Y-%m-%d}/${now:%H-%M-%S}

    sweep:
        dir: ${core.experiments_root_dir}/hydra_experiments/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}/
        subdir: ${hydra.job.override_dirname}/seed=${seed} #${hydra.job.num}_${hydra.job.id}

    job:
        config:
          override_dirname:
            exclude_keys:
              - seed
        env_set:
            WANDB_START_METHOD: thread

    # launcher:
    #     n_jobs: 4
    #     batch_size: auto
