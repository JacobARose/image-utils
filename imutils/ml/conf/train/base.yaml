# @package train

## conf/train/base.yaml



defaults:
    - pl_trainer@pl_trainer: base
    # - ../callbacks@callbacks: default
    - _self_
    
    
# reproducibility
deterministic: False
random_seed: ${seed}

# training
# pl_trainer:
#     _target_: pytorch_lightning.Trainer
#     fast_dev_run: False # Enable this for debug purposes
#     accelerator: "gpu"
#     devices: 1
#     precision: 16
#     enable_model_summary: true
#     log_every_n_steps: 15
#     max_epochs: ${hp.max_epochs}
#     accumulate_grad_batches: 1

freeze_backbone: ${hp.freeze_backbone} #TODO: see if this option can be completely replaced with hp.freeze_backbone+model.backbone.freeze_backbone
freeze_backbone_up_to: ${hp.freeze_backbone}